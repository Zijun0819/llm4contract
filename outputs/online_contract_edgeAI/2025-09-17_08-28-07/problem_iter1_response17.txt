```python
import numpy as np
import pandas as pd
from scipy.optimize import minimize, linprog
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import warnings
warnings.filterwarnings('ignore')

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    m_outcomes = len(v)
    L = len(content)
    
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]
    
    if not accepted_logs:
        return np.array([[1/m_outcomes] * m_outcomes + [0.001]])
    
    def solve_dual_lp(w, u):
        c = np.concatenate([w, [0]])
        A_ub = np.concatenate([np.eye(m_outcomes), -np.ones((m_outcomes, 1))], axis=1)
        b_ub = v
        A_eq = np.concatenate([np.ones(m_outcomes).reshape(1, -1), np.zeros((1, 1))], axis=1)
        b_eq = [1.0]
        bounds = [(0, None)] * m_outcomes + [(u, None)]
        
        res = linprog(-c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x[:m_outcomes]
            return p / p.sum() if p.sum() > 0 else np.ones(m_outcomes) / m_outcomes
        return None
    
    candidate_distributions = []
    for log in accepted_logs:
        w = np.array(log['Contract'])
        u = log['Principal Utility']
        p = solve_dual_lp(w, u)
        if p is not None:
            candidate_distributions.append(p)
    
    if not candidate_distributions:
        candidate_distributions = [np.ones(m_outcomes) / m_outcomes]
    
    all_p = np.array(candidate_distributions)
    
    n_actions = min(max(2, len(set([tuple(p) for p in all_p]))), 8)
    
    if len(all_p) >= n_actions:
        gmm = GaussianMixture(n_components=n_actions, random_state=42, max_iter=100)
        try:
            gmm.fit(all_p)
            p_actions = gmm.means_
        except:
            kmeans = KMeans(n_clusters=n_actions, random_state=42, n_init=10)
            kmeans.fit(all_p)
            p_actions = kmeans.cluster_centers_
    else:
        p_actions = all_p.copy()
        while len(p_actions) < n_actions:
            noise = np.random.dirichlet(np.ones(m_outcomes) * 0.1)
            p_actions = np.vstack([p_actions, noise])
    
    for i in range(len(p_actions)):
        p_actions[i] = np.maximum(p_actions[i], 1e-6)
        p_actions[i] /= p_actions[i].sum()
    
    costs = np.zeros(n_actions)
    
    for a in range(n_actions):
        p = p_actions[a]
        
        max_util_accepted = -np.inf
        for log in accepted_logs:
            w = np.array(log['Contract'])
            util = p @ w
            max_util_accepted = max(max_util_accepted, util)
        
        max_util_rejected = -np.inf
        for log in rejected_logs:
            w = np.array(log['Contract'])
            util = p @ w
            max_util_rejected = max(max_util_rejected, util)
        
        if max_util_accepted > -np.inf:
            costs[a] = max_util_accepted
        
        if max_util_rejected > -np.inf:
            costs[a] = max(costs[a], max_util_rejected + 1e-6)
        
        costs[a] = max(costs[a], 0.0)
    
    def objective(params):
        n = len(p_actions)
        p_flat = params[:n * m_outcomes].reshape(n, m_outcomes)
        c = params[n * m_outcomes:]
        
        error = 0.0
        for log in content:
            w = np.array(log['Contract'])
            action = log['Agent Action']
            
            utilities = p_flat @ w - c
            best_action = np.argmax(utilities)
            best_utility = utilities[best_action]
            
            if action == 1 and best_utility < 0:
                error += abs(best_utility) + 0.01
            elif action == -1 and best_utility >= 0:
                error += best_utility + 0.01
        
        regularization = 0.001 * np.sum((p_flat - p_actions)**2) + 0.001 * np.sum((c - costs)**2)
        return error + regularization
    
    def constraint_simplex(params):
        n = len(p_actions)
        p_flat = params[:n * m_outcomes].reshape(n, m_outcomes)
        return p_flat.sum(axis=1) - 1
    
    def constraint_positive_p(params):
        n = len(p_actions)
        p_flat = params[:n * m_outcomes].reshape(n, m_outcomes)
        return p_flat.flatten()
    
    def constraint_positive_c(params):
        n = len(p_actions)
        c = params[n * m_outcomes:]
        return c
    
    initial_params = np.concatenate([p_actions.flatten(), costs])
    
    constraints = [
        {'type': 'eq', 'fun': constraint_simplex},
        {'type': 'ineq', 'fun': constraint_positive_p},
        {'type': 'ineq', 'fun': constraint_positive_c}
    ]
    
    try:
        result = minimize(objective, initial_params, method='SLSQP', constraints=constraints, 
                         options={'maxiter': 1000, 'ftol': 1e-9})
        if result.success:
            n = len(p_actions)
            optimized_p = result.x[:n * m_outcomes].reshape(n, m_outcomes)
            optimized_c = result.x[n * m_outcomes:]
            
            for i in range(n):
                optimized_p[i] = np.maximum(optimized_p[i], 1e-8)
                optimized_p[i] /= optimized_p[i].sum()
            
            optimized_c = np.maximum(optimized_c, 0)
            agent_setting = np.hstack([optimized_p, optimized_c.reshape(-1, 1)])
        else:
            agent_setting = np.hstack([p_actions, costs.reshape(-1, 1)])
    except:
        agent_setting = np.hstack([p_actions, costs.reshape(-1, 1)])
    
    return agent_setting
```
