```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog, minimize
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    n_outcomes = len(v)
    L = len(content)
    
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]
    
    if not accepted_logs:
        default_p = np.ones(n_outcomes) / n_outcomes
        return np.hstack([default_p.reshape(1, -1), np.array([[0.0]])])
    
    def solve_dual_lp(w, u):
        c = np.concatenate([w, [0]])
        A_ub = np.concatenate([np.eye(n_outcomes), -np.ones((n_outcomes, 1))], axis=1)
        b_ub = np.zeros(n_outcomes)
        A_eq = np.concatenate([v - w, [1]]).reshape(1, -1)
        b_eq = np.array([u])
        bounds = [(0, None)] * n_outcomes + [(None, None)]
        
        res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success and res.x is not None:
            lambdas = res.x[:n_outcomes]
            return lambdas / lambdas.sum() if lambdas.sum() > 0 else np.ones(n_outcomes) / n_outcomes
        return None
    
    candidate_distributions = []
    for log in accepted_logs:
        w = np.array(log['Contract'])
        u = log['Principal Utility']
        p = solve_dual_lp(w, u)
        if p is not None:
            candidate_distributions.append(p)
    
    if not candidate_distributions:
        default_p = np.ones(n_outcomes) / n_outcomes
        return np.hstack([default_p.reshape(1, -1), np.array([[0.0]])])
    
    distributions = np.array(candidate_distributions)
    
    n_actions = min(max(2, len(accepted_logs) // 10), 8)
    
    if len(distributions) >= n_actions:
        gmm = GaussianMixture(n_components=n_actions, random_state=42, max_iter=200)
        gmm.fit(distributions)
        action_probs = gmm.means_
    else:
        action_probs = distributions
        n_actions = len(distributions)
    
    action_probs = np.maximum(action_probs, 1e-8)
    action_probs = action_probs / action_probs.sum(axis=1, keepdims=True)
    
    def compute_cost_bounds(action_idx):
        wages_accepted = []
        wages_rejected = []
        
        for log in accepted_logs:
            w = np.array(log['Contract'])
            expected_utility = np.dot(action_probs[action_idx], w)
            wages_accepted.append(expected_utility)
        
        for log in rejected_logs:
            w = np.array(log['Contract'])
            expected_utility = np.dot(action_probs[action_idx], w)
            wages_rejected.append(expected_utility)
        
        min_accepted = min(wages_accepted) if wages_accepted else 0
        max_rejected = max(wages_rejected) if wages_rejected else 0
        
        return max(min_accepted - 1e-6, max_rejected + 1e-6, 0)
    
    costs = np.array([compute_cost_bounds(i) for i in range(n_actions)])
    
    def objective(params):
        n_vars = n_actions * (n_outcomes + 1)
        probs = params[:n_actions * n_outcomes].reshape(n_actions, n_outcomes)
        costs_opt = params[n_actions * n_outcomes:]
        
        penalty = 0
        for i, log in enumerate(content):
            w = np.array(log['Contract'])
            utilities = np.dot(probs, w) - costs_opt
            max_utility = np.max(utilities)
            
            if log['Agent Action'] == 1:
                if max_utility < -1e-6:
                    penalty += (max_utility + 1e-6) ** 2
            else:
                if max_utility > 1e-6:
                    penalty += (max_utility - 1e-6) ** 2
        
        regularization = 0.01 * np.sum((probs - action_probs) ** 2)
        return penalty + regularization
    
    def constraints(params):
        probs = params[:n_actions * n_outcomes].reshape(n_actions, n_outcomes)
        constraints_vals = []
        for i in range(n_actions):
            constraints_vals.append(np.sum(probs[i]) - 1)
        return np.array(constraints_vals)
    
    initial_params = np.concatenate([action_probs.flatten(), costs])
    bounds = [(1e-8, 1)] * (n_actions * n_outcomes) + [(0, None)] * n_actions
    
    constraint_dict = {'type': 'eq', 'fun': constraints}
    
    try:
        result = minimize(objective, initial_params, bounds=bounds, constraints=constraint_dict, method='SLSQP')
        if result.success:
            optimized_probs = result.x[:n_actions * n_outcomes].reshape(n_actions, n_outcomes)
            optimized_costs = result.x[n_actions * n_outcomes:]
        else:
            optimized_probs = action_probs
            optimized_costs = costs
    except:
        optimized_probs = action_probs
        optimized_costs = costs
    
    optimized_probs = np.maximum(optimized_probs, 1e-8)
    optimized_probs = optimized_probs / optimized_probs.sum(axis=1, keepdims=True)
    optimized_costs = np.maximum(optimized_costs, 0)
    
    agent_setting = np.hstack([optimized_probs, optimized_costs.reshape(-1, 1)])
    return agent_setting
```
