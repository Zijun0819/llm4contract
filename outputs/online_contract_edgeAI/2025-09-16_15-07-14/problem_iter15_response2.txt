```python
import numpy as np
from scipy.optimize import linprog
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import pairwise_distances_argmin_min
from scipy.spatial.distance import cdist

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    # Extract accepted and rejected logs
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]
    
    # If no accepted logs, return default agent setting
    if not accepted_logs:
        return np.hstack([np.eye(len(v)), np.zeros((len(v), 1))])
    
    # Helper function to solve mini LP for a given contract and principal utility
    def solve_mini_lp(w, u_principal):
        m = len(w)
        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1, u_principal])
        res = linprog(-w, A_eq=A_eq, b_eq=b_eq, bounds=[(0, 1)] * m, method='highs')
        return res.x if res.success else None
    
    # Generate probability candidates from accepted logs
    ps_candidates = []
    for log in accepted_logs:
        p_candidate = solve_mini_lp(np.array(log['Contract']), log['Principal Utility'])
        if p_candidate is not None:
            ps_candidates.append(p_candidate)
    
    # If no valid candidates, return default
    if not ps_candidates:
        return np.hstack([np.eye(len(v)), np.zeros((len(v), 1))])
    
    all_p = np.array(ps_candidates)
    
    # Determine number of clusters based on data size, with a cap
    n_clusters = min(10, max(1, len(all_p) // 5))
    
    # Use KMeans for initial clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(all_p)
    cluster_centers = kmeans.cluster_centers_
    cluster_assignments = kmeans.labels_
    
    # Calculate initial action costs with epsilon-tight bounds
    action_costs = np.zeros(n_clusters)
    for i in range(n_clusters):
        cluster_member_indices = np.where(cluster_assignments == i)[0]
        if cluster_member_indices.size > 0:
            wages = np.array([accepted_logs[j]['Contract'] for j in cluster_member_indices])
            utilities = np.sum(wages * cluster_centers[i], axis=1)
            action_costs[i] = np.min(utilities) - 1e-10
        else:
            action_costs[i] = 0.0
    
    # Adjust costs based on rejected logs with stricter IR
    if rejected_logs:
        rejected_wages = np.array([log['Contract'] for log in rejected_logs])
        rejected_utilities = rejected_wages @ cluster_centers.T
        max_rejected_utilities = np.max(rejected_utilities, axis=0)
        action_costs = np.maximum(action_costs, max_rejected_utilities + 1e-10)
    
    action_costs = np.maximum(action_costs, 0)
    
    # Refine with GMM for better probability modeling
    gmm = GaussianMixture(n_components=n_clusters, random_state=42)
    gmm.fit(all_p)
    gmm_centers = gmm.means_
    gmm_assignments = gmm.predict(all_p)
    
    # Recalibrate costs with GMM centers
    refined_costs = np.zeros(n_clusters)
    for i in range(n_clusters):
        cluster_member_indices = np.where(gmm_assignments == i)[0]
        if cluster_member_indices.size > 0:
            wages = np.array([accepted_logs[j]['Contract'] for j in cluster_member_indices])
            utilities = np.sum(wages * gmm_centers[i], axis=1)
            refined_costs[i] = np.min(utilities) - 1e-10
        else:
            refined_costs[i] = 0.0
    
    # Again adjust for rejected logs
    if rejected_logs:
        rejected_utilities_gmm = rejected_wages @ gmm_centers.T
        max_rejected_utilities_gmm = np.max(rejected_utilities_gmm, axis=0)
        refined_costs = np.maximum(refined_costs, max_rejected_utilities_gmm + 1e-10)
    
    refined_costs = np.maximum(refined_costs, 0)
    
    # Merge similar clusters to reduce redundancy
    final_centers = gmm_centers
    final_costs = refined_costs
    
    # Check if KMeans and GMM centers are similar, and merge if needed
    distances = cdist(cluster_centers, gmm_centers)
    min_distances = np.min(distances, axis=1)
    if np.mean(min_distances) < 0.001:
        combined_centers = np.vstack([cluster_centers, gmm_centers])
        combined_costs = np.hstack([action_costs, refined_costs])
        # Use KMeans to merge to n_clusters
        merge_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(combined_centers)
        final_centers = merge_kmeans.cluster_centers_
        final_costs = np.zeros(n_clusters)
        for i in range(n_clusters):
            member_indices = np.where(merge_kmeans.labels_ == i)[0]
            if member_indices.size > 0:
                final_costs[i] = np.min(combined_costs[member_indices])
            else:
                final_costs[i] = 0.0
    
    # Ensure probabilities are valid (non-negative and sum to 1)
    for i in range(len(final_centers)):
        final_centers[i] = np.maximum(final_centers[i], 0)
        final_centers[i] /= np.sum(final_centers[i])
    
    # Form the agent setting matrix
    agent_setting = np.hstack([final_centers, final_costs.reshape(-1, 1)])
    return agent_setting
```
