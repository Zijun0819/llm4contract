```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans
from scipy.optimize import minimize
from scipy.spatial.distance import cdist

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    n_actions = 10
    m_outcomes = v.shape[0]
    
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]
    
    if not accepted_logs:
        return np.column_stack((np.eye(m_outcomes), np.zeros(m_outcomes)))
    
    def compute_u0(w, p, c):
        return p @ w - c
    
    def infer_p_c_from_log(log):
        w = np.array(log['Contract'])
        u_principal = log['Principal Utility']
        
        def objective(x):
            p = x[:m_outcomes]
            c = x[-1]
            agent_utility = p @ w - c
            principal_utility = p @ (v - w)
            penalty = 100 * (np.sum(p) - 1)**2 + 100 * max(0, -agent_utility)**2
            return -principal_utility + penalty
        
        x0 = np.concatenate([np.ones(m_outcomes)/m_outcomes, [0.0]])
        bounds = [(0, 1) for _ in range(m_outcomes)] + [(0, None)]
        constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x[:m_outcomes]) - 1}]
        res = minimize(objective, x0, bounds=bounds, constraints=constraints, method='SLSQP')
        
        if res.success:
            p = res.x[:m_outcomes]
            c = res.x[-1]
            agent_utility = p @ w - c
            if agent_utility >= -1e-6:
                return p, c
        return None, None
    
    candidate_pc = []
    for log in accepted_logs:
        p, c = infer_p_c_from_log(log)
        if p is not None:
            candidate_pc.append((p, c))
    
    if not candidate_pc:
        candidate_pc = [(np.ones(m_outcomes)/m_outcomes, 0.0)]
    
    all_p = np.array([pc[0] for pc in candidate_pc])
    all_c = np.array([pc[1] for pc in candidate_pc])
    
    n_clusters = min(n_actions, len(all_p))
    kmeans_p = KMeans(n_clusters=n_clusters, random_state=0, n_init=10).fit(all_p)
    cluster_centers_p = kmeans_p.cluster_centers_
    
    cluster_costs = []
    for i in range(n_clusters):
        cluster_mask = kmeans_p.labels_ == i
        if np.any(cluster_mask):
            cluster_costs.append(np.max(all_c[cluster_mask]))
        else:
            cluster_costs.append(0.0)
    
    action_p = cluster_centers_p
    action_c = np.array(cluster_costs)
    
    for log in rejected_logs:
        w_rej = np.array(log['Contract'])
        agent_utils = action_p @ w_rej - action_c
        if np.any(agent_utils > 1e-6):
            mask = agent_utils > 1e-6
            action_c[mask] = np.maximum(action_c[mask], action_p[mask] @ w_rej)
    
    for log in accepted_logs:
        w_acc = np.array(log['Contract'])
        agent_utils = action_p @ w_acc - action_c
        max_util_idx = np.argmax(agent_utils)
        if agent_utils[max_util_idx] < -1e-6:
            action_c[max_util_idx] = min(action_c[max_util_idx], action_p[max_util_idx] @ w_acc)
    
    assignment = np.full(len(content), -1)
    for idx, log in enumerate(content):
        if log['Agent Action'] == 1:
            w = np.array(log['Contract'])
            agent_utils = action_p @ w - action_c
            assignment[idx] = np.argmax(agent_utils)
    
    for a in range(n_clusters):
        assigned_logs = [content[i] for i in np.where(assignment == a)[0] if content[i]['Agent Action'] == 1]
        if assigned_logs:
            min_utility = min([action_p[a] @ np.array(log['Contract']) for log in assigned_logs])
            action_c[a] = min(action_c[a], min_utility)
    
    refined_p = []
    refined_c = []
    for a in range(n_clusters):
        assigned_indices = np.where(assignment == a)[0]
        if len(assigned_indices) > 0:
            assigned_contracts = [np.array(content[i]['Contract']) for i in assigned_indices]
            weights = np.vstack(assigned_contracts).T
            
            def refine_objective(x):
                p = x[:m_outcomes]
                c = x[-1]
                total_error = 0
                for w in assigned_contracts:
                    agent_u = p @ w - c
                    total_error += max(0, -agent_u)**2
                total_error += 100 * (np.sum(p) - 1)**2
                return total_error
            
            x0 = np.concatenate([action_p[a], [action_c[a]]])
            bounds = [(0, 1) for _ in range(m_outcomes)] + [(0, None)]
            constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x[:m_outcomes]) - 1}]
            res = minimize(refine_objective, x0, bounds=bounds, constraints=constraints, method='SLSQP')
            
            if res.success:
                refined_p.append(res.x[:m_outcomes])
                refined_c.append(res.x[-1])
            else:
                refined_p.append(action_p[a])
                refined_c.append(action_c[a])
        else:
            refined_p.append(action_p[a])
            refined_c.append(action_c[a])
    
    agent_setting = np.column_stack((np.vstack(refined_p), np.array(refined_c)))
    return agent_setting
```
