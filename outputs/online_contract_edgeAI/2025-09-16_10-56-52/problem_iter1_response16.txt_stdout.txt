[*] Running ...
[*] Dataset loaded: E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\dataset\train_01.pkl with 3 instances.
→ Best principal utility: 0.0006193891649213826
→ The corresponding best action is: 6
→ The corresponding agent utility is: 1.000007097528427e-16
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.
  self.H.update(self.x - self.x_prev, self.g - self.g_prev)
D:\Software\anaconda3\envs\llm4contract\lib\site-packages\sklearn\cluster\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
Traceback (most recent call last):
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\eval.py", line 170, in <module>
    obj, _ = evaluate_llm((_p, _c, _v, _contract_logs))
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\eval.py", line 105, in evaluate_llm
    inferred_agent_setting = llm_agent_solver(v, content)
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\gpt.py", line 77, in agent_solver
    
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\gpt.py", line 74, in infer_costs
    contracts_by_action = [[] for _ in range(n_actions)]
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_minimize.py", line 738, in minimize
    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_lbfgsb_py.py", line 386, in _minimize_lbfgsb
    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_optimize.py", line 291, in _prepare_scalar_function
    sf = ScalarFunction(fun, x0, args, grad, hess,
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py", line 223, in __init__
    self._update_fun()
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py", line 295, in _update_fun
    fx = self._wrapped_fun(self.x)
  File "D:\Software\anaconda3\envs\llm4contract\lib\site-packages\scipy\optimize\_differentiable_functions.py", line 21, in wrapped
    fx = fun(np.copy(x), *args)
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\gpt.py", line 74, in <lambda>
    contracts_by_action = [[] for _ in range(n_actions)]
  File "E:\Coding\pythonProject\llm4contract\problems\online_contract_edgeAI\gpt.py", line 54, in objective
    utilities = action_centers @ w
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 12)
