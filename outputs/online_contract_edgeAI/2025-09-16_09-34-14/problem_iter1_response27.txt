```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    n_candidates = 10
    m_outcomes = v.shape[0]
    L = len(content)

    def mini_lp_p(w: np.ndarray, u: float) -> np.ndarray | None:
        m = len(w)
        A_eq = [np.ones(m), v - w]
        b_eq = [1.0, u]
        bounds = [(0, 1)] * m
        res = linprog(w, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        return res.x if res.success else None

    candidate_ps = []
    for log in content:
        if log['Agent Action'] == 1:
            w_i = log['Contract']
            u_i = log['Principal Utility']
            p_i = mini_lp_p(w_i, u_i)
            if p_i is not None:
                candidate_ps.append(p_i)

    if not candidate_ps:
        raise ValueError("No valid accepted logs to infer agent strategies.")

    all_p = np.array(candidate_ps)

    kmeans = KMeans(n_clusters=n_candidates, random_state=0, n_init=10).fit(all_p)
    p0 = kmeans.cluster_centers_

    assigns = np.full(L, -1, dtype=int)
    for i, log in enumerate(content):
        if log['Agent Action'] == 1:
            w = log['Contract']
            assigns[i] = int(np.argmax(p0 @ w))

    c_ir = np.zeros(n_candidates)
    for a in range(n_candidates):
        idx = np.where(assigns == a)[0]
        if idx.size > 0:
            wages = np.array([content[i]['Contract'] for i in idx]).T
            c_ir[a] = p0[a] @ wages.min(axis=1)
        else:
            c_ir[a] = 0.0

    rej_idx = [i for i, log in enumerate(content) if log['Agent Action'] == -1]
    if rej_idx:
        wages_rej = np.array([content[i]['Contract'] for i in rej_idx]).T
        rej_utils = p0 @ wages_rej
        c_rej = rej_utils.max(axis=1)
    else:
        c_rej = np.zeros(n_candidates)

    c_init = np.maximum(c_ir, c_rej)
    agent_setting = np.hstack([p0, c_init[:, np.newaxis]])
    return agent_setting


def adjust_agent_settingcontenido(agent_setting: np.ndarray) -> np.ndarray:
    n_actions = agent_setting.shape[0]
   contenido_ajustado = np.zeros_like(agent_setting)
    for a in range(n_actions):
        if agent_setting[a, -1] < 0:
            contenido_ajustado[a, :] = agent_setting[a, :]
            contenido_ajustado[a, -1] = 0
        else:
            contenido_ajustado[a, :] = agent_setting[a, :]
    return contenido_ajustado


def infer_agent_utility_settingscontenido(v: np.ndarray, content: list[dict]) -> np.ndarray:
    agent_setting = agent_solver_v2(v, content)
    adjusted_setting = adjust_agent_settingcontenido(agent![_contenido=agent_setting)
    return adjusted_setting


def _compute_optimal_actioncontenido(w: np.ndarray, p: np.ndarray, c: float) -> float:
    utility = w @ p - c
    return utility


def _match_actioncontenido_with_wage(w: np.ndarray, actions: np.ndarray) -> int:
    utils = [_compute_optimal_actioncontenido(w, actions[i, :-1], actions[i, -1]) for i in range(actions.shape[0])]
    matched_action_idx = np.argmax(utils)
    return matched_action_idx


def _predict_agent_responsecontenido(w: np.ndarray, actions: np.ndarray) -> int:
    matched_action_idx = _match_actioncontenido_with_wage(w, actions)
    return matched_action_idx


def _get_label_from_responsecontenido(response: int) -> int:
    if response == -1:
        return -1
    else:
        return 1


def _metric_label_matchingcontenido(content: list[dict], actions: np.ndarray) -> float:
    correct_predictions = 0
    for log in content:
        w = log['Contract']
        matched_action_idx = _match_actioncontenido_with_wage(w, actions)
        if log['Agent Action'] == _get_label_from_responsecontenido(matched_action_idx):
            correct_predictions += 1
    accuracy = correct_predictions / len(content)
    return accuracy


def infercontenido_and_compute_accuracy(v: np.ndarray, content: list[dict]) -> tuple[np.ndarray, float]:
    agent_setting = infer_agent_utility_settingscontenido(v, content)
    accuracy = _metric_label_matchingcontenido(content, agent_setting)
    return agent_setting, accuracy


def cross_validatecontenido(v: np.ndarray, content: list[dict], n_folds: int = 5) -> tuple[np.ndarray, float]:
    from sklearn.model_selection import KFold
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)
    total_accuracy = 0
    total_settings = None
    for train_idx, val_idx in kf.split(content):
        train_content, val_content = [content[i] for i in train_idx], [content[i] for i in val_idx]
        val_content_array = np.array(val_content)
        agent_setting, accuracy = infercontenido_and_compute_accuracy(v, train_content)
        total_accuracy += accuracy
        if total_settings is None:
            total_settings = agent_setting
        else:
            total_settings += agent_setting
    average_accuracy = total_accuracy / n_folds
    average_setting = total_settings / n_folds
    return average_setting, average_accuracy
```
