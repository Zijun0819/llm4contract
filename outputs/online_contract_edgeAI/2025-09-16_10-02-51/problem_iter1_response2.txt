```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog, minimize
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
import warnings
warnings.filterwarnings('ignore')

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    m_outcomes = v.shape[0]
    L = len(content)
    n_auto = min(10, max(2, L // 5))

    def mini_lp_p(w: np.ndarray, u: float) -> np.ndarray | None:
        m = len(w)
        A_eq = [np.ones(m), v - w]
        b_eq = [1.0, u]
        bounds = [(0, 1)] * m
        res = linprog(w, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        return res.x if res.success else None

    candidate_ps = []
    for log in content:
        if log['Agent Action'] == 1:
            w_i = log['Contract']
            u_i = log['Principal Utility']
            p_i = mini_lp_p(w_i, u_i)
            if p_i is not None:
                candidate_ps.append(p_i)

    if not candidate_ps:
        candidate_ps = [np.ones(m_outcomes) / m_outcomes]

    all_p = np.array(candidate_ps)
    
    if len(all_p) < n_auto:
        p0 = np.vstack([all_p, np.ones((n_auto - len(all_p), m_outcomes)) / m_outcomes])
    else:
        kmeans = KMeans(n_clusters=n_auto, random_state=0, n_init=10).fit(all_p)
        p0 = kmeans.cluster_centers_

    def agent_utility(p, c, w):
        return p @ w - c

    assigns = np.full(L, -1, dtype=int)
    utilities = np.zeros(L)
    for i, log in enumerate(content):
        w = np.array(log['Contract'])
        utils = p0 @ w
        best_util = np.max(utils)
        assigns[i] = np.argmax(utils)
        utilities[i] = best_util

    def objective(c):
        penalty = 0.0
        for i, log in enumerate(content):
            w = np.array(log['Contract'])
            best_util = np.max(p0 @ w - c)
            if log['Agent Action'] == 1 and best_util < 0:
                penalty += 1000 * abs(best_util)
            elif log['Agent Action'] == -1 and best_util >= 0:
                penalty += 1000 * abs(best_util)
        return np.sum(c) + penalty

    initial_c = np.random.uniform(0, 1, n_auto)
    bounds = [(0, None)] * n_auto
    result = minimize(objective, initial_c, bounds=bounds, method='SLSQP', options={'maxiter': 1000})
    c_opt = np.clip(result.x, 0, None)

    agent_setting = np.hstack([p0, c_opt[:, np.newaxis]])
    return agent_setting
```
