```python
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (actions' outcome distributions and costs) consistent with
    historical interaction logs: contracts, principal utilities, and agent accept/reject actions.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,).
        content (list[dict]): Historical logs, each dict with keys:
            'Contract' (list/np.ndarray of 5 payments),
            'Principal Utility' (float, zero if agent rejects),
            'Agent Action' (1 accept, -1 reject).

    Returns:
        np.ndarray: n x 6 matrix (n actions), columns:
            first 5: outcome distributions (sum to 1),
            last: nonnegative agent cost.
    """
    m = len(v)  # outcomes = 5
    logs_df = pd.DataFrame(content)
    L = len(logs_df)

    # Extract arrays for convenience
    contracts = np.vstack(logs_df['Contract'].to_numpy())  # shape (L,5)
    principal_utils = logs_df['Principal Utility'].to_numpy()
    agent_actions = logs_df['Agent Action'].to_numpy()

    # We want to find n actions (n adaptive)
    # Step 1: Filter accepted logs (agent_actions==1) with positive principal utility
    accepted_idx = np.where(agent_actions == 1)[0]
    accepted_contracts = contracts[accepted_idx]
    accepted_utils = principal_utils[accepted_idx]

    if len(accepted_idx) == 0:
        # No accepted logs, trivial agent setting: one action with uniform dist and zero cost
        p = np.ones((1, m)) / m
        c = np.array([0.0])
        return np.hstack([p, c[:, None]])

    # Step 2: For each accepted log, find an agent outcome distribution p_i explaining agent's acceptance
    # We solve for p_i: p_i @ w_i - c_i >= 0 (agent accepts)
    # We only have contract w_i and principal utility u_i = p_i @ v - payment cost
    # We approximate p_i by solving a LP that finds a distribution p close to maximizing p @ w_i

    # Custom LP: maximize p @ w_i s.t. sum p = 1, p>=0, and p @ v = u_i + c_i (unknown)
    # Since c_i unknown, relax constraint to find p maximizing p @ w_i subject to sum p=1

    # We'll just collect accepted contracts' normalized payments as proxies for p's

    # Normalize accepted contracts by their max component to reduce scale variance
    norm_contracts = accepted_contracts / (accepted_contracts.max(axis=1, keepdims=True) + 1e-8)

    # Step 3: Cluster these normalized contracts into candidate agent outcome distributions
    # Number of clusters n_candidates adaptive by log size, max 10
    n_candidates = min(max(3, len(accepted_idx) // 10), 10)
    kmeans = KMeans(n_clusters=n_candidates, random_state=42, n_init=15)
    cluster_centers = kmeans.fit(norm_contracts).cluster_centers_

    # Project cluster centers to simplex (ensure nonnegative, sum=1)
    def project_simplex(y):
        """Projection onto probability simplex (sum=1, all >=0)"""
        u = np.sort(y)[::-1]
        cssv = np.cumsum(u)
        rho = np.where(u + (1 - cssv) / np.arange(1, len(y)+1) > 0)[0][-1]
        theta = (cssv[rho] - 1) / (rho + 1)
        return np.maximum(y - theta, 0)

    p_candidates = np.array([project_simplex(center) for center in cluster_centers])

    # Step 4: For each accepted log, assign to closest candidate action by max inner product with contract
    assigns = np.full(L, -1, dtype=int)
    for i in accepted_idx:
        w = contracts[i]
        # Select action a maximizing p_a @ w
        scores = p_candidates @ w
        assigns[i] = int(np.argmax(scores))

    # Step 5: Infer cost c for each action to satisfy agent IR and IC and log consistency
    # IR: For accepted logs assigned to action a: p_a @ w - c_a >= 0
    # For rejected logs: For all actions a: p_a @ w - c_a < 0
    # We estimate c_a by max_{accepted_i in a} p_a @ w_i (lowest c satisfying IR)
    # and also ensure c_a > max_{rejected_j} p_a @ w_j (for IC to reject those contracts)

    c_ir = np.zeros(n_candidates)
    for a in range(n_candidates):
        accepted_in_a = np.where(assigns == a)[0]
        if len(accepted_in_a) > 0:
            vals = (p_candidates[a] @ contracts[accepted_in_a].T)
            c_ir[a] = vals.min()  # minimal utility agent enjoys on accepted contracts
        else:
            c_ir[a] = 0.0

    # Rejected logs must have negative utility for all actions
    rejected_idx = np.where(agent_actions == -1)[0]
    if len(rejected_idx) > 0:
        rejected_contracts = contracts[rejected_idx]
        # For each action a, max agent utility over rejected contracts
        rej_utils = np.array([(p_candidates[a] @ rejected_contracts.T).max() for a in range(n_candidates)])
    else:
        rej_utils = np.zeros(n_candidates)

    # Final cost c_a >= max(IR bound, rejection bound + epsilon)
    epsilon = 1e-5
    c_final = np.maximum(c_ir, rej_utils + epsilon)
    c_final = np.clip(c_final, 0, None)  # Non-negative cost

    # Step 6: Validate distributions (numerical tolerance)
    for i in range(n_candidates):
        p_candidates[i] = np.clip(p_candidates[i], 0, None)
        p_candidates[i] /= p_candidates[i].sum() + 1e-12

    # Step 7: Return agent setting matrix: n_candidates x 6 (5 outcomes + cost)
    agent_setting = np.hstack([p_candidates, c_final[:, None]])

    return agent_setting
```
