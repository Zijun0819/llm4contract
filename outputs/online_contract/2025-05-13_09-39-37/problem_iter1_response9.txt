```python
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (n_actions x 6) explaining all logs.
    Columns: first 5 are outcome distributions, last is agent cost.
    """
    m = len(v)
    L = len(content)

    contracts = np.array([log['Contract'] for log in content])
    actions = np.array([log['Agent Action'] for log in content])
    principal_utils = np.array([log['Principal Utility'] for log in content])

    # Step 1: Separate accepted and rejected logs
    accept_idxs = np.where(actions == 1)[0]
    reject_idxs = np.where(actions == -1)[0]

    # If no accepted logs, fallback: single uniform outcome with zero cost
    if len(accept_idxs) == 0:
        p_uniform = np.ones(m) / m
        c_zero = 0.0
        return np.hstack([p_uniform.reshape(1, -1), np.array([[c_zero]])])

    # Step 2: From accepted logs, infer empirical expected outcome distributions 
    # by solving small LPs for each contract with agent utility >=0
    #
    # The agent maximizes expected payment - cost, and IR: EU >= cost >= 0
    # We know Principal Utility = v @ p - payment @ p
    # But payments are given, so agent utility depends on cost and p.
    #
    # To infer p for each accepted contract, we try to find p s.t:
    # p @ contract - cost >= 0 (agent accepts)
    # principal utility = v @ p - payment @ p
    # Since cost unknown, we just infer p that explains contract and utility.

    def infer_p(contract, principal_utility):
        # Solve for p to satisfy:
        # sum p = 1, p >=0
        # principal_utility = v@p - contract@p => (v - contract)@p = principal_utility
        # p in simplex
        A_eq = np.vstack([np.ones(m), v - contract])
        b_eq = np.array([1.0, principal_utility])
        bounds = [(0, 1) for _ in range(m)]
        # Minimize 0 (feasibility)
        res = linprog(c=np.zeros(m), A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            return res.x
        return None

    inferred_ps = []
    for i in accept_idxs:
        p_ = infer_p(contracts[i], principal_utils[i])
        if p_ is not None:
            inferred_ps.append(p_)
    inferred_ps = np.array(inferred_ps)
    if len(inferred_ps) == 0:
        # Failed to infer any distribution from accepted logs, fallback uniform
        p_uniform = np.ones(m) / m
        c_zero = 0.0
        return np.hstack([p_uniform.reshape(1, -1), np.array([[c_zero]])])

    # Step 3: Cluster inferred p to get candidate agent actions
    # Pick n_actions adaptively: between 4 and 10, using elbow heuristic (silhouette or inertia)
    best_n = 4
    best_inertia = np.inf
    for n_cand in range(4, 11):
        kmeans = KMeans(n_clusters=n_cand, random_state=42, n_init=10).fit(inferred_ps)
        if kmeans.inertia_ < best_inertia:
            best_inertia = kmeans.inertia_
            best_n = n_cand
            best_centers = kmeans.cluster_centers_
            best_labels = kmeans.labels_

    p_actions = best_centers
    n_actions = p_actions.shape[0]

    # Step 4: For each action, estimate minimal agent cost consistent with IR on accepted logs assigned to it
    c_actions = np.zeros(n_actions)
    # For each action a, cost <= expected payment of contract that agent accepts with that action.
    # agent utility = expected payment - cost >=0 => cost <= expected payment
    for a in range(n_actions):
        assigned_idx = accept_idxs[best_labels == a]
        if len(assigned_idx) == 0:
            c_actions[a] = 0.0
            continue
        # For each assigned accepted contract, expected payment for p_actions[a]
        exp_payments = contracts[assigned_idx] @ p_actions[a]
        # Maximal cost consistent with IR is minimal expected payment
        c_actions[a] = np.min(exp_payments)

    # Step 5: IC constraints to ensure agent picks best action for each accepted contract
    # For each accepted contract i, agent's payoff for chosen action a_i >= payoff for other actions a_j
    # payoff(i,a) = contract_i @ p_actions[a] - c_actions[a]
    # We enforce this by adjusting costs c_actions upward to satisfy IC

    # We iteratively enforce IC constraints by adjusting costs upward
    max_iters = 100
    for _ in range(max_iters):
        violated = False
        for i in accept_idxs:
            contract_i = contracts[i]
            # agent's chosen action label:
            a_i = best_labels[np.where(accept_idxs == i)[0][0]]
            payoff_i = contract_i @ p_actions[a_i] - c_actions[a_i]
            for a_j in range(n_actions):
                if a_j == a_i:
                    continue
                payoff_j = contract_i @ p_actions[a_j] - c_actions[a_j]
                if payoff_j > payoff_i + 1e-8:  # violation
                    # increase c_actions[a_j] to fix:
                    diff = payoff_j - payoff_i
                    c_actions[a_j] += diff
                    violated = True
        if not violated:
            break

    # Step 6: Rejection consistency
    # For each rejected contract j:
    # agent payoff for all actions < 0 (otherwise agent would accept)
    # So max_{a} contract_j @ p_actions[a] - c_actions[a] < 0
    # If violated, increase costs minimally to satisfy rejection IR

    for i in reject_idxs:
        contract_r = contracts[i]
        payoffs_r = contract_r @ p_actions - c_actions
        max_payoff = np.max(payoffs_r)
        if max_payoff >= 0:
            # Need to increase costs of actions with max payoff to fix
            # Increase cost of action(s) with payoff max_payoff by at least max_payoff + epsilon
            idxs = np.where(np.abs(payoffs_r - max_payoff) < 1e-8)[0]
            increment = max_payoff + 1e-6
            for idx in idxs:
                c_actions[idx] += increment

    # Step 7: Ensure costs nonnegative
    c_actions = np.maximum(c_actions, 0.0)

    # Step 8: Normalize p_actions to ensure valid distributions (sum=1, >=0)
    p_actions = np.clip(p_actions, 0, None)
    p_actions = p_actions / p_actions.sum(axis=1, keepdims=True)

    # Compose agent setting matrix
    agent_setting = np.hstack([p_actions, c_actions.reshape(-1,1)])

    return agent_setting
```
