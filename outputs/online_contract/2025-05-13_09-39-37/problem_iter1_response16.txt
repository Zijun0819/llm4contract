```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (actions: outcome distributions + costs)
    consistent with historical logs of contracts, principal utilities,
    and agent acceptance/rejection decisions.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Historical logs, each dict with keys:
            - 'Contract': 5-dim list/array of payments
            - 'Principal Utility': float, zero if rejected
            - 'Agent Action': 1 (accept) or -1 (reject)

    Returns:
        np.ndarray: n x 6 matrix, each row is an action:
            - first 5 columns: outcome probabilities (sum to 1)
            - last column: non-negative cost
    """
    m = len(v)  # Number of outcomes (5)
    logs_df = pd.DataFrame(content)

    # Extract contracts/payments as numpy array, shape (L,5)
    contracts = np.vstack(logs_df['Contract'].values)
    actions = logs_df['Agent Action'].values
    principals_utils = logs_df['Principal Utility'].values

    L = len(content)

    # Step 0: Preprocessing
    # Separate accepted and rejected logs
    accepted_idx = np.where(actions == 1)[0]
    rejected_idx = np.where(actions == -1)[0]

    # If no accepted logs, trivial action: uniform distribution + zero cost
    if len(accepted_idx) == 0:
        p_uniform = np.ones(m) / m
        cost_zero = 0.0
        return np.hstack([p_uniform, cost_zero])[np.newaxis, :]

    # Step 1: Infer candidate outcome distributions from accepted logs
    # For each accepted log: Solve LP to find p that satisfies
    #   p . w_i = principal utility_i + agent cost for the action (unknown)
    # Since cost unknown, approximate p by maximizing likelihood that p @ w_i approximates principal utility

    # We form a feasibility LP for each accepted log:
    # p in simplex
    # p @ w_i = principal utility_i + cost (unknown)
    # Instead, we use p @ w_i ≈ principal utility_i + cost_i, but cost_i unknown
    # To get candidate p_i, ignore cost_i momentarily by normalizing

    candidate_ps = []
    for i in accepted_idx:
        w_i = contracts[i]
        u_i = principals_utils[i]
        # We want to find p_i s.t. p_i @ w_i >= u_i (agent utility ≥ 0)
        # with p_i in simplex.
        # Form LP: minimize 0 subject to p_i @ w_i >= u_i, sum p_i=1, p_i≥0

        # Use LP to find feasible p_i with minimal l1 distance to uniform prob
        c_obj = np.zeros(m)
        A_ub = np.array([-w_i])
        b_ub = np.array([-u_i])
        A_eq = np.ones((1, m))
        b_eq = np.array([1.0])
        bounds = [(0, 1)] * m
        res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method="highs")
        if res.success:
            p_i = res.x
            candidate_ps.append(p_i)
        else:
            # fallback: uniform distribution (unlikely)
            candidate_ps.append(np.ones(m) / m)

    candidate_ps = np.array(candidate_ps)

    # Step 2: Cluster candidate outcome distributions to form actions
    # Use Agglomerative Clustering to adaptively choose number of actions
    # We pick number of clusters by silhouette score or fixed max 10 clusters

    from sklearn.metrics import silhouette_score

    max_clusters = min(10, len(candidate_ps))
    best_k = 1
    best_score = -1
    for k in range(2, max_clusters + 1):
        clustering = AgglomerativeClustering(n_clusters=k).fit(candidate_ps)
        labels = clustering.labels_
        try:
            score = silhouette_score(candidate_ps, labels)
        except:
            score = -1
        if score > best_score:
            best_score = score
            best_k = k

    clustering = AgglomerativeClustering(n_clusters=best_k).fit(candidate_ps)
    labels = clustering.labels_

    # Step 3: For each cluster, compute centroid p_action
    p_actions = np.zeros((best_k, m))
    for i in range(best_k):
        cluster_ps = candidate_ps[labels == i]
        if len(cluster_ps) == 0:
            p_actions[i] = np.ones(m) / m
        else:
            p_actions[i] = cluster_ps.mean(axis=0)
        # Ensure valid probability simplex by projection
        p_actions[i] = np.maximum(p_actions[i], 0)
        p_actions[i] /= p_actions[i].sum()

    # Step 4: Estimate agent cost for each action to satisfy IR and IC
    # IR: for accepted contracts assigned to this action, agent utility ≥ 0:
    # p_a @ w - c_a ≥ 0  =>  c_a ≤ min_i p_a @ w_i for accepted logs assigned to a
    # IC: for rejected contracts, agent utility < 0:
    # p_a @ w_j - c_a < 0  =>  c_a > max_j p_a @ w_j for rejected logs

    # Assign accepted logs p_i to closest action (min L2 distance)
    assigned_actions = np.zeros(len(accepted_idx), dtype=int)
    for idx_i, i in enumerate(accepted_idx):
        p_i = candidate_ps[idx_i]
        dist = np.linalg.norm(p_actions - p_i, axis=1)
        assigned_actions[idx_i] = np.argmin(dist)

    # Compute IR upper bounds for costs
    c_ub = np.full(best_k, np.inf)
    for a in range(best_k):
        inds = np.where(assigned_actions == a)[0]
        if len(inds) == 0:
            # No accepted logs assigned: cost can be zero
            c_ub[a] = 0.0
        else:
            w_accepted = contracts[accepted_idx[inds]]
            vals = w_accepted @ p_actions[a]
            c_ub[a] = vals.min()  # max cost to keep IR

    # Compute IC lower bounds for costs (rejection consistency)
    # Agent must prefer other actions or reject contract
    # For rejected logs: agent utility < 0 for all actions
    if len(rejected_idx) > 0:
        w_rejected = contracts[rejected_idx]
        # max agent utility over rejected contracts for each action
        vals_rej = w_rejected @ p_actions.T  # shape (rej_count, num_actions)
        # For each action, agent utility must be < cost => cost > max agent util for rejected contracts
        c_lb = vals_rej.max(axis=0) + 1e-8
        c_lb = np.maximum(c_lb, 0)
    else:
        c_lb = np.zeros(best_k)

    # Step 5: Final costs: must satisfy c_lb < c ≤ c_ub, pick feasible cost in interval
    c_final = np.zeros(best_k)
    for a in range(best_k):
        low = c_lb[a]
        high = c_ub[a]
        if high < low:
            # Conflict, relax by setting cost = max(low,0)
            c_final[a] = max(low, 0)
        else:
            # Pick cost slightly above lower bound to ensure rejection consistency
            c_final[a] = low + 1e-4

    # Step 6: Verify IR and IC constraints for accepted and rejected logs:
    # If violated, adjust costs upward minimally to fix
    # Iterate few times for convergence
    for _ in range(3):
        # IR check
        for a in range(best_k):
            inds = np.where(assigned_actions == a)[0]
            if len(inds) == 0:
                continue
            w_accepted = contracts[accepted_idx[inds]]
            agent_utils = w_accepted @ p_actions[a] - c_final[a]
            min_util = agent_utils.min()
            if min_util < -1e-8:
                c_final[a] -= min_util  # increase cost to restore IR

        # IC check on rejected logs
        if len(rejected_idx) > 0:
            w_rejected = contracts[rejected_idx]
            for a in range(best_k):
                agent_utils_rej = w_rejected @ p_actions[a] - c_final[a]
                max_util = agent_utils_rej.max()
                if max_util >= 0:
                    c_final[a] += max_util + 1e-6  # increase cost to restore IC

    # Step 7: Assemble final agent setting matrix: n x 6
    agent_setting = np.hstack([p_actions, c_final[:, None]])

    # Final validation: probabilities sum to 1 and costs non-negative
    agent_setting[:, :m] = np.clip(agent_setting[:, :m], 0, 1)
    agent_setting[:, :m] /= agent_setting[:, :m].sum(axis=1, keepdims=True)
    agent_setting[:, -1] = np.maximum(agent_setting[:, -1], 0)

    return agent_setting
```
