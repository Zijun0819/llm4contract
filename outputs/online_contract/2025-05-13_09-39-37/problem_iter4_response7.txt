```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by clustering accepted logs' inferred outcome distributions and inferring costs
    to satisfy IR and IC constraints for accepted and rejected contracts.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains:
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (â‰¥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform dist and zero cost
    if accepted.empty:
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.zeros((1, 1))])

    # Step 1: Infer p for each accepted contract by LP
    # p: distribution over outcomes for agent action that accepted contract corresponds to
    def infer_p_for_log(w, u_p):
        # Solve LP:
        # variables: p (length m_outcomes)
        # constraints:
        #   sum p = 1
        #   p @ (v - w) = u_p
        # bounds: p_i in [0,1]
        # objective: maximize p @ w (agent expected payment)
        c_obj = -np.array(w)
        A_eq = [np.ones(m_outcomes), v - np.array(w)]
        b_eq = [1.0, u_p]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical fix: clip and re-normalize
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p
        else:
            # fallback: uniform distribution
            return np.ones(m_outcomes) / m_outcomes

    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, m_outcomes)

    # Step 2: Cluster p_array adaptively using DBSCAN with cosine metric
    # Tune eps by silhouette score over candidates
    eps_candidates = np.linspace(0.05, 0.3, 6)
    best_eps = None
    best_labels = None
    best_silhouette = -1

    for eps in eps_candidates:
        clustering = DBSCAN(eps=eps, min_samples=2, metric='cosine').fit(p_array)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters >= 2:
            # silhouette_score requires no noise points, filter them out
            mask = labels != -1
            if np.sum(mask) >= 2:  # silhouette needs at least 2 samples
                try:
                    sil = silhouette_score(p_array[mask], labels[mask], metric='cosine')
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_eps = eps
                        best_labels = labels.copy()
                except Exception:
                    pass
        elif n_clusters == 1 and -1 not in labels:
            # single cluster with no noise, silhouette undefined but acceptable
            best_eps = eps
            best_labels = labels.copy()
            best_silhouette = 0.0

    if best_labels is None:
        # fallback: assign all to one cluster
        best_labels = np.zeros(len(p_array), dtype=int)

    labels = best_labels
    noise_idx = np.where(labels == -1)[0]
    assigned_idx = np.where(labels != -1)[0]
    unique_labels = sorted(set(labels) - {-1})
    n_actions = len(unique_labels)
    if n_actions == 0:
        # all noise fallback
        n_actions = 1
        labels[:] = 0
        unique_labels = [0]

    # Step 3: Compute cluster centers (mean p per cluster)
    centers = np.zeros((n_actions, m_outcomes))
    for i, lab in enumerate(unique_labels):
        centers[i] = p_array[labels == lab].mean(axis=0)
    # Normalize centers to sum to 1 and clip negatives
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)

    # Assign noise points to nearest cluster center by cosine similarity
    if noise_idx.size > 0:
        from sklearn.metrics.pairwise import cosine_similarity
        noise_p = p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # shape (len(noise_idx), n_actions)
        assign_labels = sim.argmax(axis=1)
        for idx, a_lab in zip(noise_idx, assign_labels):
            labels[idx] = unique_labels[a_lab]

    # Map all labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    # Step 4: Infer minimal costs c_a satisfying IR and IC constraints
    # IR: For accepted contracts assigned to action a: p_a @ w - c_a >= 0 => c_a <= min p_a @ w
    # IC: For rejected contracts: for all a: p_a @ w - c_a < 0 => c_a > max p_a @ w among rejects
    eps_cost = 1e-8
    contract_acc = np.array(accepted['Contract'].tolist())
    contract_rej = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))

    costs = np.zeros(n_actions)
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(mapped_labels == a)[0]
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0

        if contract_rej.shape[0] > 0:
            rej_vals = contract_rej @ p_a
            max_rej = np.max(rej_vals)
        else:
            max_rej = -np.inf

        cost_a = max(min_acc, max_rej + eps_cost)
        costs[a] = max(cost_a, 0.0)

    # Step 5: Validate and refine costs iteratively to ensure all IR and IC hold
    max_iter = 20
    for _ in range(max_iter):
        # Accepted logs: must have some action a with p_a @ w - c_a >= 0
        accept_utils = contract_acc @ centers.T - costs  # shape (n_accept, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected logs: must have all actions a with p_a @ w - c_a < 0
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # shape (n_reject, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs slightly to enforce constraints
        costs += eps_cost * 10
        costs = np.maximum(costs, 0.0)

    # Final normalization for numerical stability
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
