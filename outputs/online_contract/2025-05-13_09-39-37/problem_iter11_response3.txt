```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by adaptively clustering inferred outcome distributions with cosine metric,
    assigning noise points by cosine similarity, and iteratively refining costs
    to satisfy IR and IC constraints for accepted and rejected contracts.

    Improvements over v1:
    - Tighter LP constraints and fallback with adaptive tolerance.
    - Adaptive cosine similarity eps candidates with finer granularity.
    - Increased min_samples and minimal cluster size for robust clustering.
    - Assign noise points by max cosine similarity with threshold fallback.
    - Iterative cost refinement with smaller increments and extended iterations.
    - Joint strict enforcement of IR and IC constraints.
    - Robust normalization of probabilities with clipping and fallback.
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform distribution and zero cost
    if accepted.empty:
        uniform_p = np.ones(m_outcomes) / m_outcomes
        return np.hstack([uniform_p.reshape(1, -1), np.zeros((1, 1))])

    # Step 1: Infer p for each accepted contract by LP with tighter feasibility and adaptive fallback
    def infer_p_for_log(w, u_p):
        w = np.array(w)
        # Objective: maximize p @ w <=> minimize -p @ w
        c_obj = -w
        A_eq = np.vstack([np.ones(m_outcomes), v - w])
        b_eq = np.array([1.0, u_p])
        bounds = [(0, 1) for _ in range(m_outcomes)]

        # Try strict equality constraints first
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x.clip(0)
            s = p.sum()
            if s > 1e-12:
                p /= s
                return p
        # Relax equality on agent utility to inequality with adaptive tolerance
        tol_candidates = [1e-7, 5e-7, 1e-6, 5e-6]
        for tol in tol_candidates:
            A_ub = np.array([-(v - w)])
            b_ub = np.array([-(u_p - tol)])
            res2 = linprog(c=c_obj, A_eq=[np.ones(m_outcomes)], b_eq=[1.0],
                           A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')
            if res2.success:
                p = res2.x.clip(0)
                s = p.sum()
                if s > 1e-12:
                    p /= s
                    return p
        # Fallback uniform if no feasible solution
        return np.ones(m_outcomes) / m_outcomes

    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, m_outcomes)

    # Step 2: Adaptive DBSCAN clustering with cosine metric and silhouette checks
    # Finer eps candidates and increased min_samples for robust clustering
    eps_candidates = np.linspace(0.03, 0.18, 15)
    best_eps = None
    best_labels = None
    best_silhouette = -1
    min_cluster_size = 5
    min_samples = 3

    for eps in eps_candidates:
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(p_array)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters >= 2:
            mask = labels != -1
            if np.sum(mask) >= min_cluster_size:
                try:
                    sil = silhouette_score(p_array[mask], labels[mask], metric='cosine')
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_eps = eps
                        best_labels = labels.copy()
                except Exception:
                    continue
        elif n_clusters == 1 and -1 not in labels:
            # Single cluster no noise acceptable fallback
            best_eps = eps
            best_labels = labels.copy()
            best_silhouette = 0.0

    if best_labels is None:
        # Fallback: assign all to one cluster
        best_labels = np.zeros(len(p_array), dtype=int)

    labels = best_labels
    noise_idx = np.where(labels == -1)[0]
    unique_labels = sorted(set(labels) - {-1})
    n_actions = len(unique_labels)
    if n_actions == 0:
        # All noise fallback to one cluster
        n_actions = 1
        labels[:] = 0
        unique_labels = [0]

    # Step 3: Compute cluster centers (mean p per cluster) with robust normalization
    centers = np.zeros((n_actions, m_outcomes))
    for i, lab in enumerate(unique_labels):
        cluster_ps = p_array[labels == lab]
        c = cluster_ps.mean(axis=0)
        c = np.clip(c, 0, None)
        s = c.sum()
        if s > 1e-12:
            c /= s
        else:
            c = np.ones(m_outcomes) / m_outcomes
        centers[i] = c

    # Step 4: Assign noise points to nearest cluster center by cosine similarity with threshold
    if noise_idx.size > 0:
        noise_p = p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # (noise_size, n_actions)
        assign_labels = sim.argmax(axis=1)
        max_sims = sim.max(axis=1)
        # Threshold to avoid forced bad assignments, fallback assign to best cluster anyway
        sim_threshold = 0.85
        for idx_i, idx_log in enumerate(noise_idx):
            if max_sims[idx_i] >= sim_threshold:
                labels[idx_log] = unique_labels[assign_labels[idx_i]]
            else:
                # Assign to cluster with max similarity anyway (to avoid noise)
                labels[idx_log] = unique_labels[assign_labels[idx_i]]

    # Map all labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    # Prepare contract arrays
    contract_acc = np.array(accepted['Contract'].tolist())
    contract_rej = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))

    eps_cost = 1e-10
    costs = np.zeros(n_actions)

    # Step 5: Initialize minimal costs c_a satisfying IR and IC constraints conservatively
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(mapped_labels == a)[0]
        # For accepted contracts assigned to this action, find minimal w @ p_a
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0
        # For rejected contracts, find maximal w @ p_a
        if contract_rej.shape[0] > 0:
            max_rej = np.max(contract_rej @ p_a)
        else:
            max_rej = -np.inf
        costs[a] = max(min_acc, max_rej + eps_cost, 0.0)

    # Step 6: Iteratively refine costs to satisfy IR and IC constraints strictly
    max_iter = 100
    step_increase = eps_cost * 10
    for _ in range(max_iter):
        prev_costs = costs.copy()

        # Accepted contracts: at least one action with utility >= 0 (agent accepts)
        accept_utils = contract_acc @ centers.T - costs  # (n_accept, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected contracts: all actions utility < 0 (agent rejects)
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # (n_reject, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs to fix IR violations on accepted contracts
        for i, feasible in enumerate(accept_feasible):
            if not feasible:
                violated_actions = np.where(accept_utils[i] < -eps_cost)[0]
                if violated_actions.size > 0:
                    costs[violated_actions] += step_increase

        # Increase costs to fix IC violations on rejected contracts
        if contract_rej.shape[0] > 0:
            for i, feasible in enumerate(reject_feasible):
                if not feasible:
                    violated_actions = np.where(reject_utils[i] >= eps_cost)[0]
                    if violated_actions.size > 0:
                        costs[violated_actions] += step_increase

        costs = np.maximum(costs, 0.0)

        # Early stop if costs stabilized
        if np.allclose(costs, prev_costs, atol=eps_cost):
            break

    # Final cleanup: robust normalize centers and ensure costs non-negative
    centers = np.clip(centers, 0, None)
    centers_sum = centers.sum(axis=1, keepdims=True)
    centers_sum[centers_sum < 1e-12] = 1.0
    centers /= centers_sum
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
