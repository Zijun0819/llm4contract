```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog, minimize
from sklearn.mixture import GaussianMixture


def agent_solver_v2(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer a valid agent setting (actions) explaining the historical contract logs.

    Args:
        v: principal's reward vector of length 5.
        content: DataFrame with columns ['Contract', 'Principal Utility', 'Agent Action'].

    Returns:
        agent_setting: n x 6 ndarray, each row: [p_1,...,p_5, cost],
                       where p sums to 1 and cost >= 0.
    """
    m = v.shape[0]
    contracts = np.vstack(content['Contract'].to_numpy())
    utilities = content['Principal Utility'].to_numpy()
    actions = content['Agent Action'].to_numpy()
    L = len(content)

    # Filter accepted and rejected logs
    accepted_idx = np.where(actions == 1)[0]
    rejected_idx = np.where(actions == -1)[0]

    # Step 1: Extract p-like vectors from accepted logs by solving LP:
    # For accepted log i: find p_i s.t sum(p_i)=1, p_i >=0, and p_i @ w_i = utility_i + cost_i
    # cost_i unknown but >=0; we treat cost_i as slack: p_i @ w_i >= cost_i + 0 (agent IR)
    # We try to find probability vectors p_i consistent with accepted contracts by solving:
    # maximize 0 subject to p_i @ w_i >= cost_i (unknown)
    # To get candidate p_i, we fix cost_i=0 and solve feasibility for p_i: p_i @ w_i >= 0 and sum(p_i)=1, p_i>=0
    # Because we don't know cost, we relax and solve min/max p @ w_i subject to p
    candidate_ps = []
    for i in accepted_idx:
        w = contracts[i]
        # Solve LP: find p s.t sum(p)=1, p>=0, and p @ w >= u (agent utility >= 0)
        # Because agent utility = p @ w - cost >= 0, and cost unknown >=0, at least p @ w >= cost
        # We attempt to find p maximizing p @ w with sum(p)=1 and p>=0. The max p @ w is max v_i (since p is prob)
        # But we want some p that explains observed utility ~ principal utility (approximate)
        # Here we try to find p s.t p @ w = utilities[i] + cost_i unknown, 
        # assume cost_i minimal and p @ w >= 0, solve feasibility:
        # We'll solve a quadratic program to find p close to uniform but satisfying p @ w >= epsilon
        c = np.zeros(m)
        A_eq = [np.ones(m)]
        b_eq = [1.0]
        bounds = [(0,1)]*m

        # We try to find p that yields agent utility >=0, so p @ w >= cost >=0
        # Let's just solve min ||p - uniform||^2 s.t sum p=1, p>=0, p @ w >= 0
        # Using scipy minimize with constraints
        x0 = np.ones(m)/m
        cons = ({
            'type': 'eq', 'fun': lambda p: np.sum(p)-1
        }, {
            'type': 'ineq', 'fun': lambda p: np.dot(p, w)  # p @ w >=0
        })
        res = minimize(lambda p: np.sum((p - x0)**2), x0, method='SLSQP', bounds=bounds, constraints=cons)
        if res.success:
            candidate_ps.append(res.x)
        else:
            # fallback: uniform distribution
            candidate_ps.append(x0)

    if len(candidate_ps) == 0:
        raise ValueError("No accepted logs to infer agent setting")

    candidate_ps = np.array(candidate_ps)

    # Step 2: Use Gaussian Mixture Model (GMM) to cluster p vectors to infer latent actions
    # GMM allows soft clustering and determines number of clusters by BIC
    lowest_bic = np.inf
    best_gmm = None
    n_components_range = range(2, min(10, len(candidate_ps))+1)
    for n_comp in n_components_range:
        gmm = GaussianMixture(n_components=n_comp, covariance_type='full', random_state=0, n_init=5)
        gmm.fit(candidate_ps)
        bic = gmm.bic(candidate_ps)
        if bic < lowest_bic:
            lowest_bic = bic
            best_gmm = gmm

    # Extract cluster centers as actions' outcome distributions p
    p_actions = best_gmm.means_
    # Normalize p_actions to ensure each sums to 1 and nonnegative (due to numeric noise)
    p_actions = np.clip(p_actions, 1e-12, None)
    p_actions /= p_actions.sum(axis=1, keepdims=True)

    n_actions = p_actions.shape[0]

    # Step 3: For each accepted log, assign to closest action by minimal L2 distance on p
    accepted_ps = candidate_ps
    assigned_actions = best_gmm.predict(accepted_ps)

    # Step 4: Estimate cost per action c_a satisfying:
    # For accepted logs assigned to action a: p_a @ w_i - c_a >= 0 (IR)
    # For rejected logs: max_a (p_a @ w_j - c_a) < 0 (IR violation)
    # Formulate LP to find minimal nonnegative c to satisfy constraints

    # Build constraints:
    # Variables: c = [c_0, ..., c_{n_actions-1}] >= 0

    # Accepted constraints: for each accepted log i with assigned action a:
    # p_a @ w_i - c_a >= 0  => -c_a >= -p_a @ w_i
    # Rejected constraints: for each rejected log j:
    # max_a (p_a @ w_j - c_a) < 0 => p_a @ w_j - c_a <= 0 for all a

    # So we have inequality constraints:
    # For accepted logs i:
    # -c_a <= -p_a @ w_i  (one constraint per accepted log)
    # For rejected logs j and all a:
    # p_a @ w_j - c_a <= 0

    # Build matrices for linprog: min sum c_a (or zeros) s.t constraints and c_a >=0

    c_obj = np.ones(n_actions)  # minimize sum costs to keep them small

    A_ub = []
    b_ub = []

    # Accepted constraints:
    for i, a in zip(accepted_idx, assigned_actions):
        val = np.dot(p_actions[a], contracts[i])
        row = np.zeros(n_actions)
        row[a] = -1
        A_ub.append(row)
        b_ub.append(-val)

    # Rejected constraints:
    for j in rejected_idx:
        w_j = contracts[j]
        for a in range(n_actions):
            val = np.dot(p_actions[a], w_j)
            row = np.zeros(n_actions)
            row[a] = 1
            A_ub.append(row)
            b_ub.append(val)

    A_ub = np.array(A_ub) if A_ub else None
    b_ub = np.array(b_ub) if b_ub else None

    bounds = [(0, None)]*n_actions

    # Solve LP
    res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if not res.success:
        # fallback: set costs to zero
        c_actions = np.zeros(n_actions)
    else:
        c_actions = res.x

    # Step 5: Return agent setting matrix [p_1,...,p_5, cost]
    agent_setting = np.hstack([p_actions, c_actions[:, None]])

    return agent_setting
```
