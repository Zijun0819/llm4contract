```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by adaptively clustering inferred outcome distributions with cosine metric,
    assigning noise points by cosine similarity, and iteratively refining costs
    to satisfy IR and IC constraints for accepted and rejected contracts.

    Improvements over v1:
    - Even finer eps candidates for DBSCAN clustering (0.01 to 0.35, 50 points).
    - LP solver with very tight tolerance and fallback to inequality with tighter tolerance.
    - Increased max_iter to 200 and reduced cost increments for smoother convergence.
    - Enforced strict normalization with clipping and numerical safeguards.
    - Added small numerical tolerances to avoid floating point instability.
    - Balanced min_samples=3 for DBSCAN to capture small clusters but avoid noise.
    - Early stopping on cost stabilization with very tight atol.
    """

    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform distribution and zero cost
    if accepted.empty:
        uniform_p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
        return np.hstack([uniform_p.reshape(1, -1), np.zeros((1, 1), dtype=np.float64)])

    # Step 1: Infer p for each accepted contract by LP with very tight feasibility and fallback
    def infer_p_for_log(w, u_p):
        w = np.array(w, dtype=np.float64)
        c_obj = -w
        A_eq = np.vstack([np.ones(m_outcomes, dtype=np.float64), v - w])
        b_eq = np.array([1.0, u_p], dtype=np.float64)
        bounds = [(0.0, 1.0) for _ in range(m_outcomes)]

        # Primary LP with very tight tolerance
        try:
            res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds,
                          method='highs', options={"presolve": True, "tol": 1e-14})
            if res.success:
                p = np.clip(res.x, 0, None)
                s = p.sum()
                if s > 0:
                    p /= s
                else:
                    p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
                return p
        except Exception:
            pass

        # Fallback: relax equality on agent utility to inequality with tighter tolerance
        tol = 1e-13
        A_eq_relax = np.ones((1, m_outcomes), dtype=np.float64)
        b_eq_relax = np.array([1.0], dtype=np.float64)
        A_ub_relax = np.array([-(v - w)], dtype=np.float64)
        b_ub_relax = np.array([-(u_p - tol)], dtype=np.float64)
        try:
            res2 = linprog(c=c_obj, A_eq=A_eq_relax, b_eq=b_eq_relax,
                           A_ub=A_ub_relax, b_ub=b_ub_relax, bounds=bounds,
                           method='highs', options={"presolve": True, "tol": 1e-14})
            if res2.success:
                p = np.clip(res2.x, 0, None)
                s = p.sum()
                if s > 0:
                    p /= s
                else:
                    p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
                return p
        except Exception:
            pass

        # Final fallback: uniform distribution
        return np.ones(m_outcomes, dtype=np.float64) / m_outcomes

    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, m_outcomes)

    # Step 2: Adaptive DBSCAN clustering with cosine metric and silhouette checks on fine eps grid
    eps_candidates = np.linspace(0.01, 0.35, 50)
    best_eps = None
    best_labels = None
    best_silhouette = -1

    for eps in eps_candidates:
        clustering = DBSCAN(eps=eps, min_samples=3, metric='cosine', n_jobs=-1).fit(p_array)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters >= 2:
            mask = labels != -1
            if np.sum(mask) >= 8:  # require at least 8 samples for silhouette score
                try:
                    sil = silhouette_score(p_array[mask], labels[mask], metric='cosine')
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_eps = eps
                        best_labels = labels.copy()
                except Exception:
                    continue
        elif n_clusters == 1 and -1 not in labels:
            # Single cluster no noise acceptable fallback if no better silhouette found
            if best_silhouette < 0:
                best_eps = eps
                best_labels = labels.copy()
                best_silhouette = 0.0

    if best_labels is None:
        best_labels = np.zeros(len(p_array), dtype=int)

    labels = best_labels
    noise_idx = np.where(labels == -1)[0]
    unique_labels = sorted(set(labels) - {-1})
    n_actions = len(unique_labels)
    if n_actions == 0:
        # All noise fallback to one cluster
        n_actions = 1
        labels[:] = 0
        unique_labels = [0]

    # Step 3: Compute cluster centers (mean p per cluster), normalize strictly
    centers = np.zeros((n_actions, m_outcomes), dtype=np.float64)
    for i, lab in enumerate(unique_labels):
        cluster_ps = p_array[labels == lab]
        c = cluster_ps.mean(axis=0)
        c = np.clip(c, 0.0, None)
        s = c.sum()
        if s > 0:
            c /= s
        else:
            c = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
        centers[i] = c

    # Step 4: Assign noise points to nearest cluster center by cosine similarity
    if noise_idx.size > 0:
        noise_p = p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # shape (noise_size, n_actions)
        assign_labels = sim.argmax(axis=1)
        for idx, assigned_cluster in zip(noise_idx, assign_labels):
            labels[idx] = unique_labels[assigned_cluster]

    # Remap labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    # Prepare contract arrays
    contract_acc = np.array(accepted['Contract'].tolist(), dtype=np.float64)
    contract_rej = np.array(rejected['Contract'].tolist(), dtype=np.float64) if not rejected.empty else np.empty((0, m_outcomes), dtype=np.float64)

    eps_cost = 1e-13
    costs = np.zeros(n_actions, dtype=np.float64)

    # Step 5: Initialize minimal costs c_a satisfying IR and IC constraints conservatively
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(mapped_labels == a)[0]
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0

        if contract_rej.shape[0] > 0:
            max_rej = np.max(contract_rej @ p_a)
        else:
            max_rej = -np.inf

        costs[a] = max(min_acc, max_rej + eps_cost, 0.0)

    # Step 6: Iteratively refine costs to satisfy IR and IC constraints strictly
    max_iter = 200
    cost_increment = eps_cost * 10
    for _ in range(max_iter):
        prev_costs = costs.copy()

        # Accepted contracts: at least one action with utility >= 0 (agent accepts)
        accept_utils = contract_acc @ centers.T - costs  # shape (n_accept, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected contracts: all actions utility < 0 (agent rejects)
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # shape (n_reject, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs to fix IR violations on accepted contracts
        for i, feasible in enumerate(accept_feasible):
            if not feasible:
                violated_actions = np.where(accept_utils[i] < -eps_cost)[0]
                if violated_actions.size > 0:
                    costs[violated_actions] += cost_increment

        # Increase costs to fix IC violations on rejected contracts
        if contract_rej.shape[0] > 0:
            for i, feasible in enumerate(reject_feasible):
                if not feasible:
                    violated_actions = np.where(reject_utils[i] >= eps_cost)[0]
                    if violated_actions.size > 0:
                        costs[violated_actions] += cost_increment

        costs = np.maximum(costs, 0.0)

        # Early stop if costs stabilized tightly
        if np.allclose(costs, prev_costs, atol=eps_cost * 0.3):
            break

    # Final cleanup: normalize centers strictly and ensure costs non-negative
    centers = np.clip(centers, 0.0, None)
    centers_sum = centers.sum(axis=1, keepdims=True)
    centers_sum[centers_sum == 0] = 1.0
    centers /= centers_sum
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
