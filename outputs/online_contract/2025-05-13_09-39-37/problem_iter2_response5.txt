```python
import numpy as np
from scipy.optimize import linprog
from sklearn.cluster import DBSCAN

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost]) that explains the
    historical interactions between principal and agent under IR (Individual Rationality) and IC (Incentive Compatibility).

    Args:
        v: np.ndarray, shape (5,) - principal's reward vector for 5 outcomes.
        content: list of dicts, each dict has keys:
            - 'Contract': list or np.ndarray, 5-dimensional payment vector for 5 outcomes
            - 'Principal Utility': float, principal's utility under contract (0 if agent rejects)
            - 'Agent Action': int, 1 if accepted (agent utility â‰¥ 0), -1 if rejected (agent utility < 0)

    Returns:
        np.ndarray of shape (n_actions, 6): [p_1,...,p_5,cost] with:
            - p_i: probability of ith outcome for that action (sum to 1)
            - cost: non-negative agent cost of action
    """

    contracts = np.array([log['Contract'] for log in content], dtype=np.float64)  # (L,5)
    principal_utils = np.array([log['Principal Utility'] for log in content], dtype=np.float64)  # (L,)
    agent_actions = np.array([log['Agent Action'] for log in content], dtype=int)  # (L,)

    L, m = contracts.shape
    assert m == 5 and v.size == 5

    accepted_idx = np.where(agent_actions == 1)[0]
    rejected_idx = np.where(agent_actions == -1)[0]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if accepted_idx.size == 0:
        uniform_p = np.ones(m) / m
        return np.hstack([uniform_p.reshape(1, -1), np.array([[0.0]])])

    # Step 1: Infer candidate p distributions for accepted contracts
    candidate_p = []
    for idx in accepted_idx:
        w = contracts[idx]
        pu = principal_utils[idx]

        # Constraints:
        # sum p_i = 1
        # (v - w).p = pu
        # p.w >= 0  (agent utility >= 0)
        # p_i >= 0

        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1.0, pu])

        A_ub = -w.reshape(1, -1)
        b_ub = np.array([0.0])

        bounds = [(0, 1) for _ in range(m)]

        # Objective: minimize L1 distance from uniform distribution p=1/5
        c = np.ones(m) / m

        res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq,
                      bounds=bounds, method='highs')

        if res.success:
            p_sol = res.x
            p_sol[p_sol < 0] = 0
            s = p_sol.sum()
            if s > 0:
                p_sol /= s
            else:
                p_sol = np.ones(m) / m
            candidate_p.append(p_sol)
            continue

        # If failed, relax agent utility >=0 constraint and try again (ignore A_ub,b_ub)
        res_alt = linprog(c=c, A_eq=A_eq, b_eq=b_eq,
                          bounds=bounds, method='highs')
        if res_alt.success:
            p_sol_alt = res_alt.x
            p_sol_alt[p_sol_alt < 0] = 0
            s = p_sol_alt.sum()
            if s > 0:
                p_sol_alt /= s
            else:
                p_sol_alt = np.ones(m) / m
            candidate_p.append(p_sol_alt)
            continue

        # Fallback uniform if no solution
        candidate_p.append(np.ones(m) / m)

    candidate_p = np.array(candidate_p)  # shape (num_accepted, 5)

    # Step 2: Cluster candidate_p adaptively with DBSCAN to detect distinct actions
    # eps=0.05 chosen to allow small variations; min_samples=2 to form clusters
    clustering = DBSCAN(eps=0.05, min_samples=2).fit(candidate_p)
    labels = clustering.labels_

    # Handle noise points (-1)
    unique_labels = np.unique(labels)
    non_noise_labels = unique_labels[unique_labels != -1]

    if non_noise_labels.size == 0:
        # all noise, assign all to one cluster 0
        labels[:] = 0
        non_noise_labels = np.array([0])

    # For noise points, assign to nearest cluster center
    noise_idx = np.where(labels == -1)[0]
    if noise_idx.size > 0 and non_noise_labels.size > 0:
        centers = np.array([candidate_p[labels == lab].mean(axis=0) for lab in non_noise_labels])
        for ni in noise_idx:
            dist = np.linalg.norm(centers - candidate_p[ni], axis=1)
            assign_label = non_noise_labels[np.argmin(dist)]
            labels[ni] = assign_label

    n_actions = len(non_noise_labels)
    # Order clusters by label ascending
    sorted_labels = np.sort(non_noise_labels)
    # Compute cluster centers (mean p)
    p_cluster = np.vstack([candidate_p[labels == lab].mean(axis=0) for lab in sorted_labels])
    # Normalize cluster centers to simplex (non-negativity + sum 1)
    p_cluster[p_cluster < 0] = 0
    p_cluster /= p_cluster.sum(axis=1, keepdims=True)

    # Step 3: Infer costs for each action under IR and IC constraints

    # Map accepted logs to cluster indices (0..n_actions-1)
    label_to_idx = {lab: i for i, lab in enumerate(sorted_labels)}
    accepted_cluster_idx = np.array([label_to_idx[lab] for lab in labels])

    # IR: For each action a, cost_a <= min_{accepted logs assigned to a} p_a.w
    cost_lower_bounds = np.zeros(n_actions)  # cost >= 0
    cost_upper_bounds = np.full(n_actions, np.inf)

    for a in range(n_actions):
        assigned_accepted_indices = np.where(accepted_cluster_idx == a)[0]
        if assigned_accepted_indices.size == 0:
            # no accepted logs assigned, cost upper bound infinite
            cost_upper_bounds[a] = np.inf
            cost_lower_bounds[a] = 0.0
            continue
        # accepted_idx[assigned_accepted_indices] maps to original accepted_idx
        orig_indices = accepted_idx[assigned_accepted_indices]
        w_assigned = contracts[orig_indices]  # (num_assigned, 5)
        # agent utility = p_a . w_assigned.T
        agent_utils = w_assigned @ p_cluster[a]
        cost_upper_bounds[a] = agent_utils.min()
        cost_lower_bounds[a] = 0.0  # cost non-negative

    # IC: For rejected logs, agent utility < cost_a for all actions a
    # => cost_a > max_{rejected logs} p_a.w
    if rejected_idx.size > 0:
        w_rejected = contracts[rejected_idx]  # (num_rejected, 5)
        for a in range(n_actions):
            agent_util_rej = w_rejected @ p_cluster[a]  # (num_rejected,)
            max_agent_util_rej = agent_util_rej.max()
            # cost_a must be strictly greater than max_agent_util_rej
            # add small margin epsilon
            margin = 1e-6
            if cost_lower_bounds[a] < max_agent_util_rej + margin:
                cost_lower_bounds[a] = max_agent_util_rej + margin

    # Ensure feasibility: cost_lower_bounds <= cost_upper_bounds
    # If violated, relax cost_upper_bounds to be at least cost_lower_bounds + small margin
    for i in range(n_actions):
        if cost_upper_bounds[i] < cost_lower_bounds[i]:
            cost_upper_bounds[i] = cost_lower_bounds[i] + 1e-4

    # Step 4: Solve LP to find costs minimizing sum costs subject to bounds
    # Variables: costs c in R^{n_actions}, with bounds cost_lower_bounds <= c <= cost_upper_bounds
    # Objective: minimize sum costs (to keep costs small)
    c_obj = np.ones(n_actions)
    bounds = [(cost_lower_bounds[i], cost_upper_bounds[i]) for i in range(n_actions)]

    res = linprog(c=c_obj, bounds=bounds, method='highs')
    if res.success:
        costs = res.x
    else:
        # fallback: midpoint of bounds
        costs = (cost_lower_bounds + cost_upper_bounds) / 2

    # Final sanity: costs >= 0
    costs[costs < 0] = 0.0

    # Step 5: Return agent setting matrix (n_actions x 6)
    # Columns: first 5 probabilities, last cost
    agent_setting = np.hstack([p_cluster, costs.reshape(-1, 1)])

    return agent_setting
```
