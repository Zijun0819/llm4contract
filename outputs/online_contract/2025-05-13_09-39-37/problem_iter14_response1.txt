```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by:
      - inferring outcome distributions p for accepted contracts with relaxed LP constraints,
      - adaptively clustering inferred p's with cosine metric and finer eps grid,
      - carefully assigning noise points by cosine similarity with fallback,
      - initializing costs conservatively from IR and IC constraints,
      - aggressively refining costs with adaptive increments until IR/IC strictly hold,
      - enforcing strict normalization and non-negativity for probabilities and costs.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains:
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (â‰¥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform distribution and zero cost
    if accepted.empty:
        uniform_p = np.ones(m_outcomes) / m_outcomes
        return np.hstack([uniform_p.reshape(1, -1), np.zeros((1, 1))])

    # Step 1: Infer p for each accepted contract by LP with more relaxed feasibility and fallbacks
    def infer_p_for_log(w, u_p):
        w = np.array(w, dtype=np.float64)
        c_obj = -w
        bounds = [(0, 1) for _ in range(m_outcomes)]

        # Attempt 1: strict equalities sum(p)=1 and p@(v-w)=u_p
        A_eq = np.vstack([np.ones(m_outcomes), v - w])
        b_eq = np.array([1.0, u_p], dtype=np.float64)
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p

        # Attempt 2: relax agent utility equality to inequality p@(v-w) >= u_p - tol, sum(p)=1
        tol = 1e-6
        A_eq_relaxed = np.ones(m_outcomes).reshape(1, -1)
        b_eq_relaxed = np.array([1.0])
        A_ub = np.array([-(v - w)])
        b_ub = np.array([-(u_p - tol)])
        res2 = linprog(c=c_obj, A_eq=A_eq_relaxed, b_eq=b_eq_relaxed,
                       A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')
        if res2.success:
            p = res2.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p

        # Attempt 3: relax sum(p)=1 to [0.98,1.02], agent utility inequality as above
        A_ub_relaxed = np.vstack([
            -(v - w),
            np.ones(m_outcomes),
            -np.ones(m_outcomes)
        ])
        b_ub_relaxed = np.array([
            -(u_p - tol),
            1.02,
            -0.98
        ])
        res3 = linprog(c=c_obj, A_ub=A_ub_relaxed, b_ub=b_ub_relaxed, bounds=bounds, method='highs')
        if res3.success:
            p = res3.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p

        # Fallback uniform distribution
        return np.ones(m_outcomes) / m_outcomes

    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, m_outcomes)

    # Step 2: Adaptive DBSCAN clustering with cosine metric and finer eps candidates
    eps_candidates = np.linspace(0.02, 0.18, 18)
    best_eps = None
    best_labels = None
    best_silhouette = -1

    for eps in eps_candidates:
        clustering = DBSCAN(eps=eps, min_samples=2, metric='cosine').fit(p_array)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters >= 2:
            mask = labels != -1
            if np.sum(mask) >= 4:
                try:
                    sil = silhouette_score(p_array[mask], labels[mask], metric='cosine')
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_eps = eps
                        best_labels = labels.copy()
                except Exception:
                    continue
        elif n_clusters == 1 and -1 not in labels:
            best_eps = eps
            best_labels = labels.copy()
            best_silhouette = 0.0

    if best_labels is None:
        best_labels = np.zeros(len(p_array), dtype=int)

    labels = best_labels
    noise_idx = np.where(labels == -1)[0]
    unique_labels = sorted(set(labels) - {-1})
    n_actions = len(unique_labels)
    if n_actions == 0:
        n_actions = 1
        labels[:] = 0
        unique_labels = [0]

    # Step 3: Compute cluster centers (mean p per cluster), normalize strictly
    centers = np.zeros((n_actions, m_outcomes), dtype=np.float64)
    for i, lab in enumerate(unique_labels):
        cluster_ps = p_array[labels == lab]
        c = cluster_ps.mean(axis=0)
        c = np.clip(c, 0, None)
        s = c.sum()
        if s > 0:
            c /= s
        else:
            c = np.ones(m_outcomes) / m_outcomes
        centers[i] = c

    # Step 4: Assign noise points to nearest cluster center by cosine similarity with fallback
    if noise_idx.size > 0:
        noise_p = p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # shape (noise_size, n_actions)
        assign_labels = sim.argmax(axis=1)
        for idx, assigned_cluster in zip(noise_idx, assign_labels):
            labels[idx] = unique_labels[assigned_cluster]

    # Map all labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    # Prepare contract arrays
    contract_acc = np.array(accepted['Contract'].tolist(), dtype=np.float64)
    contract_rej = np.array(rejected['Contract'].tolist(), dtype=np.float64) if not rejected.empty else np.empty((0, m_outcomes), dtype=np.float64)

    eps_cost = 1e-8
    costs = np.zeros(n_actions, dtype=np.float64)

    # Step 5: Initialize minimal costs c_a satisfying IR and IC constraints conservatively
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(mapped_labels == a)[0]
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0

        if contract_rej.shape[0] > 0:
            max_rej = np.max(contract_rej @ p_a)
        else:
            max_rej = -np.inf

        costs[a] = max(min_acc, max_rej + eps_cost, 0.0)

    # Step 6: Iteratively refine costs to satisfy IR and IC constraints strictly with adaptive increments
    max_iter = 60
    cost_increment_base = eps_cost * 100
    for iteration in range(max_iter):
        prev_costs = costs.copy()

        # Accepted contracts: at least one action with utility >= 0 (agent accepts)
        accept_utils = contract_acc @ centers.T - costs  # shape (n_accept, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected contracts: all actions utility < 0 (agent rejects)
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # shape (n_reject, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs adaptively to fix IR violations on accepted contracts
        for i, feasible in enumerate(accept_feasible):
            if not feasible:
                violated_actions = np.where(accept_utils[i] < -eps_cost)[0]
                if violated_actions.size > 0:
                    # Increase costs proportionally to violation magnitude, clipped
                    violation_amounts = (-accept_utils[i][violated_actions]).clip(min=eps_cost)
                    costs[violated_actions] += np.minimum(cost_increment_base, violation_amounts * 200)

        # Increase costs adaptively to fix IC violations on rejected contracts
        if contract_rej.shape[0] > 0:
            for i, feasible in enumerate(reject_feasible):
                if not feasible:
                    violated_actions = np.where(reject_utils[i] >= eps_cost)[0]
                    if violated_actions.size > 0:
                        violation_amounts = (reject_utils[i][violated_actions]).clip(min=eps_cost)
                        costs[violated_actions] += np.minimum(cost_increment_base, violation_amounts * 200)

        costs = np.maximum(costs, 0.0)

        # Early stop if costs stabilized within tighter tolerance
        if np.allclose(costs, prev_costs, atol=eps_cost / 5):
            break

    # Final cleanup: normalize centers strictly and ensure costs non-negative
    centers = np.clip(centers, 0, None)
    centers_sum = centers.sum(axis=1, keepdims=True)
    centers_sum[centers_sum == 0] = 1.0
    centers /= centers_sum
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
