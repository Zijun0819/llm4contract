```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (prob distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (≥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # Sanity check
    if accepted.empty:
        # No acceptances, trivial agent with one action: uniform dist and zero cost
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.zeros((1, 1))])

    # Step 1: For each accepted contract, estimate agent outcome distribution p
    # by solving LP: find p s.t.
    #   - p probability vector (sum 1, p ≥0)
    #   - p @ w >= c (agent utility ≥0)
    # Here, c unknown but agent utility ≥0 means p @ w - c ≥0
    # We approximate p by maximizing agent expected wage under constraints 
    # that principal utility equals p@(v - w) and p is valid distribution.
    # We use principal utility = p @ (v - w) from logs.

    def infer_p_for_log(w, u_p):
        # Solve LP:
        # variables: p (length m_outcomes)
        # constraints:
        #   sum p = 1
        #   p @ (v - w) = u_p (principal utility observed)
        # bounds: p_i in [0,1]
        # objective: maximize p @ w (agent expected payment)
        c_obj = -np.array(w)  # maximize p @ w <=> minimize -p @ w
        A_eq = [np.ones(m_outcomes), v - w]
        b_eq = [1.0, u_p]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            return res.x
        else:
            # fallback: uniform distribution
            return np.ones(m_outcomes) / m_outcomes

    p_list = []
    for _, row in accepted.iterrows():
        p_vec = infer_p_for_log(row['Contract'], row['Principal Utility'])
        p_list.append(p_vec)
    p_array = np.vstack(p_list)

    # Step 2: Cluster these p's into groups representing distinct agent actions
    # Use Agglomerative Clustering with distance threshold to adapt number of actions
    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.15).fit(p_array)
    labels = clustering.labels_
    n_actions = labels.max() + 1

    # Compute cluster centers (mean p per cluster)
    centers = np.zeros((n_actions, m_outcomes))
    for a in range(n_actions):
        centers[a] = p_array[labels == a].mean(axis=0)

    # Normalize centers to ensure valid distributions (handle numerical issues)
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)

    # Step 3: For each action, infer minimal agent cost c_a consistent with IR and IC
    # - For IR (accept logs assigned to cluster): agent utility >= 0 => p_a @ w - c_a >=0 => c_a <= p_a @ w
    # - For IC (reject logs): c_a > p_a @ w for all rejected contracts
    # To safely satisfy both:
    #   c_a = max(
    #       max_{accepted assigned to a} [p_a @ w],
    #       max_{rejected} [p_a @ w] + epsilon
    #   )
    eps = 1e-6
    costs = np.zeros(n_actions)
    contract_matrix = np.array(accepted['Contract'].tolist())
    reject_matrix = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))

    for a in range(n_actions):
        p_a = centers[a]

        # Accepted contracts assigned to cluster a
        accepted_idx = np.where(labels == a)[0]
        if accepted_idx.size > 0:
            w_acc = contract_matrix[accepted_idx]
            max_acc = np.max(w_acc @ p_a)
        else:
            max_acc = 0.0

        # Rejected contracts
        if reject_matrix.shape[0] > 0:
            rej_vals = reject_matrix @ p_a
            max_rej = rej_vals.max()
        else:
            max_rej = -np.inf

        # Cost must be at least max_acc (IR), and strictly larger than max_rej (IC)
        cost_a = max(max_acc, max_rej + eps)
        costs[a] = max(cost_a, 0.0)  # ensure non-negativity

    # Step 4: Validate inferred agent setting on all logs:
    # Accepted logs must have p_a @ w - c_a >= 0 for some action a
    # Rejected logs must have max_a (p_a @ w - c_a) < 0
    # We'll re-assign accepted logs to best action under inferred model
    # Rejected logs remain rejected if no action yields non-negative utility

    # Combine p and costs for convenience
    agent_setting = np.hstack([centers, costs[:, None]])

    # Check accept logs feasibility & reassign
    accept_w = np.array(accepted['Contract'].tolist())
    accept_utilities = accept_w @ centers.T - costs  # shape (n_accept, n_actions)
    accept_best_action = accept_utilities.argmax(axis=1)
    accept_best_util = accept_utilities.max(axis=1)
    if np.any(accept_best_util < -eps):
        # Some accepted log cannot be explained => increase cluster count and retry or relax eps
        # Here just fallback: increase cost slightly
        costs += eps
        agent_setting[:, -1] = costs

    # Check rejected logs feasibility
    if reject_matrix.shape[0] > 0:
        reject_utilities = reject_matrix @ centers.T - costs  # shape (n_reject, n_actions)
        reject_max_util = reject_utilities.max(axis=1)
        if np.any(reject_max_util >= -eps):
            # Some rejected logs wrongly explained as accepted, increase costs slightly
            costs += eps * 10
            agent_setting[:, -1] = costs

    return agent_setting
```
