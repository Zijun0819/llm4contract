```python
import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.optimize import linprog
from sklearn.metrics.pairwise import cosine_similarity

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by jointly clustering accepted and rejected logs' inferred outcome distributions,
    and inferring costs to satisfy IR and IC constraints.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains:
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (â‰¥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform dist and zero cost
    if accepted.empty:
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.zeros((1, 1))])

    # Step 1: Infer p for each accepted contract by LP
    # p: distribution over outcomes for agent action that accepted contract corresponds to
    def infer_p_for_log(w, u_p):
        # Solve LP:
        # variables: p (length m_outcomes)
        # constraints:
        #   sum p = 1
        #   p @ (v - w) = u_p
        # bounds: p_i in [0,1]
        # objective: maximize p @ w (agent expected payment)
        c_obj = -np.array(w)
        A_eq = [np.ones(m_outcomes), v - np.array(w)]
        b_eq = [1.0, u_p]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical fix: clip and re-normalize
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p
        else:
            # fallback: uniform distribution
            return np.ones(m_outcomes) / m_outcomes

    p_acc_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_acc_array = np.vstack(p_acc_list)  # shape (n_accept, m_outcomes)

    # Step 2: For rejected contracts, infer approximate p by LP to explain rejection
    # We want p s.t. max_p p @ w - c < 0 for all costs c >=0, so p @ w < 0 for rejected contracts
    # We approximate p by solving LP to find p that minimizes p @ w subject to sum p=1, p in [0,1]
    # This gives a plausible outcome distribution for rejection (agent utility < 0)
    p_rej_list = []
    for _, row in rejected.iterrows():
        w = np.array(row['Contract'])
        # minimize p @ w subject to sum p=1, p_i in [0,1]
        c_obj = w
        A_eq = [np.ones(m_outcomes)]
        b_eq = [1.0]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            p_rej_list.append(p)
        else:
            p_rej_list.append(np.ones(m_outcomes) / m_outcomes)
    if len(p_rej_list) > 0:
        p_rej_array = np.vstack(p_rej_list)
    else:
        p_rej_array = np.empty((0, m_outcomes))

    # Step 3: Combine accepted and rejected inferred p's for joint clustering
    # Label accepted as 1, rejected as 0 for clustering guidance (not used in clustering directly)
    combined_p = np.vstack([p_acc_array, p_rej_array])
    n_acc = p_acc_array.shape[0]
    n_rej = p_rej_array.shape[0]

    # Step 4: Use Agglomerative Clustering with cosine metric and adaptive n_clusters selection
    # Candidate cluster numbers: from 2 to min(10, number_of_samples)
    min_clusters = 2
    max_clusters = min(10, combined_p.shape[0])
    best_n_clusters = None
    best_labels = None
    best_silhouette = -1

    # To avoid errors, only compute silhouette if clusters > 1 and no noise points
    # AgglomerativeClustering does not produce noise points

    for n_clusters in range(min_clusters, max_clusters + 1):
        clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='cosine', linkage='average')
        labels = clustering.fit_predict(combined_p)
        try:
            sil = silhouette_score(combined_p, labels, metric='cosine')
            if sil > best_silhouette:
                best_silhouette = sil
                best_n_clusters = n_clusters
                best_labels = labels.copy()
        except Exception:
            # In case silhouette_score fails (e.g. all points in one cluster), skip
            continue

    # If silhouette failed or only one cluster possible, fallback to 1 cluster
    if best_labels is None:
        best_n_clusters = 1
        best_labels = np.zeros(combined_p.shape[0], dtype=int)

    labels = best_labels
    n_actions = best_n_clusters

    # Step 5: Compute cluster centers (mean p per cluster), normalize and clip negatives
    centers = np.zeros((n_actions, m_outcomes))
    for a in range(n_actions):
        cluster_points = combined_p[labels == a]
        if cluster_points.shape[0] == 0:
            centers[a] = np.ones(m_outcomes) / m_outcomes
        else:
            c = cluster_points.mean(axis=0)
            c = np.clip(c, 0, None)
            s = c.sum()
            if s > 0:
                centers[a] = c / s
            else:
                centers[a] = np.ones(m_outcomes) / m_outcomes

    # Step 6: Infer minimal costs c_a satisfying IR and IC constraints
    # IR: For accepted contracts assigned to action a: p_a @ w - c_a >= 0 => c_a <= min p_a @ w
    # IC: For rejected contracts: for all a: p_a @ w - c_a < 0 => c_a > max p_a @ w among rejects
    eps_cost = 1e-8
    contract_acc = np.array(accepted['Contract'].tolist())
    contract_rej = np.array(rejected['Contract'].tolist()) if n_rej > 0 else np.empty((0, m_outcomes))

    # Map accepted logs to clusters by nearest center (cosine similarity)
    acc_sim = cosine_similarity(p_acc_array, centers)  # shape (n_acc, n_actions)
    acc_assign = acc_sim.argmax(axis=1)

    costs = np.zeros(n_actions)
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(acc_assign == a)[0]
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0

        if contract_rej.shape[0] > 0:
            rej_vals = contract_rej @ p_a
            max_rej = np.max(rej_vals)
        else:
            max_rej = -np.inf

        cost_a = max(min_acc, max_rej + eps_cost)
        costs[a] = max(cost_a, 0.0)

    # Step 7: Validate and refine costs iteratively to ensure all IR and IC hold
    max_iter = 30
    for _ in range(max_iter):
        # Accepted logs: must have some action a with p_a @ w - c_a >= 0
        accept_utils = contract_acc @ centers.T - costs  # shape (n_acc, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected logs: must have all actions a with p_a @ w - c_a < 0
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # shape (n_rej, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs slightly to enforce constraints
        costs += eps_cost * 10
        costs = np.maximum(costs, 0.0)

    # Final normalization for numerical stability
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
