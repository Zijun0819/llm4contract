```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + cost)
    consistent with historical interaction logs: contracts, principal utilities, and agent actions.

    Parameters:
    - v: (5,) numpy array, principal's reward vector for 5 outcomes.
    - content: list of dicts, each with keys:
        'Contract' (list of 5 floats),
        'Principal Utility' (float),
        'Agent Action' (1 for accept, -1 for reject).

    Returns:
    - np.ndarray of shape (n_actions, 6),
      where columns 0-4 are probabilities over outcomes,
      and column 5 is nonnegative cost of that action.
    """
    m = len(v)  # number of outcomes (expected 5)
    logs_df = pd.DataFrame(content)
    n_logs = len(logs_df)

    # Separate accepted and rejected logs
    accepted = logs_df[logs_df['Agent Action'] == 1]
    rejected = logs_df[logs_df['Agent Action'] == -1]

    if accepted.empty:
        # No accepted contracts, return trivial single action with uniform distribution and zero cost
        p_uniform = np.ones(m) / m
        cost_zero = 0.0
        return np.hstack([p_uniform, cost_zero]).reshape(1, m + 1)

    # LP to infer p for each accepted log: find p s.t
    # sum p = 1, p >= 0, p·(w - v) >= -u (u = principal utility)
    # feasibility LP with zero objective
    def infer_p_for_log(w: np.ndarray, u: float):
        c = np.zeros(m)
        A_eq = [np.ones(m)]
        b_eq = [1.0]
        A_ub = [-(w - v)]
        b_ub = [u]
        bounds = [(0, 1)] * m
        res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical fix: clip and renormalize
            p = np.clip(p, 0, 1)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m) / m
            return p
        else:
            return None

    candidate_p = []
    u_vals = []
    accepted_idx = accepted.index.values
    for idx in accepted_idx:
        row = accepted.loc[idx]
        w = np.array(row['Contract'])
        u = row['Principal Utility']
        p_candidate = infer_p_for_log(w, u)
        if p_candidate is not None:
            candidate_p.append(p_candidate)
            u_vals.append(u)
    if len(candidate_p) == 0:
        # Fallback: uniform distributions for all accepted logs
        candidate_p = [np.ones(m) / m] * len(accepted)
        u_vals = accepted['Principal Utility'].values.tolist()

    candidate_p = np.vstack(candidate_p)
    u_vals = np.array(u_vals)

    # Adaptive clustering of candidate_p to find agent actions
    max_clusters = min(10, len(candidate_p))
    if max_clusters == 1:
        n_clusters = 1
    else:
        best_score = -1
        best_k = 1
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(candidate_p)
            try:
                score = silhouette_score(candidate_p, kmeans.labels_)
            except:
                score = -1
            if score > best_score:
                best_score = score
                best_k = k
        n_clusters = best_k

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20).fit(candidate_p)
    centers_p = kmeans.cluster_centers_

    # Ensure centers_p are valid distributions
    centers_p = np.clip(centers_p, 0, None)
    centers_p_sum = centers_p.sum(axis=1, keepdims=True)
    # Avoid division by zero
    centers_p_sum[centers_p_sum == 0] = 1.0
    centers_p /= centers_p_sum

    # Estimate initial costs per cluster:
    # cost_a <= p_a·v - u_i for all accepted logs i in cluster a
    costs = np.zeros(n_clusters)
    for a in range(n_clusters):
        assigned_idx = np.where(kmeans.labels_ == a)[0]
        cluster_p = centers_p[a]
        if len(assigned_idx) == 0:
            costs[a] = 0.0
        else:
            cost_candidates = cluster_p @ v - u_vals[assigned_idx]
            costs[a] = max(0., np.min(cost_candidates))

    # Assign each accepted log to closest cluster by Euclidean distance on p
    assigned_actions = kmeans.labels_

    # Adjust costs to satisfy IR (agent utility ≥ 0) for accepted logs
    for a in range(n_clusters):
        idxs = np.where(assigned_actions == a)[0]
        if len(idxs) == 0:
            continue
        cluster_p = centers_p[a]
        for i in idxs:
            w_i = np.array(accepted.iloc[i]['Contract'])
            agent_util = cluster_p @ w_i - costs[a]
            if agent_util < 0:
                # Decrease cost to ensure agent utility ≥ 0, but cost ≥ 0
                costs[a] = max(0., costs[a] + agent_util)

    # Enforce IC for rejected logs: agent utility < 0 for all actions
    rejected_idx = rejected.index.values
    for idxr in rejected_idx:
        w_r = np.array(rejected.loc[idxr]['Contract'])
        for a in range(n_clusters):
            util = centers_p[a] @ w_r - costs[a]
            if util >= 0:
                # Increase cost slightly to enforce rejection
                costs[a] = util + 1e-5

    # Final normalization of centers_p for numerical stability
    centers_p = np.clip(centers_p, 0, None)
    centers_p_sum = centers_p.sum(axis=1, keepdims=True)
    centers_p_sum[centers_p_sum == 0] = 1.0
    centers_p /= centers_p_sum

    # Compose agent setting matrix: [p_action | cost]
    agent_setting = np.hstack([centers_p, costs.reshape(-1, 1)])

    return agent_setting
```
