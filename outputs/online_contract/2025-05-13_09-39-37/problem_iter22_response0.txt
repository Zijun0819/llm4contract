```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from scipy.optimize import linprog

def agent_solver(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (probability distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    by adaptively clustering inferred outcome distributions with cosine metric,
    assigning noise points by cosine similarity, and iteratively refining costs
    to satisfy IR and IC constraints for accepted and rejected contracts.

    Improvements:
    - Balanced DBSCAN clustering with min_samples=3 and fine eps grid.
    - Robust LP solving with fallback and tight tolerances.
    - Strict normalization and clipping of probability vectors.
    - Adaptive cost increments with early stopping on convergence.
    - Noise assignment by cosine similarity for stable cluster assignment.
    - Careful handling of empty accepted/rejected sets.
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no accepted contracts, return trivial agent with uniform distribution and zero cost
    if accepted.empty:
        uniform_p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
        return np.hstack([uniform_p.reshape(1, -1), np.zeros((1, 1), dtype=np.float64)])

    def infer_p_for_log(w, u_p):
        w = np.array(w, dtype=np.float64)
        c_obj = -w
        A_eq = np.vstack([np.ones(m_outcomes, dtype=np.float64), v - w])
        b_eq = np.array([1.0, u_p], dtype=np.float64)
        bounds = [(0.0, 1.0)] * m_outcomes

        # Primary LP: equality constraints with tight tolerance
        try:
            res = linprog(
                c=c_obj,
                A_eq=A_eq,
                b_eq=b_eq,
                bounds=bounds,
                method='highs',
                options={"presolve": True, "tol": 1e-13}
            )
            if res.success:
                p = res.x
                p = np.clip(p, 0.0, None)
                s = p.sum()
                if s > 0:
                    p /= s
                else:
                    p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
                return p
        except Exception:
            pass

        # Fallback: relax equality on agent utility to inequality with small tolerance
        tol = 1e-11
        A_eq_relax = np.ones((1, m_outcomes), dtype=np.float64)
        b_eq_relax = np.array([1.0], dtype=np.float64)
        A_ub_relax = np.array([-(v - w)], dtype=np.float64)
        b_ub_relax = np.array([-(u_p - tol)], dtype=np.float64)

        try:
            res2 = linprog(
                c=c_obj,
                A_eq=A_eq_relax,
                b_eq=b_eq_relax,
                A_ub=A_ub_relax,
                b_ub=b_ub_relax,
                bounds=bounds,
                method='highs',
                options={"presolve": True, "tol": 1e-13}
            )
            if res2.success:
                p = res2.x
                p = np.clip(p, 0.0, None)
                s = p.sum()
                if s > 0:
                    p /= s
                else:
                    p = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
                return p
        except Exception:
            pass

        # Final fallback uniform distribution
        return np.ones(m_outcomes, dtype=np.float64) / m_outcomes

    # Infer outcome distributions p for all accepted contracts
    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, m_outcomes)

    # Adaptive DBSCAN clustering with cosine metric and silhouette score
    eps_candidates = np.linspace(0.015, 0.20, 30)
    min_samples = 3
    best_eps = None
    best_labels = None
    best_silhouette = -1.0

    for eps in eps_candidates:
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(p_array)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters >= 2:
            mask = labels != -1
            if np.sum(mask) >= 6:
                try:
                    sil = silhouette_score(p_array[mask], labels[mask], metric='cosine')
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_eps = eps
                        best_labels = labels.copy()
                except Exception:
                    continue
        elif n_clusters == 1 and -1 not in labels:
            # Single cluster no noise fallback
            if best_silhouette < 0:
                best_eps = eps
                best_labels = labels.copy()
                best_silhouette = 0.0

    if best_labels is None:
        best_labels = np.zeros(len(p_array), dtype=int)

    labels = best_labels
    noise_idx = np.where(labels == -1)[0]
    unique_labels = sorted(set(labels) - {-1})
    n_actions = len(unique_labels)
    if n_actions == 0:
        n_actions = 1
        labels[:] = 0
        unique_labels = [0]

    # Compute cluster centers as mean p per cluster, normalize strictly
    centers = np.zeros((n_actions, m_outcomes), dtype=np.float64)
    for i, lab in enumerate(unique_labels):
        cluster_ps = p_array[labels == lab]
        c = cluster_ps.mean(axis=0)
        c = np.clip(c, 0.0, None)
        s = c.sum()
        if s > 0:
            c /= s
        else:
            c = np.ones(m_outcomes, dtype=np.float64) / m_outcomes
        centers[i] = c

    # Assign noise points to nearest cluster center by cosine similarity
    if noise_idx.size > 0:
        noise_p = p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # shape (noise_size, n_actions)
        assign_labels = sim.argmax(axis=1)
        for idx, assigned_cluster in zip(noise_idx, assign_labels):
            labels[idx] = unique_labels[assigned_cluster]

    # Map all labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    contract_acc = np.array(accepted['Contract'].tolist(), dtype=np.float64)
    contract_rej = np.array(rejected['Contract'].tolist(), dtype=np.float64) if not rejected.empty else np.empty((0, m_outcomes), dtype=np.float64)

    eps_cost = 5e-12
    costs = np.zeros(n_actions, dtype=np.float64)

    # Initialize minimal costs c_a satisfying IR and IC constraints conservatively
    for a in range(n_actions):
        p_a = centers[a]
        assigned_acc_idx = np.where(mapped_labels == a)[0]
        if assigned_acc_idx.size > 0:
            w_acc = contract_acc[assigned_acc_idx]
            min_acc = np.min(w_acc @ p_a)
        else:
            min_acc = 0.0

        if contract_rej.shape[0] > 0:
            max_rej = np.max(contract_rej @ p_a)
        else:
            max_rej = -np.inf

        costs[a] = max(min_acc, max_rej + eps_cost, 0.0)

    max_iter = 150
    cost_increment = eps_cost * 25

    for _ in range(max_iter):
        prev_costs = costs.copy()

        # Accepted contracts: at least one action with utility >= -eps_cost (agent accepts)
        accept_utils = contract_acc @ centers.T - costs  # shape (n_accept, n_actions)
        accept_feasible = (accept_utils >= -eps_cost).any(axis=1)

        # Rejected contracts: all actions utility < eps_cost (agent rejects)
        if contract_rej.shape[0] > 0:
            reject_utils = contract_rej @ centers.T - costs  # shape (n_reject, n_actions)
            reject_feasible = (reject_utils < eps_cost).all(axis=1)
        else:
            reject_feasible = np.array([True])

        if accept_feasible.all() and reject_feasible.all():
            break

        # Increase costs to fix IR violations on accepted contracts
        for i, feasible in enumerate(accept_feasible):
            if not feasible:
                violated_actions = np.where(accept_utils[i] < -eps_cost)[0]
                if violated_actions.size > 0:
                    costs[violated_actions] += cost_increment

        # Increase costs to fix IC violations on rejected contracts
        if contract_rej.shape[0] > 0:
            for i, feasible in enumerate(reject_feasible):
                if not feasible:
                    violated_actions = np.where(reject_utils[i] >= eps_cost)[0]
                    if violated_actions.size > 0:
                        costs[violated_actions] += cost_increment

        costs = np.maximum(costs, 0.0)

        # Early stop if costs stabilized within tolerance
        if np.allclose(costs, prev_costs, atol=eps_cost):
            break

    # Final strict normalization and cleanup
    centers = np.clip(centers, 0.0, None)
    centers_sum = centers.sum(axis=1, keepdims=True)
    centers_sum[centers_sum == 0.0] = 1.0
    centers /= centers_sum
    costs = np.maximum(costs, 0.0)

    agent_setting = np.hstack([centers, costs[:, None]])
    return agent_setting
```
