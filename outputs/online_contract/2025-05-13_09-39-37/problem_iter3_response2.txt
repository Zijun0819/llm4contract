```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (prob distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    incorporating both accepted and rejected logs in clustering and cost inference.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (â‰¥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no acceptances, trivial agent with uniform dist and zero cost
    if accepted.empty:
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.zeros((1, 1))])

    # Step 1: Infer agent outcome distributions p for accepted logs via LP
    def infer_p_for_log(w, u_p):
        # Solve LP:
        # variables: p (length m_outcomes)
        # constraints:
        #   sum p = 1
        #   p @ (v - w) = u_p
        # bounds: 0 <= p_i <= 1
        # objective: maximize p @ w
        c_obj = -np.array(w)
        A_eq = [np.ones(m_outcomes), v - w]
        b_eq = [1.0, u_p]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical fix: clip and renormalize
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p
        else:
            # fallback uniform distribution
            return np.ones(m_outcomes) / m_outcomes

    p_list = [infer_p_for_log(row['Contract'], row['Principal Utility']) for _, row in accepted.iterrows()]
    p_array = np.vstack(p_list)  # shape (n_accept, 5)

    # Step 2: Incorporate rejected logs into clustering by augmenting with inferred p's for rejects
    # For rejected logs, agent utility < 0 => no direct p inferred
    # Approximate p's for rejected logs by solving LP relaxing IR constraint to just p sum=1 and p>=0
    # but with p @ (v - w) < 0 (agent utility negative)
    # We attempt to find a p that satisfies p @ (v - w) <= -epsilon to separate rejects.
    # If infeasible, fallback to uniform distribution.

    eps_rej = 1e-3
    reject_p_list = []
    reject_contracts = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))
    for i in range(reject_contracts.shape[0]):
        w = reject_contracts[i]
        # Try to find p s.t sum p=1, p>=0, p@(v-w) <= -eps_rej
        # Objective: minimize 0 (feasibility check)
        c_obj = np.zeros(m_outcomes)
        A_eq = [np.ones(m_outcomes)]
        b_eq = [1.0]
        A_ub = [v - w]
        b_ub = [-eps_rej]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            reject_p_list.append(p)
        else:
            # fallback uniform distribution
            reject_p_list.append(np.ones(m_outcomes) / m_outcomes)

    if len(reject_p_list) > 0:
        reject_p_array = np.vstack(reject_p_list)
        combined_p_array = np.vstack([p_array, reject_p_array])
    else:
        combined_p_array = p_array.copy()

    # Step 3: Cluster all inferred p's (accepted + rejected) with DBSCAN to adaptively determine actions
    # Use cosine distance to capture distribution similarity
    # Tune eps by silhouette score on a small grid

    def dbscan_cluster(p_data, eps_val):
        clustering = DBSCAN(eps=eps_val, min_samples=2, metric='cosine').fit(p_data)
        labels = clustering.labels_
        # Filter out noise points (-1)
        if np.all(labels == -1):
            # all noise => assign all to one cluster
            labels = np.zeros(len(labels), dtype=int)
        return labels

    eps_candidates = np.linspace(0.05, 0.3, 6)
    best_eps = eps_candidates[0]
    best_score = -1
    for eps_cand in eps_candidates:
        labels = dbscan_cluster(combined_p_array, eps_cand)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters <= 1:
            # silhouette score undefined or meaningless for 1 cluster
            continue
        try:
            score = silhouette_score(combined_p_array, labels, metric='cosine')
        except:
            score = -1
        if score > best_score:
            best_score = score
            best_eps = eps_cand

    labels = dbscan_cluster(combined_p_array, best_eps)
    # Replace noise (-1) by nearest cluster center assignment later
    noise_idx = np.where(labels == -1)[0]
    assigned_idx = np.where(labels != -1)[0]

    # Compute cluster centers from assigned points
    unique_labels = set(labels)
    if -1 in unique_labels:
        unique_labels.remove(-1)
    unique_labels = sorted(unique_labels)
    n_actions = len(unique_labels)
    centers = np.zeros((n_actions, m_outcomes))
    for i, lab in enumerate(unique_labels):
        centers[i] = combined_p_array[labels == lab].mean(axis=0)
    # Normalize centers
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)

    # Assign noise points to nearest cluster center by cosine similarity
    if len(noise_idx) > 0:
        from sklearn.metrics.pairwise import cosine_similarity
        noise_p = combined_p_array[noise_idx]
        sim = cosine_similarity(noise_p, centers)  # shape (len(noise_idx), n_actions)
        assign_labels = sim.argmax(axis=1)
        for idx, a_lab in zip(noise_idx, assign_labels):
            labels[idx] = unique_labels[a_lab]

    # Map all labels to 0..n_actions-1
    label_map = {lab: i for i, lab in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[lab] for lab in labels])

    # Step 4: Infer minimal costs per action consistent with IR and IC constraints
    # IR: For accepted logs assigned to action a: p_a @ w - c_a >= 0 => c_a <= min p_a @ w (tightest)
    # IC: For rejected logs: for all a: p_a @ w - c_a < 0 => c_a > max p_a @ w among rejects
    # To satisfy both, set:
    # c_a = max(max_{accepted assigned to a} p_a @ w, max_{rejected} p_a @ w + eps)

    eps_cost = 1e-6
    costs = np.zeros(n_actions)
    accept_contracts = np.array(accepted['Contract'].tolist())
    reject_contracts = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))

    for a in range(n_actions):
        p_a = centers[a]

        # Accepted logs assigned to this action
        accept_idx = np.where(mapped_labels[:len(accepted)] == a)[0]
        if accept_idx.size > 0:
            w_acc = accept_contracts[accept_idx]
            acc_vals = w_acc @ p_a
            min_acc = acc_vals.min()  # minimal agent expected payment (tightest IR)
        else:
            min_acc = 0.0

        # Rejected logs
        if reject_contracts.shape[0] > 0:
            rej_vals = reject_contracts @ p_a
            max_rej = rej_vals.max()
        else:
            max_rej = -np.inf

        cost_a = max(min_acc, max_rej + eps_cost)
        costs[a] = max(cost_a, 0.0)

    # Step 5: Validate inferred agent setting on all logs and refine costs if needed
    # Accepted logs must have at least one action a with p_a @ w - c_a >= 0
    # Rejected logs must have all actions a with p_a @ w - c_a < 0

    agent_setting = np.hstack([centers, costs[:, None]])

    def validate_costs(agent_setting, accept_contracts, reject_contracts, eps_margin=1e-8):
        centers_ = agent_setting[:, :m_outcomes]
        costs_ = agent_setting[:, m_outcomes]
        # Accepted logs
        accept_util = accept_contracts @ centers_.T - costs_  # (n_accept, n_actions)
        accept_feasible = (accept_util >= -eps_margin).any(axis=1)
        # Rejected logs
        if reject_contracts.shape[0] > 0:
            reject_util = reject_contracts @ centers_.T - costs_  # (n_reject, n_actions)
            reject_feasible = (reject_util < eps_margin).all(axis=1)
        else:
            reject_feasible = np.array([], dtype=bool)
        return accept_feasible.all() and (reject_feasible.size == 0 or reject_feasible.all())

    max_iter = 20
    iter_count = 0
    while iter_count < max_iter:
        if validate_costs(agent_setting, accept_contracts, reject_contracts):
            break
        # If validation fails, increase all costs slightly to enforce IR/IC strictly
        costs += eps_cost * 10
        agent_setting[:, m_outcomes] = costs
        iter_count += 1

    # Final normalization of centers (numerical stability)
    centers = agent_setting[:, :m_outcomes]
    centers = np.clip(centers, 0, None)
    centers /= centers.sum(axis=1, keepdims=True)
    costs = np.maximum(agent_setting[:, m_outcomes], 0)
    agent_setting = np.hstack([centers, costs[:, None]])

    return agent_setting
```
