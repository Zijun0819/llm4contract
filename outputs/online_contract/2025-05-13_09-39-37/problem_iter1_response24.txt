```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering

def agent_solver_v2(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer agent setting (actions as outcome distributions and costs) consistent with historical logs.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,).
        content (pd.DataFrame): Historical logs with columns ['Contract', 'Principal Utility', 'Agent Action'].

    Returns:
        np.ndarray: Agent setting matrix of shape (n_actions, 6),
                    first 5 columns are outcome distributions (sum=1),
                    last column is non-negative cost.
    """
    # Parameters
    m = v.shape[0]  # number of outcomes (5)
    logs = content
    L = len(logs)

    # Extract arrays from logs
    contracts = np.array(logs['Contract'].tolist())  # shape (L,5)
    p_utils = np.array(logs['Principal Utility'])    # shape (L,)
    actions = np.array(logs['Agent Action'])         # shape (L,)

    # Step 1: Separate accepted and rejected logs
    accepted_mask = (actions == 1)
    rejected_mask = (actions == -1)

    contracts_acc = contracts[accepted_mask]
    p_utils_acc = p_utils[accepted_mask]
    contracts_rej = contracts[rejected_mask]

    # If no accepted logs, fallback trivial single action: uniform distribution & cost 0
    if len(contracts_acc) == 0:
        uniform_p = np.ones(m) / m
        return np.hstack([uniform_p, [0.0]])

    # Step 2: Infer outcome distributions p for accepted logs by solving LPs:
    # For each accepted log i, find p_i s.t:
    #   p_i @ contract_i = principal utility_i + agent cost_i (unknown)
    #   sum(p_i) = 1, p_i >= 0
    # But agent cost unknown, so we focus on feasibility:
    # Consider p_i as distribution that explains contract acceptance with utility >= 0
    # We relax agent cost unknown by clustering outcome distributions that explain acceptance

    # We attempt to find for each accepted contract a p_i with minimal cost c_i >=0 s.t:
    #     p_i @ w_i - c_i >= 0 (agent utility from accepted contract non-negative)
    #     sum p_i =1, p_i >= 0

    # To find p_i, we fix c_i = min possible cost for p_i to satisfy acceptance:
    # This is equivalent to min c_i = p_i @ w_i s.t sum p_i=1, p_i>=0 and agent IR holds:
    # For each accepted contract:
    #   min_{p} p @ w_i, s.t sum p=1, p>=0

    # We use linprog to get minimal cost distributions per accepted contract
    p_candidates = []
    c_candidates = []
    for i in range(len(contracts_acc)):
        w = contracts_acc[i]
        # Minimize p @ w subject to sum p=1, p>=0
        c = w  # costs vector for linprog
        A_eq = np.ones((1, m))
        b_eq = np.array([1.0])
        bounds = [(0, 1) for _ in range(m)]
        # minimize c @ p
        res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p_candidates.append(res.x)
            c_candidates.append(res.fun)
        else:
            # fallback uniform if LP fails
            p_candidates.append(np.ones(m)/m)
            c_candidates.append(np.dot(np.ones(m)/m, w))

    p_candidates = np.array(p_candidates)  # shape (num_accept, m)
    c_candidates = np.array(c_candidates)  # shape (num_accept,)

    # Step 3: Cluster p_candidates to reduce number of agent actions adaptively
    # Use Agglomerative clustering with distance threshold to determine number of clusters adaptively
    cluster_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0.15)
    cluster_labels = cluster_model.fit_predict(p_candidates)

    n_actions = cluster_labels.max() + 1

    # Step 4: For each cluster (agent action), aggregate p and estimate cost
    p_actions = np.zeros((n_actions, m))
    c_actions = np.zeros(n_actions)

    for a in range(n_actions):
        p_in_cluster = p_candidates[cluster_labels == a]
        c_in_cluster = c_candidates[cluster_labels == a]
        # Aggregate outcome distribution: centroid of cluster (mean)
        p_mean = p_in_cluster.mean(axis=0)
        # Normalize to ensure sum=1 (numerical stability)
        p_mean = np.clip(p_mean, 0, None)
        p_mean /= p_mean.sum()
        p_actions[a] = p_mean
        # Cost estimate for action: max minimal cost over cluster
        c_actions[a] = c_in_cluster.max()

    # Step 5: Enforce IR & IC constraints using all logs:
    # IR: agent utility from chosen action >= 0 for accepted contracts
    # IC: agent utility from rejected contracts < 0 for all actions

    # IR check: for each accepted log i, exists action a s.t:
    # p_actions[a] @ contract_i - c_actions[a] >= 0 and agent chose accept (1)
    # We verify and fix costs if needed by increasing c_actions slightly

    eps = 1e-8
    for i, (w_i, ua) in enumerate(zip(contracts_acc, p_utils_acc)):
        utilities = p_actions @ w_i - c_actions  # utilities from all actions on contract_i
        # Check if any action explains acceptance (utility >=0)
        if not np.any(utilities >= -eps):
            # Increase cost downward to allow feasibility: decrease c_actions for best matching action
            a_best = np.argmax(utilities)
            c_actions[a_best] = p_actions[a_best] @ w_i

    # IC check: for each rejected contract j:
    # For all actions a: p_a @ w_j - c_a < 0 (agent strictly rejects)
    # If violated, increase c_a to satisfy p_a @ w_j - c_a < 0

    for w_j in contracts_rej:
        utilities = p_actions @ w_j - c_actions
        violated = utilities >= 0
        if np.any(violated):
            for a in np.where(violated)[0]:
                # Increase cost slightly above current utility to enforce rejection
                c_actions[a] = p_actions[a] @ w_j + 1e-4

    # Step 6: Ensure costs non-negative & normalize p_actions again
    c_actions = np.maximum(c_actions, 0.0)
    p_actions = np.clip(p_actions, 0, None)
    p_actions /= p_actions.sum(axis=1, keepdims=True)

    # Step 7: Output final agent setting matrix shape (n_actions, 6)
    agent_setting = np.hstack([p_actions, c_actions.reshape(-1, 1)])

    return agent_setting
```
