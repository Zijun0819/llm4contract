```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (rows: agent actions; cols: 5 outcome probs + cost)
    consistent with historical contracts, principal utilities, and agent actions.
    """

    # Parameters
    m = v.size  # number of outcomes
    logs_df = pd.DataFrame(content)
    L = len(content)

    # Extract arrays for convenience
    contracts = np.array(logs_df['Contract'].tolist())  # shape (L, m)
    principal_utils = np.array(logs_df['Principal Utility'].tolist())  # shape (L,)
    agent_actions = np.array(logs_df['Agent Action'].tolist())  # shape (L,)

    # Step 0: Basic checks
    assert contracts.shape[1] == m, "Contract dimension mismatch with v"

    # Step 1: Separate accepted and rejected indices
    accept_idx = np.where(agent_actions == 1)[0]
    reject_idx = np.where(agent_actions == -1)[0]

    # If no accepts, return trivial single action (uniform prob, zero cost)
    if len(accept_idx) == 0:
        p = np.ones(m) / m
        c = 0.0
        return np.hstack([p, c]).reshape(1, m + 1)

    # Step 2: For accepted logs, estimate empirical outcome distributions p_i by solving LP:
    # For each accepted contract w_i and principal utility u_i:
    # Find p_i s.t. sum(p_i) = 1, p_i >= 0,
    # p_i @ w_i = principal utility + agent cost (unknown),
    # but agent cost unknown, so we guess p_i that "explains" acceptance, aiming to cluster later.

    # We approximate p_i by solving:
    # max_p p @ w_i subject to sum(p) = 1 and p>=0, expecting p @ w_i >= agent cost (unknown)
    # Instead, we find p_i maximizing p_i @ w_i to approximate agent behavior.
    # Since we don't know agent cost, we take p_i maximizing p_i @ w_i (maximizing expected payment).

    def solve_p_given_w(w):
        # maximize p @ w
        c_obj = -w  # maximize -> minimize negative
        A_eq = [np.ones(m)]
        b_eq = [1]
        bounds = [(0, 1) for _ in range(m)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            return res.x
        else:
            # fallback uniform distribution if LP fails
            return np.ones(m) / m

    p_candidates = np.array([solve_p_given_w(contracts[i]) for i in accept_idx])

    # Step 3: Cluster p_candidates to identify distinct agent actions
    # Use Agglomerative Clustering with distance threshold to adapt number of clusters
    clusterer = AgglomerativeClustering(n_clusters=None, distance_threshold=0.15, linkage='average')
    cluster_labels = clusterer.fit_predict(p_candidates)
    n_actions = cluster_labels.max() + 1

    # Compute cluster centroids (mean outcome distributions)
    p_actions = np.zeros((n_actions, m))
    for a in range(n_actions):
        members = p_candidates[cluster_labels == a]
        p_actions[a] = members.mean(axis=0)
        p_actions[a] /= p_actions[a].sum()  # normalize to sum to 1

    # Step 4: Infer action costs c_a by IR & IC constraints
    # IR constraint: For each accepted contract assigned to action a:
    # p_a @ w_i - c_a >= 0  =>  c_a <= p_a @ w_i
    # So c_a <= min over accepted logs assigned to a of p_a @ w_i

    # IC constraint: For rejected contracts:
    # p_a @ w_i - c_a < 0  =>  c_a > p_a @ w_i for all rejected logs to ensure rejection

    # Assign each accepted log to closest p_action by L1 distance
    assigned_actions = np.full(L, -1, dtype=int)
    accept_p_logs = p_candidates  # approximated p per accepted log
    for idx_i, log_i in enumerate(accept_idx):
        p_log = accept_p_logs[idx_i]
        dists = np.linalg.norm(p_actions - p_log, ord=1, axis=1)
        assigned_actions[log_i] = dists.argmin()

    # For each action, calculate c_a upper bound from accepted logs assigned to it
    c_upper_bounds = np.full(n_actions, np.inf)
    for a in range(n_actions):
        idxs = np.where(assigned_actions == a)[0]
        if len(idxs) == 0:
            # No accepted logs assigned -> cost zero minimal
            c_upper_bounds[a] = 0.0
        else:
            vals = [p_actions[a] @ contracts[i] for i in idxs]
            c_upper_bounds[a] = min(vals)

    # For each action, calculate c_a lower bound from rejected logs
    # To ensure rejection: c_a > max over rejected contracts of p_a @ w_i
    c_lower_bounds = np.zeros(n_actions)
    if len(reject_idx) > 0:
        for a in range(n_actions):
            vals = [p_actions[a] @ contracts[i] for i in reject_idx]
            c_lower_bounds[a] = max(vals) if vals else 0.0

    # Step 5: Final costs c_a chosen to satisfy IR & IC: c_lower_bounds < c_a <= c_upper_bounds
    # If empty feasible interval, relax by taking midpoint or prioritize IC constraint (reject consistency)
    c_final = np.zeros(n_actions)
    for a in range(n_actions):
        lb = c_lower_bounds[a]
        ub = c_upper_bounds[a]
        if lb < ub:
            c_final[a] = (lb + ub) / 2
        else:
            # no feasible cost satisfying both IR and IC exactly
            # prefer cost > lb to guarantee rejection consistency (strong IC),
            # accept slight IR violation (agent might accept with zero utility)
            c_final[a] = lb + 1e-5

        # ensure cost non-negative
        if c_final[a] < 0:
            c_final[a] = 0.0

    # Step 6: Verify IR for accepted logs and adjust costs downward if possible
    for a in range(n_actions):
        idxs = np.where(assigned_actions == a)[0]
        if len(idxs) > 0:
            # ensure c_final[a] <= min p_a@w_i for accepted logs
            minimal_paw = min(p_actions[a] @ contracts[i] for i in idxs)
            if c_final[a] > minimal_paw:
                c_final[a] = minimal_paw

    # Step 7: Normalize p_actions rows to sum to 1 (numerical safety)
    p_actions = np.clip(p_actions, 0, None)
    p_actions /= p_actions.sum(axis=1, keepdims=True)

    # Step 8: Compose final agent setting matrix: [p_action | c_final]
    agent_setting = np.hstack([p_actions, c_final.reshape(-1, 1)])

    return agent_setting
```
