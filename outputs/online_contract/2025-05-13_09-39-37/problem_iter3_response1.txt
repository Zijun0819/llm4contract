```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (prob distributions over outcomes + costs)
    explaining the historical interaction logs between principal and agent,
    incorporating both accepted and rejected contracts via robust clustering
    and LP feasibility checks.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes, shape (5,)
        content (list[dict]): Each dict contains
            - 'Contract': 5-dim payment vector,
            - 'Principal Utility': principal utility under contract,
            - 'Agent Action': 1 for accept, -1 for reject.

    Returns:
        np.ndarray: n x 6 matrix: first 5 cols are outcome probabilities (sum=1),
                    last column is agent cost (â‰¥0).
    """
    m_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    accepted = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # If no acceptances, trivial agent with uniform dist and zero cost
    if accepted.empty:
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.zeros((1, 1))])

    # === Step 1: Infer agent outcome distributions p for accepted contracts ===
    # Solve LP for each accepted contract to find p s.t.
    # sum p = 1, p @ (v - w) = principal utility, p in [0,1],
    # maximize p @ w (agent expected payment)
    def infer_p_for_log(w, u_p):
        c_obj = -np.array(w)
        A_eq = [np.ones(m_outcomes), v - w]
        b_eq = [1.0, u_p]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical fix: clip and renormalize if needed
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            return p
        else:
            return np.ones(m_outcomes) / m_outcomes

    p_list = []
    for _, row in accepted.iterrows():
        p_vec = infer_p_for_log(row['Contract'], row['Principal Utility'])
        p_list.append(p_vec)
    p_array = np.vstack(p_list)

    # === Step 2: Incorporate rejected contracts into clustering ===
    # We assign a proxy p for rejected logs by solving relaxed LPs that reflect rejection:
    # For rejected logs: agent utility < 0 => p@w - c < 0
    # We do not know c, so we approximate p by minimizing p@w (agent expected wage),
    # subject to sum p=1, p in [0,1].
    # This yields a proxy p that explains rejection as low expected payment.

    reject_p_list = []
    reject_contracts = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))
    for i in range(reject_contracts.shape[0]):
        w = reject_contracts[i]
        # LP: minimize p@w s.t. sum p=1, p>=0
        c_obj = np.array(w)
        A_eq = [np.ones(m_outcomes)]
        b_eq = [1.0]
        bounds = [(0, 1) for _ in range(m_outcomes)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m_outcomes) / m_outcomes
            reject_p_list.append(p)
        else:
            reject_p_list.append(np.ones(m_outcomes) / m_outcomes)
    reject_p_array = np.vstack(reject_p_list) if reject_p_list else np.empty((0, m_outcomes))

    # Combine accepted and rejected p's for clustering
    # Label accepted as 1, rejected as 0 to guide clustering
    combined_p = np.vstack([p_array, reject_p_array])
    labels_true = np.array([1]*p_array.shape[0] + [0]*reject_p_array.shape[0])

    # === Step 3: Cluster combined p's with DBSCAN to adaptively find actions ===
    # Use cosine metric for probability vectors, eps chosen by elbow heuristic
    # Min samples = 2 to avoid noise clusters
    # We try a range of eps and pick best silhouette score or fallback
    from sklearn.metrics import silhouette_score
    from sklearn.metrics.pairwise import cosine_distances

    def cluster_dbscan(p_mat, eps):
        clustering = DBSCAN(eps=eps, min_samples=2, metric='cosine').fit(p_mat)
        return clustering.labels_

    # Try eps candidates
    eps_candidates = np.linspace(0.05, 0.3, 6)
    best_eps = eps_candidates[0]
    best_score = -1
    best_labels = None
    for eps in eps_candidates:
        labels = cluster_dbscan(combined_p, eps)
        if len(set(labels)) <= 1 or (labels == -1).all():
            continue
        # Filter noise points (-1) for silhouette
        mask = labels != -1
        if np.sum(mask) < 2:
            continue
        try:
            score = silhouette_score(combined_p[mask], labels[mask], metric='cosine')
        except Exception:
            continue
        if score > best_score:
            best_score = score
            best_eps = eps
            best_labels = labels

    if best_labels is None:
        # fallback: all accepted in one cluster, all rejected noise
        best_labels = np.concatenate([np.zeros(p_array.shape[0], dtype=int), -np.ones(reject_p_array.shape[0], dtype=int)])

    labels = best_labels
    # Extract clusters ignoring noise (-1)
    clustered_indices = labels != -1
    clustered_p = combined_p[clustered_indices]
    clustered_labels = labels[clustered_indices]
    n_actions = clustered_labels.max() + 1 if clustered_labels.size > 0 else 1
    if n_actions == 0:
        n_actions = 1

    # === Step 4: Compute cluster centers (mean p per cluster) ===
    centers = np.zeros((n_actions, m_outcomes))
    for a in range(n_actions):
        cluster_ps = clustered_p[clustered_labels == a]
        if cluster_ps.shape[0] == 0:
            centers[a] = np.ones(m_outcomes) / m_outcomes
        else:
            mean_p = cluster_ps.mean(axis=0)
            mean_p = np.clip(mean_p, 0, None)
            s = mean_p.sum()
            if s > 0:
                mean_p /= s
            else:
                mean_p = np.ones(m_outcomes) / m_outcomes
            centers[a] = mean_p

    # === Step 5: Infer minimal costs c_a per action satisfying IR and IC ===
    # IR: For accepted logs assigned to cluster a, p_a @ w - c_a >= 0 => c_a <= p_a @ w
    # IC: For rejected logs, for all a: p_a @ w - c_a < 0 => c_a > p_a @ w
    # Assign accepted logs to closest cluster center by p-distance (cosine)
    from sklearn.metrics.pairwise import cosine_distances as cos_dist

    # Assign accepted logs to clusters
    accepted_p = p_array
    dist_matrix = cos_dist(accepted_p, centers)
    accepted_assign = dist_matrix.argmin(axis=1)

    contract_accept = np.array(accepted['Contract'].tolist())
    contract_reject = np.array(rejected['Contract'].tolist()) if not rejected.empty else np.empty((0, m_outcomes))

    eps = 1e-7
    costs = np.zeros(n_actions)
    for a in range(n_actions):
        p_a = centers[a]

        # Accepted assigned to cluster a
        idx_acc = np.where(accepted_assign == a)[0]
        if idx_acc.size > 0:
            w_acc = contract_accept[idx_acc]
            max_acc = np.max(w_acc @ p_a)
        else:
            max_acc = 0.0

        # Rejected contracts
        if contract_reject.shape[0] > 0:
            rej_vals = contract_reject @ p_a
            max_rej = rej_vals.max()
        else:
            max_rej = -np.inf

        # Cost must satisfy IR and IC:
        # c_a >= max_acc (IR)
        # c_a > max_rej (IC)
        cost_a = max(max_acc, max_rej + eps)
        costs[a] = max(cost_a, 0.0)

    # === Step 6: Validate agent setting and refine costs if needed ===
    # Accepted logs: must have some action a with p_a @ w - c_a >= 0
    # Rejected logs: no action a with p_a @ w - c_a >= 0

    agent_setting = np.hstack([centers, costs[:, None]])

    # Check accepted logs feasibility
    accept_utilities = contract_accept @ centers.T - costs  # shape (n_accept, n_actions)
    accept_max_util = accept_utilities.max(axis=1)
    if np.any(accept_max_util < -eps):
        # Increase costs slightly to fix IR violation
        costs += eps * 10
        agent_setting[:, -1] = costs

    # Check rejected logs feasibility
    if contract_reject.shape[0] > 0:
        reject_utilities = contract_reject @ centers.T - costs  # shape (n_reject, n_actions)
        reject_max_util = reject_utilities.max(axis=1)
        if np.any(reject_max_util >= -eps):
            # Increase costs to fix IC violation
            costs += eps * 100
            agent_setting[:, -1] = costs

    # Final normalization of centers to ensure valid distributions
    centers = np.clip(agent_setting[:, :m_outcomes], 0, None)
    centers /= centers.sum(axis=1, keepdims=True)
    agent_setting[:, :m_outcomes] = centers
    agent_setting[:, -1] = np.maximum(agent_setting[:, -1], 0)

    return agent_setting
```
