```python
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting given principal's reward vector v and historical interaction logs.

    Args:
    - v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes
    - content: list of dicts, each dict has keys:
        'Contract': list or np.ndarray of length 5 (payments vector)
        'Principal Utility': float, principal's utility under contract (0 if rejected)
        'Agent Action': int, 1 if accepted contract, -1 if rejected

    Returns:
    - agent_setting: np.ndarray shape (n_actions, 6),
      each row: [p0, p1, p2, p3, p4, cost],
      where p_i are probabilities summing to 1 over outcomes,
      and cost >= 0 is agent action cost.
    """

    payoff_dim = v.shape[0]
    logs = pd.DataFrame(content)
    L = len(logs)

    contracts = np.vstack(logs['Contract'].values)  # shape (L, 5)
    principals_util = logs['Principal Utility'].values
    agent_actions = logs['Agent Action'].values

    accepted_idx = np.where(agent_actions == 1)[0]
    rejected_idx = np.where(agent_actions == -1)[0]

    # Fallback: if no accepted contracts, return uniform distribution and zero cost
    if len(accepted_idx) == 0:
        uniform_p = np.ones(payoff_dim) / payoff_dim
        return np.hstack([uniform_p.reshape(1, -1), np.array([[0.0]])])

    # Step 1: Infer candidate outcome distributions (p) per accepted contract
    # Heuristic: put all mass on outcome with max contract payment (max wage index)
    def infer_p_given_w(w):
        p = np.zeros_like(w)
        p[np.argmax(w)] = 1.0
        return p

    candidate_ps = np.array([infer_p_given_w(contracts[i]) for i in accepted_idx])

    # Step 2: Cluster candidate_ps into n_candidates groups (actions)
    n_candidates = min(7, len(accepted_idx))
    kmeans = KMeans(n_clusters=n_candidates, random_state=0, n_init=10)
    cluster_labels = kmeans.fit_predict(candidate_ps)
    p0 = kmeans.cluster_centers_
    # Ensure each cluster center is a valid probability vector (clip & normalize)
    p0 = np.clip(p0, 0, None)
    p0 /= p0.sum(axis=1, keepdims=True)

    # Step 3: Assign accepted contracts to best matching cluster by max expected wage
    assigns = np.full(L, -1, dtype=int)
    for i in accepted_idx:
        util = p0 @ contracts[i]
        assigns[i] = int(np.argmax(util))

    # Step 4: Build linear inequalities for cost vector c of length n_candidates

    # Constraints:
    # IR: For each accepted i assigned to a,
    #      c[a] <= p0[a] @ contracts[i]
    # IC: For each accepted i assigned to a, for all a' != a,
    #      c[a'] - c[a] >= p0[a']@contracts[i] - p0[a]@contracts[i]
    #      => -c[a'] + c[a] <= -(p0[a']@contracts[i] - p0[a]@contracts[i])
    # Rejection: For each rejected j, for all a,
    #      c[a] > p0[a] @ contracts[j]  =>  -c[a] < -p0[a]@contracts[j]
    #      approximate strict inequality with epsilon slack

    epsilon = 1e-6
    n = n_candidates

    A_ub_rows = []
    b_ub_rows = []

    # IR constraints (upper bounds on c[a])
    for a in range(n):
        idxs = np.where(assigns == a)[0]
        if len(idxs) > 0:
            vals = np.array([p0[a] @ contracts[i] for i in idxs])
            ub = vals.min()  # c[a] <= min_i p0[a]@w[i]
            row = np.zeros(n)
            row[a] = -1  # -c[a] <= -ub
            A_ub_rows.append(row)
            b_ub_rows.append(-ub)

    # IC constraints
    for i in accepted_idx:
        a = assigns[i]
        w = contracts[i]
        for a2 in range(n):
            if a2 == a:
                continue
            val = (p0[a2] @ w) - (p0[a] @ w)
            row = np.zeros(n)
            row[a2] = -1
            row[a] = +1
            A_ub_rows.append(row)
            b_ub_rows.append(-val)

    # Rejection constraints
    for j in rejected_idx:
        w = contracts[j]
        for a in range(n):
            val = p0[a] @ w
            row = np.zeros(n)
            row[a] = -1
            A_ub_rows.append(row)
            b_ub_rows.append(-val - epsilon)

    A_ub = np.vstack(A_ub_rows) if A_ub_rows else np.zeros((0, n))
    b_ub = np.array(b_ub_rows) if b_ub_rows else np.array([])

    bounds = [(0, None) for _ in range(n)]  # costs >= 0

    # Objective: minimize sum of costs for parsimony
    c_obj = np.ones(n)

    res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if not res.success:
        # fallback: for each action a, cost = min IR upper bound if any, else zero
        costs = np.zeros(n)
        for a in range(n):
            idxs = np.where(assigns == a)[0]
            if len(idxs) > 0:
                vals = np.array([p0[a] @ contracts[i] for i in idxs])
                costs[a] = vals.min()
        # add small slack to respect rejection constraints if possible
        if len(rejected_idx) > 0:
            slack = 1e-4
            for a in range(n):
                max_rej_util = max(p0[a] @ contracts[j] for j in rejected_idx)
                if costs[a] <= max_rej_util:
                    costs[a] = max_rej_util + slack
    else:
        costs = res.x
        # Adjust costs to strictly satisfy rejection constraints by small slack
        if len(rejected_idx) > 0:
            slack = 1e-4
            for a in range(n):
                max_rej_util = max(p0[a] @ contracts[j] for j in rejected_idx)
                if costs[a] <= max_rej_util:
                    costs[a] = max_rej_util + slack

    # Final agent setting matrix
    agent_setting = np.hstack([p0, costs.reshape(-1, 1)])

    # Safety normalization and clipping
    outcome_probs = agent_setting[:, :payoff_dim]
    outcome_probs = np.clip(outcome_probs, 0, None)
    sums = outcome_probs.sum(axis=1, keepdims=True)
    sums[sums == 0] = 1
    outcome_probs /= sums
    agent_setting[:, :payoff_dim] = outcome_probs

    agent_setting[:, -1] = np.maximum(agent_setting[:, -1], 0)

    return agent_setting
```
