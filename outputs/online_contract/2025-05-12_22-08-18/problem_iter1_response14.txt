```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import DBSCAN


def agent_solver(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [prob_5_outcomes + cost])
    consistent with historical logs where each log has:
        - Contract: 5-dimensional payment vector
        - Principal Utility: utility under the contract (0 if rejected)
        - Agent Action: 1 = accept, -1 = reject

    Parameters:
        v: np.ndarray of shape (5,) -- Principal's reward vector per outcome
        content: pd.DataFrame with columns ['Contract', 'Principal Utility', 'Agent Action']

    Returns:
        agent_setting: np.ndarray of shape (n_actions, 6)
            Each row corresponds to an action: first 5 entries are a prob. distribution,
            last entry is the cost of performing that action (>=0)
    """
    n_outcomes = v.shape[0]
    contracts = np.vstack(content['Contract'].to_numpy())  # (L,5)
    principal_utils = content['Principal Utility'].to_numpy()  # (L,)
    actions = content['Agent Action'].to_numpy()  # (L,)

    accept_mask = actions == 1
    reject_mask = actions == -1
    accepted_cs = contracts[accept_mask]
    accepted_us = principal_utils[accept_mask]
    rejected_cs = contracts[reject_mask]

    # Step 1: Infer outcome distributions p per accepted contract by fitting mini LP:
    # maximize payment vector p (prob distribution) s.t. fixed expected utility for agent.
    # Here, for each accepted contract compute the implied expected utility u_agent:
    # u_agent = p@w - c, but c unknown, so consider p to explain given c later.

    def infer_p(w, u_est, tol=1e-8):
        # Solve for p maximizing zero (feasibility) with p sum=1, p >=0, and p@w = u_est
        # Formulate as linprog minimizing zero, constraints:
        # A_eq: sum(p)=1, p@w =u_est
        m = len(w)
        c_obj = np.zeros(m)
        A_eq = np.vstack([np.ones(m), w])
        b_eq = np.array([1.0, u_est])
        bounds = [(0, 1) for _ in range(m)]
        res = linprog(c=c_obj, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p_sol = res.x
            # Numerical safeguard for sum and non-negativity
            p_sol = np.clip(p_sol, 0, 1)
            p_sol /= p_sol.sum()
            # Check constraints:
            if abs(p_sol.sum() - 1) > tol or abs(p_sol @ w - u_est) > tol:
                return None
            return p_sol
        else:
            return None

    # Step 2: To guess u_agent for each accepted contract, recall:
    # principal_utility_i = p_i @ (v - w_i)
    # agent utility u_i = p_i @ w_i - c_i = agent's expected payoff minus cost c_i
    # agent chooses acceptance iff u_i >= 0
    # We don't know p_i or c_i, but have principal utilities and contracts; cannot recover any u_i directly.
    # Instead, we'll initially assume zero cost and back-fit p_i by guessing u_agent_i = (p_i@w_i - c)
    # Since we do not know c or p_i, we approximate p_i by QP fitting on acceptance data.

    # Fall back to a heuristic: For each accepted contract,
    # approximate expected payout = principal_reward + principal utility + agent cost unknown,
    # assume cost zero temporarily and use principal reward u_p = sum over outcomes of (v-w)*p
    # This is complex, so abandon trying to infer p_i from single contracts.

    # Instead, do clustering on contracts corresponding to accepted logs, mapping them to outcome distributions:
    # Since agent accepts iff expected utility is >=0:
    # For contract w: agent picks action a if p_a @ w - c_a >= 0 and satisfies IC + IR.

    # Step 3: Cluster accepted contracts contracts_w into groups to define candidate actions:
    # We treat clusters of contracts as proxies for agent's actions inducing distinct p_a:
    # Use DBSCAN with eps tuned to yield reasonable cluster count adaptively.

    # Normalize contracts for clustering stability:
    contracts_norm = accepted_cs / (np.linalg.norm(accepted_cs, axis=1, keepdims=True) + 1e-8)
    dbscan = DBSCAN(eps=0.2, min_samples=5)
    cluster_labels = dbscan.fit_predict(contracts_norm)
    # treat each cluster as an action if label != -1
    valid_idx = cluster_labels != -1
    if np.sum(valid_idx) == 0:
        # no clusters found: fallback to all accepted contracts as one cluster
        cluster_labels = np.zeros(len(accepted_cs), dtype=int)
    else:
        # remove noise points from accepted_cs for inference
        accepted_cs = accepted_cs[valid_idx]
        cluster_labels = cluster_labels[valid_idx]

    unique_actions = np.unique(cluster_labels)
    n_actions = len(unique_actions)

    p_mat = np.zeros((n_actions, n_outcomes))  # to store inferred p_a per action
    c_vec = np.zeros(n_actions)

    # For each cluster (action), infer p_a as distribution maximizing the min agent utility needed to accept all cluster contracts:
    for i, act in enumerate(unique_actions):
        # contracts in this action cluster:
        idxs = np.where(cluster_labels == act)[0]
        w_act = accepted_cs[idxs]  # contracts for this action

        # Step 4: Infer p_a by solving feasibility & maximizing min utility margin over w_act
        # Variables: p (prob vector), c (cost, >=0)
        # Constraints: for all contracts in action cluster, accept => p @ w_i - c >= 0
        #               p sum=1, p >=0, c>=0
        # Objective: maximize min_i (p @ w_i - c) i.e. max min surplus

        # This is a linear max-min problem, transform to LP:
        # Introduce variable t = min_i (p @ w_i - c)
        # Constraints:
        #  For all i : p @ w_i - c >= t
        #  sum(p) =1, p>=0, c>=0
        # Maximize t

        # Setup LP in standard form (variables p(5) and c and t scalar):
        # Let x = [p0,p1,p2,p3,p4, c, t]
        # maximize t => minimize -t

        from scipy.optimize import linprog

        c_obj = np.zeros(n_outcomes + 2)
        c_obj[-1] = -1  # maximize t => minimize -t

        A_ub = []
        b_ub = []

        # - (p @ w_i - c) + t <= 0  =>  -p@w_i + c - t <=0
        # convert to matrix form row-wise
        for w_i in w_act:
            row = np.zeros(n_outcomes + 2)
            row[:n_outcomes] = -w_i
            row[n_outcomes] = 1.0  # for c
            row[n_outcomes + 1] = -1.0  # for t
            A_ub.append(row)
            b_ub.append(0.0)
        A_ub = np.vstack(A_ub)
        b_ub = np.array(b_ub)

        # Equality sum(p) = 1
        A_eq = np.zeros((1, n_outcomes + 2))
        A_eq[0, :n_outcomes] = 1.0
        b_eq = np.array([1.0])

        # Bounds
        # p_i >=0
        bounds = [(0, 1) for _ in range(n_outcomes)]
        bounds += [(0, None),  # c >=0
                   (None, None)] # t free

        res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub,
                      A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')

        if res.success:
            p_a = res.x[:n_outcomes]
            c_a = res.x[n_outcomes]
            # Normalize p in case small drift
            p_a = np.clip(p_a, 0, 1)
            p_a /= p_a.sum()
            c_a = max(0, c_a)
            p_mat[i] = p_a
            c_vec[i] = c_a
        else:
            # fallback: uniform p, zero cost
            p_mat[i] = np.ones(n_outcomes) / n_outcomes
            c_vec[i] = 0.0

    # Step 5: Validate rejection constraints:
    # For all rejected contracts, agent utility < 0 for all actions:
    # p_a @ w_j - c_a < 0 for all a, j (j: rejected contracts)
    if rejected_cs.shape[0] > 0:
        utilities_rej = p_mat @ rejected_cs.T - c_vec[:, np.newaxis]  # shape (n_actions, n_rejected)
        if np.any(utilities_rej >= -1e-8):
            # Invalidate infeasible situation, increase c to strictly reject
            adjustments = np.maximum(utilities_rej, 0) + 1e-4
            c_vec += adjustments.max(axis=1)

    # Step 6: Finalize agent setting matrix of shape (n_actions, 6)
    agent_setting = np.hstack([p_mat, c_vec[:, None]])

    return agent_setting
```
