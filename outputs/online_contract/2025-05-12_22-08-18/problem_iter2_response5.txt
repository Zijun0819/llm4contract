```python
import numpy as np
from sklearn.cluster import KMeans
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting given principal's reward vector v and historical interaction logs.
    
    Each action corresponds to:
      - a probability distribution over 5 outcomes (sums to 1),
      - a non-negative cost.
      
    Constraints to satisfy:
      - IR: For accepted contracts i assigned to action a,
            p_a @ w_i - c_a >= 0,
      - IC: For accepted contracts i assigned to action a and any other action a',
            p_a @ w_i - c_a >= p_a' @ w_i - c_a',
      - Rejection: For rejected contracts j,
            for all actions a: p_a @ w_j - c_a < 0 (strict),
    where w_i is the contract payment vector.

    Steps:
      1. Extract accepted and rejected indices.
      2. Infer candidate outcome distributions by clustering "best outcome indicators" on accepted contracts.
      3. Solve LP to find costs c satisfying IR, IC, rejection constraints.
      4. Add small slack to rejection constraints to ensure strict inequalities.
      5. Normalize outcome distributions and clip costs nonnegative.
    """

    m_outcomes = v.shape[0]
    L = len(content)

    # Extract contract payments, principal utilities, and agent actions as arrays
    contracts = np.array([log['Contract'] for log in content])  # shape (L, m_outcomes)
    agent_actions = np.array([log['Agent Action'] for log in content])  # shape (L,)

    accepted_idx = np.where(agent_actions == 1)[0]
    rejected_idx = np.where(agent_actions == -1)[0]

    # If no accepted contracts, return trivial agent setting: uniform distribution, zero cost
    if len(accepted_idx) == 0:
        uniform_p = np.ones(m_outcomes) / m_outcomes
        return np.hstack([uniform_p.reshape(1, -1), np.array([[0.0]])])

    # --- Step 1 & 2: Infer candidate outcome distributions for accepted contracts ---

    # For each accepted contract w_i, infer a pure strategy p_i putting all mass on max payout outcome index
    # This yields a "best response" outcome distribution for that contract
    def infer_p_given_contract(w):
        p = np.zeros_like(w)
        max_idx = np.argmax(w)
        p[max_idx] = 1.0
        return p

    candidate_ps = np.array([infer_p_given_contract(contracts[i]) for i in accepted_idx])

    # Choose number of clusters (actions) adaptively: min(7, number of accepted contracts)
    n_actions = min(7, len(accepted_idx))

    # Cluster candidate_ps to identify distinct agent actions
    kmeans = KMeans(n_clusters=n_actions, random_state=0, n_init=10)
    cluster_labels = kmeans.fit_predict(candidate_ps)
    p0 = kmeans.cluster_centers_

    # Ensure each cluster center is a valid probability distribution
    p0 = np.clip(p0, 0, None)
    p0 /= p0.sum(axis=1, keepdims=True)

    # Assign each accepted contract to the action achieving the highest expected payout p_a @ w_i
    assigns = np.full(L, -1, dtype=int)
    for i in accepted_idx:
        w = contracts[i]
        utilities = p0 @ w
        assigns[i] = int(np.argmax(utilities))

    # --- Step 3: Formulate LP constraints to find costs c ---

    n = n_actions
    epsilon = 1e-8  # small slack for strict inequalities (rejection)

    A_ub_rows = []
    b_ub_rows = []

    # IR constraints: For accepted contracts i assigned to a,
    # c[a] <= p0[a] @ w_i
    for a in range(n):
        idxs = np.where(assigns == a)[0]
        if len(idxs) > 0:
            vals = np.array([p0[a] @ contracts[i] for i in idxs])
            ub = vals.min()
            # convert c[a] <= ub to -c[a] <= -ub
            row = np.zeros(n)
            row[a] = -1
            A_ub_rows.append(row)
            b_ub_rows.append(-ub)
    # IC constraints: For each accepted contract i assigned to a, for each action a' != a:
    # c[a'] - c[a] >= p0[a'] @ w_i - p0[a] @ w_i
    # => -c[a'] + c[a] <= -(p0[a'] @ w_i - p0[a] @ w_i)
    for i in accepted_idx:
        a = assigns[i]
        w = contracts[i]
        for a2 in range(n):
            if a2 == a:
                continue
            val = (p0[a2] @ w) - (p0[a] @ w)
            row = np.zeros(n)
            row[a2] = -1
            row[a] = +1
            A_ub_rows.append(row)
            b_ub_rows.append(-val)

    # Rejection constraints: For each rejected contract j and all actions a,
    # p0[a] @ w_j - c[a] < 0
    # => c[a] > p0[a] @ w_j
    # => -c[a] < -p0[a] @ w_j - epsilon
    for j in rejected_idx:
        w = contracts[j]
        for a in range(n):
            val = p0[a] @ w
            row = np.zeros(n)
            row[a] = -1
            A_ub_rows.append(row)
            b_ub_rows.append(-val - epsilon)

    A_ub = np.vstack(A_ub_rows) if A_ub_rows else np.zeros((0, n))
    b_ub = np.array(b_ub_rows) if b_ub_rows else np.array([])

    bounds = [(0, None) for _ in range(n)]  # costs non-negative

    # Objective: minimize sum of costs to encourage parsimonious costs
    c_obj = np.ones(n)

    res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if not res.success:
        # Fallback: set costs as minimal IR upper bounds or zero if none
        costs = np.zeros(n)
        for a in range(n):
            idxs = np.where(assigns == a)[0]
            if len(idxs) > 0:
                vals = np.array([p0[a] @ contracts[i] for i in idxs])
                costs[a] = vals.min()
            else:
                costs[a] = 0.0
    else:
        costs = res.x

    # --- Step 4: Post-process costs to strictly reject rejected contracts ---

    if len(rejected_idx) > 0:
        slack = 1e-6
        for a in range(n):
            max_rej_util = max((p0[a] @ contracts[j]) for j in rejected_idx)
            if costs[a] <= max_rej_util:
                costs[a] = max_rej_util + slack

    # --- Step 5: Normalize outcome distributions and clip costs nonnegative ---

    p0 = np.clip(p0, 0, None)
    p0_sum = p0.sum(axis=1, keepdims=True)
    p0_sum[p0_sum == 0] = 1.0  # prevent division by zero if any row is zero
    p0 /= p0_sum

    costs = np.maximum(costs, 0)

    agent_setting = np.hstack([p0, costs.reshape(-1, 1)])

    return agent_setting
```
