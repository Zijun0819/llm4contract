```python
import numpy as np
from sklearn.cluster import AgglomerativeClustering

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting consistent with historical contract logs,
    with enhanced clustering robustness, adaptive pruning, and refined cost adjustments.

    Params:
        v (np.ndarray): Principal's value vector (length 5)
        content (list[dict]): each dict has keys: 'Contract' (list of 5),
                              'Principal Utility' (float),
                              'Agent Action' (1 or -1)
    Returns:
        np.ndarray: n_actions x 6 (5 probabilities + 1 cost)
    """

    m_outcomes = v.size
    L = len(content)

    # Prepare arrays
    contracts = np.array([log['Contract'] for log in content])  # (L,5)
    ag_actions = np.array([log['Agent Action'] for log in content])  # (L,)
    p_util = np.array([log['Principal Utility'] for log in content])  # (L,)

    accepted_ix = np.where(ag_actions == 1)[0]
    rejected_ix = np.where(ag_actions == -1)[0]

    # Handle no accepted contracts: trivial uniform action, zero cost
    if accepted_ix.size == 0:
        return np.hstack([np.ones((1, m_outcomes)) / m_outcomes, np.array([[0]])])

    accepted_w = contracts[accepted_ix]

    # Normalize accepted contracts for clustering (sum to 1)
    norm_w = accepted_w / (accepted_w.sum(axis=1, keepdims=True) + 1e-12)

    # Adaptive number of clusters based on acceptance count and diversity
    n_max_actions = max(1, min(10, int(np.sqrt(len(accepted_ix))), len(accepted_ix)))

    # Use average linkage clustering on normalized accepted contracts
    clustering = AgglomerativeClustering(n_clusters=n_max_actions, linkage='average')
    labels = clustering.fit_predict(norm_w)

    # For stability: iteratively refine clusters by removing outliers with low similarity
    # Compute cluster centroids and prune far points, repeat once for robustness
    def refine_clusters(norm_w, labels, n_clusters):
        for _ in range(1):
            new_labels = labels.copy()
            changed = False
            for c in range(n_clusters):
                cluster_points = norm_w[labels == c]
                if cluster_points.shape[0] == 0:
                    continue
                centroid = cluster_points.mean(axis=0)
                centroid /= centroid.sum() + 1e-12
                # Cosine similarity threshold for pruning outliers
                sims = cluster_points @ centroid / (np.linalg.norm(cluster_points, axis=1) * np.linalg.norm(centroid) + 1e-12)
                outliers = np.where(sims < 0.95)[0]
                if outliers.size > 0 and cluster_points.shape[0] > 3:
                    # Remove outliers by assigning to new cluster (-1)
                    idx_in_cluster = np.where(labels == c)[0]
                    outlier_global_ix = idx_in_cluster[outliers]
                    new_labels[outlier_global_ix] = -1
                    changed = True
            # Assign outliers as new clusters if any
            if changed:
                outlier_indices = np.where(new_labels == -1)[0]
                for i, oi in enumerate(outlier_indices):
                    new_labels[oi] = n_clusters + i
                n_clusters = new_labels.max() + 1
                labels = new_labels
            else:
                break
        return labels, n_clusters

    labels, n_max_actions = refine_clusters(norm_w, labels, n_max_actions)

    p_candidates = []
    costs_lower_bound = []

    # Compute cluster representatives and cost lower bounds with weighted averaging
    for c in range(n_max_actions):
        cluster_idx = accepted_ix[labels == c]
        if cluster_idx.size == 0:
            continue
        cluster_w = contracts[cluster_idx]

        # Weight by shifted principal utility to emphasize better contracts in cluster
        weights = p_util[cluster_idx] - p_util[cluster_idx].min() + 1
        avg_w = np.average(cluster_w, axis=0, weights=weights)

        # Project to simplex by clipping and normalizing
        p = np.clip(avg_w, 0, None)
        if p.sum() < 1e-12:
            p = np.ones(m_outcomes) / m_outcomes
        else:
            p = p / p.sum()

        p_candidates.append(p)

        # Cost lower bound: minimal p@w over cluster contracts (agent utility â‰¥0)
        costs_cluster = np.array([p @ contracts[i] for i in cluster_idx])
        min_cost = costs_cluster.min()
        costs_lower_bound.append(min_cost)

    p_candidates = np.array(p_candidates)
    costs_lower_bound = np.array(costs_lower_bound)

    # Handle rejected contracts: cost must be strictly > max p@w over rejected contracts
    if rejected_ix.size > 0:
        reject_contracts = contracts[rejected_ix]  # (#rej,5)
        p_w_rej = p_candidates @ reject_contracts.T  # shape (n_actions, #rej)
        c_rej_min = p_w_rej.max(axis=1) + 1e-6
    else:
        c_rej_min = np.zeros(len(p_candidates))

    costs = np.maximum(costs_lower_bound, c_rej_min)
    costs = np.maximum(costs, 0)

    # Validate each accepted contract is rationalized by some action
    def valid_action_for_contract(w_i):
        for a, (p, c) in enumerate(zip(p_candidates, costs)):
            util = p @ w_i - c
            if util >= -1e-8:
                return a
        return None

    # For accepted contracts not explained, add exact matching actions (normalized)
    for i in accepted_ix:
        w_i = contracts[i]
        if valid_action_for_contract(w_i) is None:
            p_new = w_i.copy()
            if p_new.sum() < 1e-12:
                p_new = np.ones(m_outcomes) / m_outcomes
            else:
                p_new = p_new / p_new.sum()
            c_new = p_new @ w_i
            p_candidates = np.vstack([p_candidates, p_new])
            costs = np.append(costs, c_new)

    # After augmentation, re-check rejection constraints and adjust costs if needed
    if rejected_ix.size > 0:
        reject_contracts = contracts[rejected_ix]
        p_w_rej = p_candidates @ reject_contracts.T
        for a in range(len(p_candidates)):
            max_rej_util = p_w_rej[a].max()
            if costs[a] < max_rej_util + 1e-8:
                costs[a] = max_rej_util + 1e-4  # small margin

    # Final normalization to ensure valid probability distributions
    p_candidates = np.clip(p_candidates, 0, None)
    p_sums = p_candidates.sum(axis=1, keepdims=True)
    p_sums[p_sums < 1e-12] = 1.0
    p_candidates /= p_sums

    # Assemble final agent setting matrix (n_actions x 6)
    agent_setting = np.hstack([p_candidates, costs[:, None]])

    return agent_setting
```
