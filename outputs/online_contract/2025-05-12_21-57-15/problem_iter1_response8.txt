```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering


def agent_solver_v2(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [outcome probabilities + cost])
    consistent with historical logs of contracts, principal utility & agent actions.

    Args:
        v (np.ndarray): principal's reward vector for 5 outcomes (shape=(5,))
        content (pd.DataFrame): logs with columns ['Contract', 'Principal Utility', 'Agent Action']

    Returns:
        np.ndarray: n x 6 matrix (n actions, 5 outcome probs + 1 cost)
    """
    m = len(v)  # 5 outcomes
    logs = content.copy()
    L = len(logs)

    # STEP 0: Parse contract matrix and arrays for faster ops
    contracts = np.stack(logs['Contract'].to_numpy())  # shape (L,5)
    p_utils_principal = logs['Principal Utility'].to_numpy()  # (L,)
    agent_actions = logs['Agent Action'].to_numpy()  # (L,), values in {1, -1}

    # Separate accepted and rejected logs for convenience
    accept_mask = agent_actions == 1
    reject_mask = agent_actions == -1

    contracts_accept = contracts[accept_mask]
    p_utils_accept = p_utils_principal[accept_mask]

    contracts_reject = contracts[reject_mask]

    # --- Step 1: Infer candidate outcome distributions p from accepted logs
    # Use mini LP for each accepted log to find an outcome distribution p:
    # Solve: find p (over outcomes), such that:
    #  sum p = 1
    #  u_agent = p @ w - c >= 0 (agent IR), but c unknown now,
    #  and principal utility = p@(v-w) >=0 from logs,
    #  so little info directly on p except: p @ w = u_agent + c
    # We'll approximate p by minimizing distance from normalized contract to principal rewards
    # or by projecting contracts towards v.

    # Instead, we use a data-driven approach: Cluster contracts_ACCEPT into groups as proxies
    # for different action outcome distributions, since agent picks action inducing p.

    # Use agglomerative clustering (more adaptive than KMeans, to discover n)
    # Target number of clusters max 10 but adaptive with elbow
    max_clusters = min(10, max(2, np.sum(accept_mask)//10))
    if max_clusters < 2:
        max_clusters = 2

    # Preprocess accepted contracts by normalizing relative to principal reward v
    normed_accept = contracts_accept / (np.linalg.norm(contracts_accept, axis=1, keepdims=True)+1e-8)

    linkage = AgglomerativeClustering(n_clusters=None, distance_threshold=0.15)
    clustering = linkage.fit(normed_accept)
    labels = clustering.labels_
    n_actions = len(np.unique(labels))

    # Compute centroid (mean contract) per action cluster:
    centroids = np.zeros((n_actions, m))
    for a in range(n_actions):
        centroids[a] = contracts_accept[labels == a].mean(axis=0)

    # --- Step 2: Infer p outcome distributions per action via LP:
    # For each centroid wage vector w_centroid, solve LP:
    # minimize ||p - p_init||_2 or just pick p to satisfy:
    #    sum p = 1
    #    p >= 0
    #    target: p @ w_centroid approx principal utility observed for this action cluster
    # We only have wages, no direct utility for the agent or cost.
    # Instead, approximate p by finding distribution p maximizing principal reward subject to p @ w_centroid fixed
    # As a heuristic, find p maximizing p @ v subject to p @ w_centroid == u_agent (unknown): we lack u_agent exactly.
    # Instead, approximate p as solution to max p @ v s.t. p @ w_centroid = median_p_w and p discrete probs sum 1.

    # Find median principal utility for each cluster as proxy
    c_action_costs = np.zeros(n_actions)
    p_actions = np.zeros((n_actions, m))

    for a in range(n_actions):
        mask_a = labels == a
        w_a = contracts_accept[mask_a]
        u_principal_a = p_utils_accept[mask_a]
        median_u_p = np.median(u_principal_a)  # proxy for agent utility lower bounds

        # LP to find p:
        # max p @ v s.t. p @ w_a.mean() = median_wage, sum p=1, p>=0
        w_cent = w_a.mean(axis=0)
        median_wage = w_cent @ np.ones(m) / m  # fallback baseline if zero
        median_wage = np.median(w_a @ np.ones(m))

        c = -v  # maximize p @ v == minimize -p @ v
        A_eq = np.array([np.ones(m), w_cent])
        b_eq = np.array([1, np.median(w_a @ w_cent)])

        bounds = [(0, 1) for _ in range(m)]
        res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')

        if res.success:
            p_hat = res.x
            p_hat[p_hat < 0] = 0
            p_hat = p_hat / p_hat.sum() if p_hat.sum() > 0 else np.ones(m)/m
            p_actions[a] = p_hat
        else:
            # fallback: uniform distribution
            p_actions[a] = np.ones(m) / m

        # --- estimate action cost as minimum agent cost to rationalize accept:
        # agent utility = p @ w - c >=0 => c <= p @ w
        # use min accepted wage perceived by action:
        min_p_wage = np.min(w_a @ p_actions[a]) if w_a.shape[0] > 0 else 0
        c_action_costs[a] = min_p_wage

    # --- Step 3: Enforce feasibility with rejection logs:
    # For rejected logs, for all inferred actions:
    # agent utility <0 =>  p_actions[a] @ w_rej - c_action_costs[a] < 0
    # which means c_action_costs[a] > max_{rej w} p_actions[a]@w

    if reject_mask.any():
        max_rej_utils = np.max(contracts_reject @ p_actions.T, axis=0)
        c_action_costs = np.maximum(c_action_costs, max_rej_utils + 1e-5)

    # --- Step 4: Normalize p_actions rows to sum to 1 just in case
    p_actions = np.clip(p_actions, 0, None)
    p_actions /= p_actions.sum(axis=1, keepdims=True)

    # --- Step 5: Ensure non-negative costs
    c_action_costs = np.clip(c_action_costs, 0, None)

    # Return n_actions x (5+1) matrix: outcome probs + cost
    agent_setting = np.hstack([p_actions, c_action_costs[:, None]])
    return agent_setting
```
