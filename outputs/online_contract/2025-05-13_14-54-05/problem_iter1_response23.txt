```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans


def agent_solver_v2(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer a valid agent setting (actions as outcome distributions + costs) consistent with 
    historical contract interaction logs, satisfying IR and IC constraints.
    
    Parameters:
    - v: principal reward vector for 5 outcomes, shape (5,)
    - content: DataFrame with columns ['Contract', 'Principal Utility', 'Agent Action']
      - Contract: list of 5 payments
      - Principal Utility: float, zero if rejected
      - Agent Action: 1 if accepted, -1 if rejected
    
    Returns:
    - agent_setting: ndarray shape (n_actions, 6), where first 5 cols are outcome probabilities,
      last col is agent cost (>=0). Rows correspond to inferred agent actions.
    """
    m_outcomes = v.shape[0]
    logs = content.reset_index(drop=True)
    
    # Prepare data arrays
    contracts = np.vstack(logs['Contract'].to_numpy())  # (L, 5)
    principals_util = logs['Principal Utility'].to_numpy()  # (L,)
    actions = logs['Agent Action'].to_numpy()  # (L,)
    L = len(logs)

    # Step 1: Candidate extraction from accepted contracts
    # We want to find outcome distributions p (prob vector) for agent actions,
    # and costs c >= 0, s.t. for accepted contracts expected agent utility = p @ w - c >= 0,
    # and for rejected contracts expected agent utility < 0 for all actions.
    # Also agent chooses action maximizing p@w - c for accepted contracts.

    # To infer plausible p's, solve LPs per accepted contract to find p:
    # max p @ w s.t p in simplex, p @ v = U_agent (unknown)
    # Since we don't know agent utility exactly, use principal utility approximation to guess p.

    # We use clustering on accepted contracts' payments to propose p's,
    # then optimize c's by LP to satisfy constraints from all logs.

    accepted_idx = np.where(actions == 1)[0]
    rejected_idx = np.where(actions == -1)[0]

    if len(accepted_idx) == 0:
        # No accepted contracts => trivial agent: one action with uniform outcome and zero cost
        p0 = np.ones((1, m_outcomes)) / m_outcomes
        c0 = np.zeros(1)
        return np.hstack([p0, c0[:, None]])

    # Use KMeans clustering on contracts from accepted logs to generate candidate action outcome probs.
    # Number of clusters adaptively chosen by elbow heuristic or limited max.
    max_actions = min(8, len(accepted_idx))  # cap for complexity
    acc_contracts = contracts[accepted_idx]

    # Normalize contracts by principal reward v to emphasize outcome structure
    normalized = acc_contracts / (v + 1e-8)  # prevent div zero
    normalized = np.clip(normalized, 0, None)  # remove negatives if any

    # Use silhouette heuristic to pick n_actions between 2 and max_actions
    from sklearn.metrics import silhouette_score
    best_k = 2
    best_sil = -1
    for k in range(2, max_actions + 1):
        km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(normalized)
        sil = silhouette_score(normalized, km.labels_)
        if sil > best_sil:
            best_sil = sil
            best_k = k
    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10).fit(normalized)

    # Cluster centers give rough shape, but not probabilities yet.
    centers = kmeans.cluster_centers_  # shape (k, 5)
    # To form p's: project cluster centers back to contracts, then find p s.t p@v approx center*v
    # Instead, approximate p as normalized positive centers (may not sum to 1), then project to simplex.

    def simplex_proj(x):
        # Project vector x to probability simplex
        u = np.sort(x)[::-1]
        cssv = np.cumsum(u)
        rho = np.nonzero(u * np.arange(1, len(x)+1) > (cssv - 1))[0][-1]
        theta = (cssv[rho] - 1) / (rho + 1)
        return np.maximum(x - theta, 0)

    p_candidates = []
    for c in centers:
        # Convert normalized center back to contract scale by element-wise multiply by v
        # Then get p by projecting center normalized by sum
        p_raw = c * v
        p = simplex_proj(p_raw)
        # Normalize to sum to 1
        if p.sum() < 1e-8:
            p = np.ones_like(p) / len(p)
        else:
            p = p / p.sum()
        p_candidates.append(p)
    p_candidates = np.array(p_candidates)  # shape (k,5)

    # Step 2: Optimize costs c >=0 for each action to satisfy IR and IC constraints on all logs
    # Variables: c = (c_1,...,c_k), costs for each action
    # Constraints:
    # For accepted logs i:
    # max_a [ p_a @ w_i - c_a ] >= 0   (agent accepts if some action is IR)
    # agent chooses action a_i: p_{a_i} @ w_i - c_{a_i} >= p_a @ w_i - c_a for all a (IC)
    # For rejected logs i:
    # max_a [ p_a @ w_i - c_a ] < 0

    # Step 2a: Assign accepted logs to best fitting action by max p@w
    action_assign = np.argmax(p_candidates @ contracts.T, axis=0)  # shape (L,)
    # We'll relabel only accepted logs' assigned actions:
    action_assign = action_assign.astype(int)

    # Step 2b: Set up LP to solve for costs c
    # Variables: c in R^k, costs >= 0
    # Constraints:
    # For each accepted i:
    # p_{a_i} @ w_i - c_{a_i} >= 0  (IR)
    # For all a: p_{a_i} @ w_i - c_{a_i} >= p_a @ w_i - c_a  (IC)
    # For each rejected i:
    # max_a (p_a @ w_i - c_a) < 0  -> p_a @ w_i - c_a <= -epsilon for all a

    epsilon = 1e-5
    n_actions = p_candidates.shape[0]

    # Construct matrices for linprog: minimize sum c (arbitrary objective for feasibility)
    c_obj = np.ones(n_actions)

    A_ub = []
    b_ub = []

    # IR constraints (accepted)
    # c_{a_i} <= p_{a_i} @ w_i
    for i in accepted_idx:
        a_i = action_assign[i]
        pi_wi = p_candidates[a_i] @ contracts[i]
        row = np.zeros(n_actions)
        row[a_i] = 1
        A_ub.append(row)
        b_ub.append(pi_wi)

    # IC constraints (accepted)
    # for each accepted i, for each a:
    # p_a_i @ w_i - c_a_i >= p_a @ w_i - c_a
    # -> c_a_i - c_a <= p_a_i @ w_i - p_a @ w_i
    for i in accepted_idx:
        a_i = action_assign[i]
        for a in range(n_actions):
            lhs = np.zeros(n_actions)
            lhs[a_i] = 1
            lhs[a] -= 1
            rhs = p_candidates[a_i] @ contracts[i] - p_candidates[a] @ contracts[i]
            A_ub.append(lhs)
            b_ub.append(rhs)

    # Rejection constraints (rejected)
    # For all rejected i: max_a p_a @ w_i - c_a < 0
    # Implement as: for all a: p_a @ w_i - c_a <= -epsilon
    for i in rejected_idx:
        w_i = contracts[i]
        for a in range(n_actions):
            row = np.zeros(n_actions)
            row[a] = -1
            rhs = - (p_candidates[a] @ w_i) - epsilon
            A_ub.append(row)
            b_ub.append(rhs)

    # Bounds: costs >= 0
    bounds = [(0, None) for _ in range(n_actions)]

    A_ub = np.array(A_ub)
    b_ub = np.array(b_ub)

    # Solve LP feasibility: minimize sum costs subject to constraints
    res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if not res.success:
        # Fallback: increase epsilon tolerance or reduce actions (try single action uniform)
        # Return trivial agent action with uniform outcomes and zero cost
        p0 = np.ones((1, m_outcomes)) / m_outcomes
        c0 = np.zeros(1)
        return np.hstack([p0, c0[:, None]])

    c_sol = res.x

    # Finally, output agent setting matrix: (n_actions, 6) = [p | cost]
    agent_setting = np.hstack([p_candidates, c_sol[:, None]])

    # Sanity fix: ensure each p sums to 1
    agent_setting[:, :m_outcomes] /= agent_setting[:, :m_outcomes].sum(axis=1, keepdims=True) + 1e-12

    # Enforce costs nonnegative (should be guaranteed)
    agent_setting[:, -1] = np.maximum(agent_setting[:, -1], 0)

    return agent_setting
```
