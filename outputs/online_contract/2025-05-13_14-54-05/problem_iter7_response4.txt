```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import pairwise_distances
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    L = len(content)

    # Separate accepted and rejected logs
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if not accepted_logs:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accepted_contracts = np.array([log['Contract'] for log in accepted_logs])

    # Normalize accepted contracts for clustering: L1 norm (sum to 1)
    # Add small epsilon to avoid division by zero
    eps = 1e-12
    accepted_norm = accepted_contracts / (accepted_contracts.sum(axis=1, keepdims=True) + eps)

    # Adaptive DBSCAN clustering on normalized contracts with L1 distance
    # Use elbow method on eps parameter to find stable clusters
    # Start with small eps and increase until reasonable number of clusters found

    def dbscan_cluster(eps_val):
        clustering = DBSCAN(eps=eps_val, min_samples=3, metric='manhattan')
        labels = clustering.fit_predict(accepted_norm)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        return labels, n_clusters

    eps_candidates = np.linspace(0.05, 0.3, 20)
    best_eps = None
    best_labels = None
    best_n_clusters = 0
    max_clusters = min(10, len(accepted_contracts))
    for eps_val in eps_candidates:
        labels, n_clusters = dbscan_cluster(eps_val)
        # pick eps with cluster count between 1 and max_clusters and minimal noise
        noise_count = np.sum(labels == -1)
        if 1 <= n_clusters <= max_clusters and noise_count < len(accepted_contracts) * 0.2:
            best_eps = eps_val
            best_labels = labels
            best_n_clusters = n_clusters
            break
    # fallback if no suitable eps found
    if best_labels is None:
        # Use Agglomerative clustering fallback as in v1
        from sklearn.cluster import AgglomerativeClustering
        best_n_clusters = min(5, len(accepted_contracts))
        clustering = AgglomerativeClustering(n_clusters=best_n_clusters)
        best_labels = clustering.fit_predict(accepted_contracts)
        noise_mask = np.zeros(len(accepted_contracts), dtype=bool)
    else:
        noise_mask = best_labels == -1

    # Assign noise points (if any) to nearest cluster center by L1 distance
    ps = []
    for a in range(best_n_clusters):
        cluster_points = accepted_norm[best_labels == a]
        if cluster_points.shape[0] == 0:
            # empty cluster, assign uniform
            ps.append(np.ones(m) / m)
        else:
            center = cluster_points.mean(axis=0)
            # normalize center to sum to 1
            s = center.sum()
            if s > eps:
                center /= s
            else:
                center = np.ones(m) / m
            ps.append(center)
    ps = np.array(ps)  # shape (actions, m)

    # Assign noise points to nearest cluster by L1 distance
    if noise_mask.any():
        noise_points = accepted_norm[noise_mask]
        dist_noise_to_centers = pairwise_distances(noise_points, ps, metric='manhattan')
        assign_noise = dist_noise_to_centers.argmin(axis=1)
        # Update labels for noise points
        best_labels = best_labels.copy()
        best_labels[noise_mask] = assign_noise

    # Now best_labels has no noise points
    # Recalculate cluster centers with all assigned points (including former noise)
    for a in range(best_n_clusters):
        cluster_points = accepted_norm[best_labels == a]
        if cluster_points.shape[0] == 0:
            ps[a] = np.ones(m) / m
        else:
            center = cluster_points.mean(axis=0)
            s = center.sum()
            if s > eps:
                center /= s
            else:
                center = np.ones(m) / m
            ps[a] = center

    # Now infer costs via LP enforcing IR and IC strictly with margin
    # Variables: costs for each action (length best_n_clusters), >=0
    # Constraints:
    # IR: For each accepted contract i assigned to action a_i:
    #     p_a_i @ w_i - cost_a_i >= margin
    # IC: For each rejected contract w_j:
    #     For all a: p_a @ w_j - cost_a <= -margin

    margin = 1e-6  # small positive margin for strict inequalities

    n_actions = best_n_clusters
    costs_var_num = n_actions

    # Build constraints matrices for linprog: minimize sum(costs) (arbitrary objective)
    # with constraints as above

    # Objective: minimize sum of costs (to keep costs minimal)
    c = np.ones(costs_var_num)

    # IR constraints: cost_a <= p_a @ w_i - margin  => cost_a - p_a @ w_i <= -margin
    # rewritten as: cost_a - p_a @ w_i <= -margin
    # We put constraints in form A_ub x <= b_ub
    A_ub = []
    b_ub = []

    # For each accepted contract i:
    for i, log in enumerate(accepted_logs):
        w = np.array(log['Contract'])
        a_i = best_labels[i]
        p_a = ps[a_i]
        val = p_a @ w
        # cost_a_i - val <= -margin
        row = np.zeros(costs_var_num)
        row[a_i] = 1.0
        A_ub.append(row)
        b_ub.append(val - margin)

    # IC constraints: For each rejected contract w_j, for all a:
    # p_a @ w_j - cost_a <= -margin  => cost_a >= p_a @ w_j + margin
    # Rewrite as -cost_a + p_a @ w_j <= -margin
    # So for each rejected contract and each action:
    if rejected_logs:
        rejected_contracts = np.array([log['Contract'] for log in rejected_logs])
        for w in rejected_contracts:
            for a in range(n_actions):
                p_a = ps[a]
                val = p_a @ w
                row = np.zeros(costs_var_num)
                row[a] = -1.0
                A_ub.append(row)
                b_ub.append(-val - margin)

    A_ub = np.array(A_ub)
    b_ub = np.array(b_ub)

    # Bounds: costs >= 0
    bounds = [(0, None) for _ in range(costs_var_num)]

    # Solve LP
    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if res.success:
        costs = res.x
    else:
        # fallback: set costs to max IR lower bounds and max rejected utilities + margin
        costs = np.zeros(n_actions)
        # IR lower bounds
        for a in range(n_actions):
            idxs = np.where(best_labels == a)[0]
            if idxs.size > 0:
                vals = [ps[a] @ np.array(accepted_logs[i]['Contract']) for i in idxs]
                costs[a] = max(costs[a], min(vals) - margin)
        # IC lower bounds
        if rejected_logs:
            rejected_contracts = np.array([log['Contract'] for log in rejected_logs])
            rej_utilities = ps @ rejected_contracts.T  # shape (actions, rejected_count)
            min_costs_from_rej = rej_utilities.max(axis=1) + margin
            costs = np.maximum(costs, min_costs_from_rej)
        costs = np.maximum(costs, 0)

    # Final agent setting matrix: each row [p1, ..., p5, cost]
    agent_setting = np.hstack([ps, costs.reshape(-1, 1)])

    return agent_setting
```
