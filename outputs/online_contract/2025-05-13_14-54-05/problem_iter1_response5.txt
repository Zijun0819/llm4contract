```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (actions with outcome distributions and costs)
    consistent with historical interaction logs under IR and IC constraints.

    Parameters:
    - v: np.ndarray, principal's reward vector for 5 outcomes (length 5).
    - content: list of dicts, each with keys:
        'Contract': list of 5 payments,
        'Principal Utility': float,
        'Agent Action': 1 (accept) or -1 (reject).

    Returns:
    - agent_setting: np.ndarray, n_actions x 6 matrix,
      first 5 columns probability distributions over outcomes,
      last column agent cost for that action (>=0).
    """

    m = len(v)  # number of outcomes (5)
    logs_df = pd.DataFrame(content)
    L = len(logs_df)

    # Separate accepted and rejected logs
    accept_df = logs_df[logs_df['Agent Action'] == 1]
    reject_df = logs_df[logs_df['Agent Action'] == -1]

    # Extract contracts and principal utilities for accepted logs
    accept_contracts = np.vstack(accept_df['Contract'].to_numpy())
    accept_putils = accept_df['Principal Utility'].to_numpy()

    # Step 1: For each accepted contract, solve LP to find a plausible outcome distribution p:
    # maximize agent expected utility = p @ wage - cost, under p @ 1 = 1,
    # p @ v = principal utility (accept_putils), and 0 ≤ p ≤ 1.
    # We solve: find p with constraints:
    #   sum p =1,
    #   p @ v = principal_utility (known),
    #   p >= 0,
    # and minimize norm(p - wage/mean(wage)) for stability.
    #
    # If infeasible, fallback to closest feasible p by relaxation.

    def solve_p_for_accept(wage: np.ndarray, pu: float) -> np.ndarray:
        # Solve LP:
        # min ||p - p_init||_2 (not linear, so approximate)
        # subject to sum p = 1, p @ v = pu, 0 ≤ p ≤1
        # Use linear programming to find any feasible p:
        # We'll minimize 0 (feasibility problem).
        c = np.zeros(m)
        A_eq = np.vstack([np.ones(m), v])
        b_eq = np.array([1.0, pu])
        bounds = [(0,1) for _ in range(m)]
        res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            return res.x
        else:
            # fallback: project p_init = wage/mean(wage) onto constraints (approx)
            p_init = wage / (wage.sum() + 1e-8)
            # LP to min ||p - p_init||_1 subject to constraints:
            # min sum abs(p_i - p_init_i)
            # To keep LP linear, use auxiliary variables:
            c = np.ones(m*2)
            # p_i - p_init_i = u_i - v_i, u_i,v_i ≥ 0
            # p = u_i - v_i + p_init_i
            # sum p = 1, p @ v = pu, 0 ≤ p ≤ 1
            # But linearizing L1 norm is complicated, skip fallback here.
            # Instead, return uniform distribution to keep feasible.
            return np.ones(m) / m

    candidate_ps = np.array([solve_p_for_accept(w, pu) for w, pu in zip(accept_contracts, accept_putils)])

    # Step 2: Cluster candidate_ps to find representative actions
    # Dynamically determine number of clusters based on elbow method heuristic
    max_clusters = min(10, len(candidate_ps))
    inertias = []
    for k in range(1, max_clusters+1):
        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(candidate_ps)
        inertias.append(kmeans.inertia_)
    # Simple elbow detection: find k where inertia drop slows
    deltas = np.diff(inertias)
    if len(deltas) == 0:
        n_actions = 1
    else:
        # Pick k with max delta drop above half the max delta
        threshold = max(deltas) * 0.5
        candidates = np.where(deltas < threshold)[0]
        n_actions = candidates[0]+1 if len(candidates)>0 else max_clusters

    kmeans = KMeans(n_clusters=n_actions, random_state=0, n_init=20).fit(candidate_ps)
    p_centers = kmeans.cluster_centers_

    # Step 3: Assign each accepted contract to closest action in terms of L2 distance
    accept_assign = kmeans.predict(candidate_ps)

    # Step 4: Infer cost per action:
    # For action a, cost c_a = min_{i assigned to a} expected payment p_a @ w_i - agent utility (≥0)
    # Agent utility for accept logs ≥ 0 by definition
    costs = np.zeros(n_actions)
    for a in range(n_actions):
        idxs = np.where(accept_assign == a)[0]
        if len(idxs) == 0:
            # No assigned samples, set cost = 0 (lowest)
            costs[a] = 0.
            continue
        wages_a = accept_contracts[idxs]  # shape (#assigned, m)
        # agent utility = p_a @ w_i - c_a ≥ 0 => c_a ≤ min_i p_a @ w_i
        costs[a] = np.min(wages_a @ p_centers[a])

    # Step 5: Enforce rejection consistency:
    # For any rejected contract w_r, agent utility < 0 for all actions:
    # p_a @ w_r - c_a < 0 => c_a > p_a @ w_r
    if len(reject_df) > 0:
        reject_contracts = np.vstack(reject_df['Contract'].to_numpy())
        # For each action a, get max p_a @ w_r over rejected contracts
        rej_max_util = np.max(reject_contracts @ p_centers.T, axis=0)  # shape (n_actions,)
        # Adjust costs to satisfy IR and rejection IC:
        costs = np.maximum(costs, rej_max_util + 1e-6)  # small margin for strictness

    # Step 6: Normalize p_centers rows to sum to 1 (numerical precaution)
    p_centers = np.clip(p_centers, 0, None)
    p_centers = p_centers / (p_centers.sum(axis=1, keepdims=True) + 1e-12)

    # Step 7: Assemble agent setting (p | cost)
    agent_setting = np.hstack([p_centers, costs.reshape(-1,1)])

    return agent_setting
```
