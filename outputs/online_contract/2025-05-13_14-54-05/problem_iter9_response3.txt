```python
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy.optimize import linprog

def agent_solver(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    logs_df = pd.DataFrame(content)

    # Separate accepted and rejected logs
    accept_df = logs_df[logs_df['Agent Action'] == 1].copy()
    reject_df = logs_df[logs_df['Agent Action'] == -1].copy()

    # If no accepted logs, return trivial uniform distribution with zero cost
    if len(accept_df) == 0:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accept_contracts = np.vstack(accept_df['Contract'].to_numpy())
    accept_putils = accept_df['Principal Utility'].to_numpy()
    reject_contracts = np.vstack(reject_df['Contract'].to_numpy()) if len(reject_df) > 0 else np.empty((0, m))

    # Step 1: For each accepted contract, infer a plausible outcome distribution p (prob vector)
    # Solve LP for each accepted contract:
    #   sum p = 1
    #   p @ (v - w) = principal utility
    #   0 <= p <= 1
    # This ensures p explains the observed principal utility given contract w.

    def solve_p_for_accept(w: np.ndarray, pu: float) -> np.ndarray:
        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1.0, pu])
        bounds = [(0, 1) for _ in range(m)]
        res = linprog(c=np.zeros(m), A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = np.clip(res.x, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m) / m
            return p
        else:
            # fallback uniform distribution if infeasible
            return np.ones(m) / m

    candidate_ps = np.array([solve_p_for_accept(w, pu) for w, pu in zip(accept_contracts, accept_putils)])

    # Step 2: Cluster candidate_ps to find representative actions (outcome distributions)
    # Use weighted clustering with contract weights proportional to principal utilities (shifted and clipped)
    # to emphasize more informative accepted logs.

    # Weights: shift principal utilities to positive scale, add small epsilon
    pu_min = accept_putils.min()
    weights = accept_putils - pu_min + 1e-3
    weights = np.clip(weights, 1e-3, None)

    max_clusters = min(10, len(candidate_ps))
    if max_clusters == 1:
        n_actions = 1
        p_centers = candidate_ps.mean(axis=0, keepdims=True)
        accept_assign = np.zeros(len(candidate_ps), dtype=int)
    else:
        inertias = []
        km_models = []
        for k in range(1, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=0, n_init=20)
            # sklearn KMeans does not support sample_weight before v1.0, so we do weighted sampling
            # Use weighted sampling to approximate weights
            # If sklearn supports sample_weight, replace below with sample_weight=weights
            kmeans.fit(candidate_ps, sample_weight=weights)
            inertias.append(kmeans.inertia_)
            km_models.append(kmeans)
        inertias = np.array(inertias)
        # Elbow heuristic: find k where relative reduction of inertia slows down
        deltas = -np.diff(inertias)  # positive reductions
        if len(deltas) >= 2:
            ratios = deltas[1:] / (deltas[:-1] + 1e-12)
            elbow_candidates = np.where(ratios < 0.5)[0] + 2  # +2 because ratios index starts at 2 clusters
            n_actions = elbow_candidates[0] if len(elbow_candidates) > 0 else max_clusters
        elif len(deltas) == 1:
            n_actions = 1 if deltas[0] < 1e-3 else 2
        else:
            n_actions = 1
        kmeans = km_models[n_actions - 1]
        p_centers = kmeans.cluster_centers_
        accept_assign = kmeans.predict(candidate_ps)

    # Step 3: Compute costs from IR constraints (agent rationality on accepted contracts)
    # For each action a:
    #   cost_a <= min_i (p_a @ w_i) for accepted contracts i assigned to a
    #   cost_a >= 0
    costs = np.zeros(n_actions)
    for a in range(n_actions):
        idxs = np.where(accept_assign == a)[0]
        if len(idxs) == 0:
            costs[a] = 0.0
            continue
        p_a = p_centers[a]
        payoffs = accept_contracts[idxs] @ p_a  # shape (#assigned,)
        # The agent's cost must be <= min payoff among accepted contracts assigned to this action
        # But agent utility = payoff - cost >=0 => cost <= payoff
        costs[a] = max(0.0, np.min(payoffs))

    # Step 4: Enforce IC constraints from rejected contracts:
    # For each rejected contract w_r and action a:
    #   agent utility < 0 => p_a @ w_r - cost_a < 0 => cost_a > p_a @ w_r
    # So cost_a > max_r p_a @ w_r for all rejected contracts r
    if reject_contracts.shape[0] > 0:
        rej_utils = reject_contracts @ p_centers.T  # shape (#rej, n_actions)
        max_rej_util = np.max(rej_utils, axis=0)  # max over rejected contracts per action
        # Add margin proportional to scale for strict inequality
        margin = np.maximum(1e-7, 1e-4 * np.maximum(1.0, max_rej_util))
        costs = np.maximum(costs, max_rej_util + margin)

    # Step 5: Iterative global cost refinement to better fit IR/IC constraints
    # Solve LP to minimize sum of slack variables for IR and IC constraints simultaneously

    # Prepare LP:
    # Variables: costs (n_actions), slacks_ir (len(accept_df)), slacks_ic (len(reject_df)*n_actions)
    # Objective: minimize sum of slacks_ir + sum of slacks_ic
    # Constraints:
    #   For accepted logs i assigned to action a_i:
    #       cost_a_i <= p_a_i @ w_i + slack_ir_i, slack_ir_i >= 0
    #   For rejected logs r and all actions a:
    #       cost_a >= p_a @ w_r - slack_ic_r,a, slack_ic_r,a >= 0

    from scipy.optimize import linprog

    n_accept = len(accept_df)
    n_reject = len(reject_df)
    n_vars = n_actions + n_accept + n_reject * n_actions  # costs + slacks_ir + slacks_ic

    c = np.zeros(n_vars)
    c[n_actions:] = 1.0  # minimize sum of all slack variables

    A_ub = []
    b_ub = []

    # IR constraints: cost_a - slack_ir_i <= p_a @ w_i
    # For each accepted log i:
    #   cost_{a_i} - slack_ir_i <= p_{a_i} @ w_i
    # => cost_{a_i} - slack_ir_i - p_{a_i} @ w_i <= 0
    # Rearranged for linprog (A_ub x <= b_ub):
    #   cost_{a_i} - slack_ir_i <= p_{a_i} @ w_i

    for i in range(n_accept):
        row = np.zeros(n_vars)
        a_i = accept_assign[i]
        row[a_i] = 1.0  # cost_a
        row[n_actions + i] = -1.0  # - slack_ir_i
        rhs = accept_contracts[i] @ p_centers[a_i]
        A_ub.append(row)
        b_ub.append(rhs)

    # IC constraints: cost_a + slack_ic_r,a >= p_a @ w_r
    # => -cost_a - slack_ic_r,a <= -p_a @ w_r
    # slack_ic_r,a >= 0
    # For each rejected log r and action a:

    for r in range(n_reject):
        for a in range(n_actions):
            row = np.zeros(n_vars)
            row[a] = -1.0  # -cost_a
            slack_idx = n_actions + n_accept + r * n_actions + a
            row[slack_idx] = -1.0  # - slack_ic_r,a
            rhs = - (reject_contracts[r] @ p_centers[a])
            A_ub.append(row)
            b_ub.append(rhs)

    A_ub = np.array(A_ub)
    b_ub = np.array(b_ub)

    bounds = [(0, None)] * n_vars  # all variables non-negative, costs and slacks

    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

    if res.success:
        costs_refined = res.x[:n_actions]
        costs_refined = np.maximum(costs_refined, 0.0)
        costs = costs_refined
    # else fallback to previous costs

    # Step 6: Normalize p_centers rows to sum to 1 (numerical safety)
    p_centers = np.clip(p_centers, 0, None)
    row_sums = p_centers.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1.0
    p_centers /= row_sums

    # Step 7: Assemble agent setting matrix: [p1,...,p5, cost]
    agent_setting = np.hstack([p_centers, costs.reshape(-1, 1)])

    return agent_setting
```
