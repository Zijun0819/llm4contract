```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from scipy.optimize import linprog

def agent_solver(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    logs_df = pd.DataFrame(content)

    accept_df = logs_df[logs_df['Agent Action'] == 1]
    reject_df = logs_df[logs_df['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if len(accept_df) == 0:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accept_contracts = np.vstack(accept_df['Contract'].to_numpy())
    accept_putils = accept_df['Principal Utility'].to_numpy()
    reject_contracts = np.vstack(reject_df['Contract'].to_numpy()) if len(reject_df) > 0 else np.empty((0, m))

    # Step 1: Infer plausible outcome distributions p for accepted contracts via LP
    def solve_p_for_accept(w: np.ndarray, pu: float) -> np.ndarray:
        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1.0, pu])
        bounds = [(0, 1) for _ in range(m)]
        res = linprog(c=np.zeros(m), A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m) / m
            return p
        else:
            return np.ones(m) / m

    candidate_ps = np.array([solve_p_for_accept(w, pu) for w, pu in zip(accept_contracts, accept_putils)])

    # Step 2: Cluster candidate_ps using DBSCAN with adaptive epsilon and weighting by frequency
    # To find representative outcome distributions (actions)
    # Use cosine distance to cluster probability vectors tightly

    from sklearn.metrics.pairwise import cosine_distances

    # Compute pairwise cosine distances
    dist_matrix = cosine_distances(candidate_ps)

    # Adaptive epsilon: median of non-zero distances multiplied by a small factor
    nonzero_dists = dist_matrix[np.triu_indices_from(dist_matrix, k=1)]
    median_dist = np.median(nonzero_dists) if len(nonzero_dists) > 0 else 0.1
    eps = median_dist * 0.35 if median_dist > 0 else 0.1

    # Weight points by counts of identical candidate_ps to emphasize dense clusters
    # Here, approximate weighting by unique rows and counts
    unique_ps, inverse_idx, counts = np.unique(np.round(candidate_ps, decimals=6), axis=0, return_inverse=True, return_counts=True)

    # Run DBSCAN on unique points with eps and min_samples=1 to allow small clusters
    clustering = DBSCAN(eps=eps, min_samples=1, metric='cosine').fit(unique_ps)

    labels = clustering.labels_
    n_actions = labels.max() + 1

    # For each cluster, compute weighted average p using counts as weights
    p_centers = np.zeros((n_actions, m))
    for a in range(n_actions):
        members = np.where(labels == a)[0]
        if len(members) == 0:
            continue
        weights = counts[members]
        weighted_p = np.average(unique_ps[members], axis=0, weights=weights)
        weighted_p = np.clip(weighted_p, 0, None)
        s = weighted_p.sum()
        if s > 0:
            weighted_p /= s
        else:
            weighted_p = np.ones(m) / m
        p_centers[a] = weighted_p

    # Assign each candidate_p to nearest cluster center (cosine similarity)
    # To assign accepted contracts to actions
    def assign_to_centers(ps, centers):
        # cosine similarity = 1 - cosine distance
        dist = cosine_distances(ps, centers)
        return np.argmin(dist, axis=1)

    accept_assign = assign_to_centers(candidate_ps, p_centers)

    # Step 3: Compute costs from IR constraints (agent rationality on accepted contracts)
    # For each action a:
    #   cost_a <= min_i (p_a @ w_i) for accepted contracts i assigned to a
    #   cost_a >= 0
    costs = np.zeros(n_actions)
    for a in range(n_actions):
        idxs = np.where(accept_assign == a)[0]
        if len(idxs) == 0:
            costs[a] = 0.0
            continue
        p_a = p_centers[a]
        payoffs = accept_contracts[idxs] @ p_a  # shape (#assigned,)
        costs[a] = max(0.0, np.min(payoffs))

    # Step 4: Enforce IC constraints from rejected contracts:
    # For each rejected contract w_r and action a:
    #   agent utility < 0 => p_a @ w_r - cost_a < 0 => cost_a > p_a @ w_r
    # So cost_a > max_r p_a @ w_r for all rejected contracts r
    if reject_contracts.shape[0] > 0:
        rej_utils = reject_contracts @ p_centers.T  # shape (#rej, n_actions)
        max_rej_util = np.max(rej_utils, axis=0)  # max over rejected contracts per action
        margin = np.maximum(1e-7, 1e-4 * np.maximum(1.0, max_rej_util))
        costs = np.maximum(costs, max_rej_util + margin)

    costs = np.maximum(costs, 0.0)

    # Step 5: Normalize p_centers rows to sum to 1 robustly
    p_centers = np.clip(p_centers, 0, None)
    row_sums = p_centers.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1.0
    p_centers /= row_sums

    # Step 6: Assemble agent setting matrix: [p1,...,p5, cost]
    agent_setting = np.hstack([p_centers, costs.reshape(-1, 1)])

    return agent_setting
```
