```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import pairwise_distances
from sklearn.neighbors import NearestCentroid
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    L = len(content)

    # Separate accepted and rejected logs
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if not accepted_logs:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accepted_contracts = np.array([log['Contract'] for log in accepted_logs])

    # Normalize contracts by their L1 norm to get outcome pattern vectors for clustering
    accepted_norms = accepted_contracts.sum(axis=1, keepdims=True)
    accepted_norms[accepted_norms < 1e-12] = 1.0  # avoid division by zero
    accepted_patterns = accepted_contracts / accepted_norms

    # Adaptive DBSCAN clustering with elbow method to select eps
    # Compute pairwise distances between normalized patterns
    dists = pairwise_distances(accepted_patterns, metric='euclidean')
    # Extract upper triangle distances for elbow detection
    tri_upper = dists[np.triu_indices_from(dists, k=1)]
    tri_upper = np.sort(tri_upper)

    # Elbow detection heuristic: find eps where second derivative is max
    if len(tri_upper) < 3:
        eps_candidates = tri_upper[-1] if len(tri_upper) > 0 else 0.1
    else:
        diffs = np.diff(tri_upper)
        diffs2 = np.diff(diffs)
        elbow_idx = np.argmax(diffs2) + 1
        eps_candidates = tri_upper[elbow_idx]

    # Use eps_candidates but bound it reasonably
    eps_candidates = max(min(eps_candidates, 0.3), 0.01)

    # Try DBSCAN with eps_candidates and min_samples=2 (to avoid too many small clusters)
    clustering = DBSCAN(eps=eps_candidates, min_samples=2, metric='euclidean')
    labels = clustering.fit_predict(accepted_patterns)

    # If DBSCAN fails to cluster (all noise), fallback to single cluster
    if np.all(labels == -1):
        labels = np.zeros(len(accepted_patterns), dtype=int)

    # Extract clusters ignoring noise (-1)
    unique_labels = set(labels)
    if -1 in unique_labels:
        unique_labels.remove(-1)
    if len(unique_labels) == 0:
        unique_labels = {0}
        labels = np.zeros(len(accepted_patterns), dtype=int)

    # For noise points, assign to nearest cluster center (if any clusters exist)
    clustered_idxs = np.where(labels != -1)[0]
    noise_idxs = np.where(labels == -1)[0]

    # Compute cluster centers in pattern space
    cluster_centers = []
    for ul in unique_labels:
        cluster_centers.append(accepted_patterns[labels == ul].mean(axis=0))
    cluster_centers = np.vstack(cluster_centers)

    if len(noise_idxs) > 0 and len(cluster_centers) > 0:
        # Assign noise points to nearest cluster center
        noise_points = accepted_patterns[noise_idxs]
        dist_noise_to_centers = pairwise_distances(noise_points, cluster_centers, metric='euclidean')
        nearest_clusters = dist_noise_to_centers.argmin(axis=1)
        for idx, c_idx in zip(noise_idxs, nearest_clusters):
            labels[idx] = list(unique_labels)[c_idx]

    # Recompute unique labels and cluster centers after assignment
    unique_labels = sorted(set(labels))
    cluster_centers = []
    for ul in unique_labels:
        cluster_centers.append(accepted_patterns[labels == ul].mean(axis=0))
    cluster_centers = np.vstack(cluster_centers)

    n_actions = len(unique_labels)

    # Now estimate payment vectors per action by averaging accepted contracts assigned to cluster
    action_payments = np.zeros((n_actions, m))
    for i, ul in enumerate(unique_labels):
        idxs = np.where(labels == ul)[0]
        if len(idxs) > 0:
            action_payments[i] = accepted_contracts[idxs].mean(axis=0)
        else:
            # fallback uniform if no assigned contracts
            action_payments[i] = np.ones(m) / m

    # Normalize each action payment vector to be non-negative and sum to 1 (probabilities)
    eps = 1e-12
    ps = np.maximum(action_payments, 0)
    sums = ps.sum(axis=1, keepdims=True)
    sums[sums < eps] = 1.0
    ps /= sums

    # Prepare LP to find cost vector c = [c_1,...,c_n_actions]
    # Variables: costs per action
    # Constraints:
    # IR for accepted contracts:
    # For each accepted contract w assigned to action a:
    # p_a @ w - c_a >= 0  =>  c_a <= p_a @ w
    # IC for rejected contracts:
    # For each rejected contract w:
    # max_a (p_a @ w - c_a) < 0
    # => For all a: c_a > p_a @ w

    # Build constraints matrices for linprog:
    # We'll minimize sum of costs (or zeros) subject to constraints

    # Variables: costs vector c of length n_actions
    c_obj = np.zeros(n_actions)

    # Inequality constraints A_ub x <= b_ub

    # IR constraints: c_a <= p_a @ w  =>  c_a - p_a @ w <= 0
    # For each accepted contract assigned to action a
    A_ir = []
    b_ir = []

    # Map accepted contracts to their assigned action index in 0..n_actions-1
    assigned_action_idx = np.zeros(len(accepted_logs), dtype=int)
    label_to_idx = {ul: i for i, ul in enumerate(unique_labels)}
    for i, label in enumerate(labels):
        assigned_action_idx[i] = label_to_idx[label]

    for i, log in enumerate(accepted_logs):
        a = assigned_action_idx[i]
        w = np.array(log['Contract'])
        val = ps[a] @ w
        row = np.zeros(n_actions)
        row[a] = 1.0
        A_ir.append(row)
        b_ir.append(val)

    # IC constraints: For each rejected contract w, for all a:
    # c_a > p_a @ w  =>  -c_a < -p_a @ w  =>  -c_a + 0 <= -p_a @ w - epsilon
    # We use strict inequality, approximate by small epsilon=1e-8
    epsilon = 1e-8
    A_ic = []
    b_ic = []

    if rejected_logs:
        rejected_contracts = np.array([log['Contract'] for log in rejected_logs])
        for w in rejected_contracts:
            for a in range(n_actions):
                val = ps[a] @ w
                row = np.zeros(n_actions)
                row[a] = -1.0
                A_ic.append(row)
                b_ic.append(-val - epsilon)

    # Combine constraints
    A_ub = np.vstack([A_ir, A_ic]) if (A_ir or A_ic) else None
    b_ub = np.array(b_ir + b_ic) if (b_ir or b_ic) else None

    # Bounds: costs >= 0
    bounds = [(0, None) for _ in range(n_actions)]

    # Solve LP: minimize sum costs subject to constraints
    # If no constraints, costs = 0
    if A_ub is not None and len(A_ub) > 0:
        lp_res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')
        if lp_res.success:
            costs = lp_res.x
        else:
            # fallback: set costs to max IR upper bounds or zero
            costs = np.zeros(n_actions)
            for i in range(n_actions):
                # max p_i @ w over accepted contracts assigned to i
                idxs = np.where(assigned_action_idx == i)[0]
                if len(idxs) > 0:
                    vals = [ps[i] @ np.array(accepted_logs[j]['Contract']) for j in idxs]
                    costs[i] = max(vals)
                else:
                    costs[i] = 0.0
            costs = np.maximum(costs, 0.0)
    else:
        costs = np.zeros(n_actions)

    # Final agent setting matrix: each row [p1, ..., p5, cost]
    agent_setting = np.hstack([ps, costs.reshape(-1, 1)])

    return agent_setting
```
