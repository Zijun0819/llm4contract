```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from scipy.optimize import linprog

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix from historical logs with contracts, principal utilities,
    and agent actions. This version adaptively chooses the number of actions via clustering,
    enforces linear feasibility for agent IR/IC constraints, robustly handles rejections,
    and carefully estimates costs with margin to ensure consistency.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes (shape (5,))
        content (list[dict]): Historical logs, each dict with keys:
            - 'Contract': 5-dim payment vector
            - 'Principal Utility': scalar utility (0 if rejected)
            - 'Agent Action': 1 (accept) or -1 (reject)

    Returns:
        np.ndarray: n_actions x 6 matrix, each row: [p(outcomes), cost]
            - p(outcomes): 5-dim probability vector summing to 1
            - cost: non-negative scalar cost of the action
    """
    # Parse logs into arrays
    contracts = np.array([log['Contract'] for log in content])  # (L,5)
    principal_utils = np.array([log['Principal Utility'] for log in content])  # (L,)
    agent_actions = np.array([log['Agent Action'] for log in content])  # (L,)
    L, m = contracts.shape

    accepted_idx = np.where(agent_actions == 1)[0]
    rejected_idx = np.where(agent_actions == -1)[0]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if len(accepted_idx) == 0:
        uniform_p = np.ones(m) / m
        return np.hstack([uniform_p.reshape(1, -1), np.array([[0.0]])])

    # For each accepted contract i, solve LP:
    # find p >=0, sum(p)=1, p@(v - w_i) = principal_util_i
    # to get candidate outcome distributions p consistent with principal utility
    candidate_ps = []
    for i in accepted_idx:
        w = contracts[i]
        u = principal_utils[i]
        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1.0, u])
        bounds = [(0, 1)] * m

        res = linprog(c=np.zeros(m), A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if res.success:
            p = res.x
            # Numerical cleanup: clip small negatives to zero
            p = np.clip(p, 0, None)
            s = p.sum()
            if s > 0:
                p /= s
            else:
                p = np.ones(m) / m
            candidate_ps.append(p)
        else:
            # If no feasible p, fallback to uniform distribution
            candidate_ps.append(np.ones(m) / m)

    candidate_ps = np.array(candidate_ps)

    # Cluster candidate_ps using DBSCAN to find natural groups and handle noise
    # eps and min_samples tuned for typical scale; can be adjusted if needed
    clustering = DBSCAN(eps=0.15, min_samples=3).fit(candidate_ps)
    labels = clustering.labels_
    unique_labels = set(labels)
    noise_label = -1
    if noise_label in unique_labels:
        unique_labels.remove(noise_label)

    # If no clusters found (all noise), treat all as one cluster
    if len(unique_labels) == 0:
        unique_labels = {0}
        labels = np.zeros(len(candidate_ps), dtype=int)

    # Compute cluster centers as mean of points in cluster
    cluster_centers = []
    for lbl in sorted(unique_labels):
        pts = candidate_ps[labels == lbl]
        center = pts.mean(axis=0)
        # Clip negatives and normalize strictly
        center = np.clip(center, 0, None)
        s = center.sum()
        if s > 0:
            center /= s
        else:
            center = np.ones(m) / m
        cluster_centers.append(center)
    cluster_centers = np.array(cluster_centers)
    n_actions = cluster_centers.shape[0]

    # Assign accepted logs to clusters (noise points get assigned to nearest cluster)
    accepted_labels = np.full(len(accepted_idx), -1, dtype=int)
    for i, p in enumerate(candidate_ps):
        lbl = labels[i]
        if lbl == noise_label:
            # Assign noise point to nearest cluster center by L1 distance
            dists = np.linalg.norm(cluster_centers - p, ord=1, axis=1)
            assigned = np.argmin(dists)
            accepted_labels[i] = assigned
        else:
            accepted_labels[i] = sorted(unique_labels).index(lbl)

    # Infer costs per action from accepted logs:
    # For each action a, cost_a <= min_{accepted contracts assigned to a} p_a @ w
    # To satisfy IR: p_a @ w - cost_a >= 0 for accepted contracts
    costs = np.zeros(n_actions)
    for a in range(n_actions):
        idxs = np.where(accepted_labels == a)[0]
        if len(idxs) == 0:
            costs[a] = 0.0
            continue
        accepted_contracts_a = contracts[accepted_idx[idxs]]
        p_a = cluster_centers[a]
        expected_payments = accepted_contracts_a @ p_a
        # cost_a <= min expected payment, so cost_a = min expected payment (maximal feasible)
        costs[a] = max(0.0, expected_payments.min())

    # Enforce rejection consistency:
    # For each rejected contract w_r and each action a:
    # agent utility = p_a @ w_r - cost_a < 0
    # => cost_a > p_a @ w_r for all rejected w_r
    if len(rejected_idx) > 0:
        rejected_contracts = contracts[rejected_idx]
        for a in range(n_actions):
            p_a = cluster_centers[a]
            vals = rejected_contracts @ p_a  # shape (num_rejected,)
            max_val = vals.max()
            # Increase cost if needed to strictly exceed max_val
            if costs[a] <= max_val:
                costs[a] = max_val + 1e-6  # small margin for strict inequality

    # Final normalization of cluster centers (probabilities)
    for a in range(n_actions):
        p = cluster_centers[a]
        p = np.clip(p, 0, None)
        s = p.sum()
        if s > 0:
            cluster_centers[a] = p / s
        else:
            cluster_centers[a] = np.ones(m) / m

    # Compose output matrix: [p(outcomes), cost]
    agent_setting = np.hstack([cluster_centers, costs.reshape(-1, 1)])

    return agent_setting
```
