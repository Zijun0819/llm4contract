```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import normalize

def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    L = len(content)

    # Separate accepted and rejected logs
    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if not accepted_logs:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accepted_contracts = np.array([log['Contract'] for log in accepted_logs])

    # Normalize accepted contracts for clustering: scale each contract to sum=1 (to capture outcome pattern)
    # Add small epsilon to avoid division by zero
    eps = 1e-12
    accepted_contracts_sum = accepted_contracts.sum(axis=1, keepdims=True)
    accepted_contracts_norm = accepted_contracts / np.maximum(accepted_contracts_sum, eps)

    # Adaptive DBSCAN clustering with elbow method on eps parameter to find suitable clusters
    # Use cosine distance on normalized contracts to capture pattern similarity
    # We'll try eps values from 0.01 to 0.3 in 30 steps and pick the one with elbow in cluster count
    # If no cluster found, fallback to one cluster
    def cluster_dbscan(data):
        # Use NearestNeighbors to estimate k-distances for elbow
        neigh = NearestNeighbors(n_neighbors=2, metric='cosine')
        neigh.fit(data)
        distances, _ = neigh.kneighbors(data)
        k_distances = np.sort(distances[:, 1])  # 2nd nearest neighbor distances sorted

        # Elbow detection via max second derivative approximation
        second_deriv = np.diff(k_distances, 2)
        if len(second_deriv) == 0:
            return None, None
        elbow_idx = np.argmax(second_deriv) + 1
        eps_candidate = k_distances[elbow_idx]

        # Run DBSCAN with eps_candidate, min_samples=3 to avoid noise clusters
        clustering = DBSCAN(eps=eps_candidate, min_samples=3, metric='cosine')
        labels = clustering.fit_predict(data)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

        # If too few clusters, try smaller eps candidates near elbow
        if n_clusters <= 0:
            # fallback to eps=0.1
            clustering = DBSCAN(eps=0.1, min_samples=3, metric='cosine')
            labels = clustering.fit_predict(data)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

        if n_clusters <= 0:
            # fallback to one cluster
            labels = np.zeros(len(data), dtype=int)
            n_clusters = 1

        return labels, n_clusters

    labels, n_clusters = cluster_dbscan(accepted_contracts_norm)
    if labels is None:
        labels = np.zeros(len(accepted_contracts), dtype=int)
        n_clusters = 1

    # For noise points (-1 labels), assign to nearest cluster center by cosine similarity
    noise_idxs = np.where(labels == -1)[0]
    if n_clusters == 0:
        # all noise, fallback to one cluster
        labels = np.zeros(len(accepted_contracts), dtype=int)
        n_clusters = 1
    else:
        # Calculate cluster centers in normalized contract space
        cluster_centers = np.zeros((n_clusters, m))
        for i in range(n_clusters):
            cluster_points = accepted_contracts_norm[labels == i]
            if cluster_points.shape[0] == 0:
                # no points, assign uniform center
                cluster_centers[i] = np.ones(m) / m
            else:
                cluster_centers[i] = cluster_points.mean(axis=0)
        cluster_centers = normalize(cluster_centers, norm='l1', axis=1)

        if len(noise_idxs) > 0:
            # Assign noise points to nearest center by cosine similarity
            noise_points = accepted_contracts_norm[noise_idxs]
            sim = noise_points @ cluster_centers.T  # cosine similarity since normalized l1 approx
            assigned = sim.argmax(axis=1)
            labels[noise_idxs] = assigned

    # Recompute cluster centers with all points assigned
    cluster_centers = np.zeros((n_clusters, m))
    for i in range(n_clusters):
        cluster_points = accepted_contracts_norm[labels == i]
        if cluster_points.shape[0] == 0:
            cluster_centers[i] = np.ones(m) / m
        else:
            cluster_centers[i] = cluster_points.mean(axis=0)
    # Normalize cluster centers to sum=1 probabilities strictly
    cluster_centers = np.maximum(cluster_centers, 0)
    cluster_centers = normalize(cluster_centers, norm='l1', axis=1)

    ps = cluster_centers  # shape (actions, m)

    # Assign accepted contracts to nearest cluster center by Euclidean distance on raw contracts
    dist_to_centers = np.linalg.norm(accepted_contracts[:, None, :] - ps[None, :, :], axis=2)
    assigned_actions = dist_to_centers.argmin(axis=1)

    # Compute initial costs per action from IR constraints (accepted contracts):
    # For each action a, cost_a <= min_{w assigned to a} p_a @ w
    costs = np.zeros(n_clusters)
    for a in range(n_clusters):
        idxs = np.where(assigned_actions == a)[0]
        if idxs.size > 0:
            pay_ins = np.array([ps[a] @ accepted_contracts[i] for i in idxs])
            costs[a] = pay_ins.min()
        else:
            costs[a] = 0.0

    # Enforce IC constraints from rejected contracts:
    # For each rejected contract w, agent utility = max_a (p_a @ w - cost_a) < 0
    # => cost_a > p_a @ w for all a, for that w
    if rejected_logs:
        rejected_contracts = np.array([log['Contract'] for log in rejected_logs])
        rej_utilities = ps @ rejected_contracts.T  # shape (actions, rejected_count)
        # For each rejected contract w, all costs must be > p_a @ w
        # So for each action a, cost_a > max rejected p_a @ w
        min_costs_from_rej = rej_utilities.max(axis=1) + 1e-8  # small epsilon for strictness
        costs = np.maximum(costs, min_costs_from_rej)

    # Enforce IR constraints globally: cost_a <= min_{accepted w} p_a @ w
    # Already done above, but if IC constraints raised costs, may violate IR
    # So we iteratively adjust costs to satisfy IR and IC:
    # If cost_a > min_{accepted w} p_a @ w, lower cost_a to min accepted or increase cost_a to max rejected?
    # We prioritize IC constraints because rejection must hold strictly.
    # If conflict, increase cost_a to max rejected p_a @ w

    # Compute min accepted pay-ins per action again
    min_accepted_payins = np.zeros(n_clusters)
    for a in range(n_clusters):
        idxs = np.where(assigned_actions == a)[0]
        if idxs.size > 0:
            pay_ins = np.array([ps[a] @ accepted_contracts[i] for i in idxs])
            min_accepted_payins[a] = pay_ins.min()
        else:
            min_accepted_payins[a] = 0.0

    # If any cost_a > min_accepted_payins[a], IR violated, so fix by:
    # lowering cost_a to min_accepted_payins[a] if possible (but IC constraints may forbid)
    # or if cost_a < min_accepted_payins[a], IR holds
    # So set cost_a = max(cost_a, min_accepted_payins[a]) to ensure IR holds
    costs = np.maximum(costs, min_accepted_payins)

    # Final check: costs non-negative
    costs = np.maximum(costs, 0.0)

    # Final agent setting matrix: each row [p1, ..., p5, cost]
    agent_setting = np.hstack([ps, costs.reshape(-1, 1)])

    return agent_setting
```
