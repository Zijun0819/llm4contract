```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import normalize

def agent_solver(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    L = len(content)

    accepted_logs = [log for log in content if log['Agent Action'] == 1]
    rejected_logs = [log for log in content if log['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if not accepted_logs:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accepted_contracts = np.array([log['Contract'] for log in accepted_logs])

    # Normalize contracts for clustering (scale invariance)
    contract_norms = np.linalg.norm(accepted_contracts, axis=1, keepdims=True)
    contract_norms[contract_norms == 0] = 1.0
    accepted_normed = accepted_contracts / contract_norms

    # Adaptive DBSCAN clustering with elbow method on eps parameter
    # to find clusters of normalized contracts by outcome pattern similarity

    def dbscan_cluster(eps):
        clustering = DBSCAN(eps=eps, min_samples=2, metric='euclidean')
        labels = clustering.fit_predict(accepted_normed)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        return labels, n_clusters

    # Search eps values logarithmically to find elbow with reasonable number of clusters
    eps_values = np.linspace(0.05, 0.5, 15)
    best_eps = None
    best_labels = None
    best_n_clusters = 0
    prev_n_clusters = None
    for eps in eps_values:
        labels, n_clusters = dbscan_cluster(eps)
        if n_clusters == 0:
            continue
        if prev_n_clusters is not None and n_clusters > prev_n_clusters:
            # Elbow found: first significant increase in clusters
            best_eps = eps
            best_labels = labels
            best_n_clusters = n_clusters
            break
        prev_n_clusters = n_clusters
    # If no elbow found, pick last eps with clusters
    if best_labels is None:
        for eps in reversed(eps_values):
            labels, n_clusters = dbscan_cluster(eps)
            if n_clusters > 0:
                best_eps = eps
                best_labels = labels
                best_n_clusters = n_clusters
                break
    # If still no clusters (very rare), fallback to one cluster
    if best_labels is None:
        best_labels = np.zeros(len(accepted_contracts), dtype=int)
        best_n_clusters = 1

    # For noise points (-1 labels), assign to nearest cluster by normalized contract distance
    noise_idx = np.where(best_labels == -1)[0]
    if noise_idx.size > 0 and best_n_clusters > 0:
        clustered_idx = np.where(best_labels != -1)[0]
        clustered_points = accepted_normed[clustered_idx]
        nbrs = NearestNeighbors(n_neighbors=1).fit(clustered_points)
        noise_points = accepted_normed[noise_idx]
        distances, indices = nbrs.kneighbors(noise_points)
        best_labels[noise_idx] = best_labels[clustered_idx[indices.flatten()]]

    # Compute cluster centers as mean of original contracts (not normalized)
    ps = np.zeros((best_n_clusters, m))
    for i in range(best_n_clusters):
        cluster_points = accepted_contracts[best_labels == i]
        if len(cluster_points) == 0:
            # If cluster empty (should not happen), assign uniform
            ps[i] = np.ones(m) / m
        else:
            center = cluster_points.mean(axis=0)
            center = np.maximum(center, 0)
            s = center.sum()
            if s > 1e-12:
                center /= s
            else:
                center = np.ones(m) / m
            ps[i] = center

    # Assign accepted contracts to closest cluster center by Euclidean distance (original scale)
    dist_to_centers = np.linalg.norm(accepted_contracts[:, None, :] - (ps * accepted_contracts.sum(axis=1)[:, None, None]).mean(axis=1), axis=2)
    # Above line is incorrect: fix to compute distances properly:
    # Actually, ps are normalized probabilities, accepted_contracts are payments.
    # We should compare contracts normalized to sum=1 to ps to assign actions.
    accepted_contracts_norm = accepted_contracts / accepted_contracts.sum(axis=1, keepdims=True)
    dist_to_centers = np.linalg.norm(accepted_contracts_norm[:, None, :] - ps[None, :, :], axis=2)
    assigned_actions = dist_to_centers.argmin(axis=1)

    # Compute initial costs from IR constraints on accepted contracts
    costs = np.zeros(best_n_clusters)
    for a in range(best_n_clusters):
        idxs = np.where(assigned_actions == a)[0]
        if idxs.size > 0:
            pay_ins = []
            for i in idxs:
                # agent utility >= 0 => p_a @ w - cost_a >=0 => cost_a <= p_a @ w
                # Use original contract payments (w), and p_a probabilities
                # p_a @ w = sum_j p_a[j]*w[j]
                pay_in = ps[a] @ accepted_contracts[i]
                pay_ins.append(pay_in)
            costs[a] = min(pay_ins)
        else:
            costs[a] = 0.0

    # Enforce IC constraints from rejected contracts:
    # For each rejected contract w, agent utility = max_a (p_a @ w - cost_a) < 0
    # => cost_a > p_a @ w for all a, for that w
    if rejected_logs:
        rejected_contracts = np.array([log['Contract'] for log in rejected_logs])
        # Normalize rejected contracts to sum=1 for consistent dot product with ps
        rejected_contracts_norm = rejected_contracts / rejected_contracts.sum(axis=1, keepdims=True)
        rej_utilities = ps @ rejected_contracts_norm.T  # shape (actions, rejected_count)
        # For each action a, cost_a > max rejected p_a @ w
        min_costs_from_rej = rej_utilities.max(axis=1) + 1e-8  # epsilon for strict inequality
        costs = np.maximum(costs, min_costs_from_rej)

    # Ensure costs non-negative
    costs = np.maximum(costs, 0.0)

    # Final agent setting matrix: each row [p1, ..., p5, cost]
    agent_setting = np.hstack([ps, costs.reshape(-1, 1)])

    return agent_setting
```
