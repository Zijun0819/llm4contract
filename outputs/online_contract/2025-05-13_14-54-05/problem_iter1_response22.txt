```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import KMeans


def agent_solver_v2(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting (actions with outcome distributions and costs)
    consistent with historical contract interaction logs.

    Args:
        v (np.ndarray): Principal's reward vector for 5 outcomes (length 5).
        content (list[dict]): Historical logs, each dict contains:
            - 'Contract': 5-dim payment vector
            - 'Principal Utility': scalar utility (0 if rejected)
            - 'Agent Action': 1 (accept) or -1 (reject)

    Returns:
        np.ndarray: n x 6 matrix, each row an agent action:
            - first 5 cols: outcome distribution (sum to 1)
            - last col: non-negative cost
    """

    # Constants
    n_outcomes = v.shape[0]
    logs_df = pd.DataFrame(content)
    n_logs = len(logs_df)

    # Separate accepted and rejected logs
    accepted_logs = logs_df[logs_df['Agent Action'] == 1]
    rejected_logs = logs_df[logs_df['Agent Action'] == -1]

    # 1) Extract wages and principal utilities from accepted logs
    accepted_contracts = np.vstack(accepted_logs['Contract'].values)  # shape (n_accept, 5)
    principal_utils = accepted_logs['Principal Utility'].values       # shape (n_accept,)

    # 2) For each accepted log, infer a candidate outcome distribution p:
    #    We solve LP: p in simplex, s.t. p @ w = principal_utility + cost (unknown)
    #    Since cost unknown, approximate p by maximizing entropy under p @ w â‰ˆ principal_utility
    #    Instead, here approximate p by solving:
    #      maximize sum log p_i  s.t. p @ w = u (fixed), sum p_i=1, p_i >=0
    #    (But since LP can't do entropy easily, approximate p by projection)
    # We'll use a heuristic: For each accepted contract, solve min ||p - uniform|| s.t. p @ w = u, p in simplex
    # This encourages p near uniform but consistent with observed utility.

    def infer_p_for_w_u(w: np.ndarray, u: float) -> np.ndarray:
        # Solve QP: min ||p - uniform||^2 s.t. p @ w = u, sum p=1, p>=0
        # Implement via linprog as two-step:
        # First find feasible p with p @ w = u, sum p=1, p>=0
        # Then pick p closest to uniform in L2 norm: min sum (p_i - 1/n)^2
        # Since this is QP, approximate by solving LP to find feasible p:
        # min 0 s.t. constraints (find feasible p)
        A_eq = np.vstack([np.ones(n_outcomes), w])
        b_eq = np.array([1.0, u])
        bounds = [(0, 1) for _ in range(n_outcomes)]
        res = linprog(c=np.zeros(n_outcomes), A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if not res.success:
            # If fail, fallback to uniform
            return np.ones(n_outcomes) / n_outcomes
        return res.x

    candidate_ps = np.array([infer_p_for_w_u(w, u) for w, u in zip(accepted_contracts, principal_utils)])

    # 3) Cluster these candidate_ps to identify distinct agent actions
    # Adaptive number of clusters: try 3 to 10, pick best silhouette
    from sklearn.metrics import silhouette_score

    best_n = 3
    best_score = -1
    best_centers = None
    for k in range(3, min(11, len(candidate_ps) + 1)):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42).fit(candidate_ps)
        if k == 1:
            score = -1  # silhouette undefined for k=1
        else:
            score = silhouette_score(candidate_ps, kmeans.labels_)
        if score > best_score:
            best_score = score
            best_n = k
            best_centers = kmeans.cluster_centers_

    action_ps = best_centers  # shape (best_n, 5)
    n_actions = best_n

    # 4) For each accepted log, assign action by max expected agent utility (to estimate cost)
    # Agent utility for action a on contract w: p_a @ w - c_a >= 0 if accepted
    # We don't know c_a yet, but can estimate c_a as minimal p_a @ w over assigned accepted contracts

    # Assign each accepted log to closest action p in L1 distance
    accepted_assigns = []
    for p_candidate in candidate_ps:
        dists = np.linalg.norm(action_ps - p_candidate, ord=1, axis=1)
        accepted_assigns.append(np.argmin(dists))
    accepted_assigns = np.array(accepted_assigns)

    # 5) Estimate costs from accepted logs by IR constraints:
    #    c_a <= p_a @ w for all accepted contracts assigned to a
    costs_lower_bounds = np.zeros(n_actions)
    for a in range(n_actions):
        idxs = np.where(accepted_assigns == a)[0]
        if len(idxs) == 0:
            costs_lower_bounds[a] = 0.0
            continue
        relevant_ws = accepted_contracts[idxs]  # shape (num_logs_a, 5)
        c_bound = np.min(relevant_ws @ action_ps[a])  # minimal expected wage
        costs_lower_bounds[a] = max(0.0, c_bound)  # cost non-negative

    # 6) Ensure rejection consistency: for no accepted contract, agent utility < 0
    # For each rejected log, p_a @ w - c_a < 0 for all a
    # => c_a > p_a @ w for all rejected w
    if len(rejected_logs) > 0:
        rejected_contracts = np.vstack(rejected_logs['Contract'].values)  # shape (n_reject, 5)
        # For each action a, find max p_a @ w over rejected contracts
        rejected_expected = rejected_contracts @ action_ps.T  # shape (n_reject, n_actions)
        max_rejected_expected = np.max(rejected_expected, axis=0)  # shape (n_actions,)
        # costs must be strictly > max_rejected_expected to reject all rejected contracts
        # increase costs_lower_bounds if necessary
        costs_lower_bounds = np.maximum(costs_lower_bounds, max_rejected_expected + 1e-6)
    else:
        # No rejected logs: costs_lower_bounds remain as estimated from accepted
        pass

    # 7) Final agent costs are costs_lower_bounds, ensure non-negative
    costs = costs_lower_bounds

    # 8) Normalize each p to sum to 1 (just in case of numerical issues)
    action_ps = np.clip(action_ps, 0, None)
    action_ps = action_ps / action_ps.sum(axis=1, keepdims=True)

    # 9) Construct output matrix: [p | cost]
    agent_setting = np.hstack([action_ps, costs[:, np.newaxis]])

    return agent_setting
```
