```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog, minimize
from sklearn.decomposition import NMF
from sklearn.cluster import AgglomerativeClustering
from typing import List, Dict

def agent_solver(v: np.ndarray, content: List[Dict]) -> np.ndarray:
    """
    Infers a valid agent setting (action distributions and costs) consistent with historical logs.
    Args:
        v: Principal's reward vector for 5 outcomes (shape: (5,))
        content: List of logs, each log is a dict with keys:
            'Contract': list of 5 payments,
            'Principal Utility': float (0 if rejected),
            'Agent Action': 1 (accept) or -1 (reject)

    Returns:
        np.ndarray of shape (n_actions, 6):
          first 5 cols: outcome probabilities per action (sum to 1)
          last col: action cost (non-negative)
    """
    m = len(v)  # number of outcomes (5)
    logs_df = pd.DataFrame(content)
    accepted_df = logs_df[logs_df['Agent Action'] == 1].reset_index(drop=True)
    rejected_df = logs_df[logs_df['Agent Action'] == -1].reset_index(drop=True)

    # Extract contracts and principal utilities for accepted logs
    W_accept = np.vstack(accepted_df['Contract'].values)  # shape (L_accept, m)
    U_accept = accepted_df['Principal Utility'].values  # shape (L_accept,)

    # Heuristic: choose n_actions adaptively between 3 and 8 based on number of accepted logs
    n_actions = min(max(3, len(accepted_df)//15), 8)

    # 1. Decompose accepted contracts as convex combination of latent action outcome distributions:
    # Using Non-negative Matrix Factorization (NMF) on W_accept.T (shape m x L_accept)
    # W_accept.T ~ P (m x n_actions) * H (n_actions x L_accept), where columns of P sum to 1

    # Normalize columns of W_accept so that payments are comparable (avoid scale issues)
    pay_scale = W_accept.max(axis=1, keepdims=True)
    pay_scale[pay_scale == 0] = 1
    W_norm = W_accept / pay_scale

    # Use NMF to find latent basis P (actions) and weights H (log contributions)
    model = NMF(n_components=n_actions, init='random', random_state=42, max_iter=1000)
    H = model.fit_transform(W_norm.T)  # shape: (L_accept, n_actions)
    P_raw = model.components_  # shape: (n_actions, m)

    # Transpose P_raw so rows correspond to actions
    P_raw = P_raw  # n_actions x m

    # Normalize each row of P_raw to sum to 1 (agent outcome distributions are probabilities)
    P = np.maximum(P_raw, 1e-8)
    P = P / P.sum(axis=1, keepdims=True)

    # 2. Estimate action costs c >= 0 for each action
    # For accepted logs: agent expected utility >= 0 => p_a @ w_i - c_a >= 0 for at least one action a explaining log i
    # We use the assignments from H: assign each accepted log to action with max weight in H
    assigns = np.argmax(H, axis=1)

    # Setup variables:
    # c = (c_1, ..., c_n_actions), costs to find minimizing sum(c) (to avoid trivial zero)
    # Constraints:
    # For each accepted log i: p_{assigns[i]} @ w_i - c_{assigns[i]} >= 0  (IR)
    # For each rejected log j: for all actions a: p_a @ w_j - c_a < 0 (reject consistency)
    # We formulate a LP or QP to find minimal c >=0 consistent with above constraints

    # Prepare matrix forms:
    # IR constraints (accepted logs)
    A_ir = np.zeros((len(accepted_df), n_actions))
    b_ir = np.zeros(len(accepted_df))
    W_accept = np.array(W_accept)

    for i, a in enumerate(assigns):
        A_ir[i, a] = -1  # move c_a to LHS
        b_ir[i] = - P[a] @ W_accept[i]  # -p_a @ w_i

    # Rejection constraints (rejected logs)
    W_reject = np.vstack(rejected_df['Contract'].values) if len(rejected_df) > 0 else np.empty((0, m))

    # For each rejected log j and each action a: p_a @ w_j - c_a < 0
    # => c_a > p_a @ w_j (strict)
    # We relax strictness by epsilon margin
    eps = 1e-6
    A_rej = np.zeros((len(W_reject)*n_actions, n_actions))
    b_rej = np.zeros(len(W_reject)*n_actions)

    for j in range(len(W_reject)):
        for a in range(n_actions):
            row = j*n_actions + a
            A_rej[row, a] = 1  # c_a
            b_rej[row] = P[a] @ W_reject[j] - eps

    # Combine constraints:
    # A_ir c <= b_ir  (note signs flipped, so rewrite inequalities)
    # IR: p_a @ w_i - c_a >= 0 => - c_a >= - p_a @ w_i => c_a <= p_a @ w_i
    # So A_ir c <= b_ir with A_ir[i,a] = 1 for assigned action, b_ir[i] = p_a @ w_i
    # We fix that:
    A_ir_fixed = np.zeros_like(A_ir)
    b_ir_fixed = np.zeros_like(b_ir)
    for i, a in enumerate(assigns):
        A_ir_fixed[i, a] = 1
        b_ir_fixed[i] = P[a] @ W_accept[i]

    # Final combined inequalities:
    # A_ub c <= b_ub where:
    A_ub = np.vstack([A_ir_fixed, -A_rej])
    b_ub = np.hstack([b_ir_fixed, -b_rej])

    # Bounds for c: c >= 0
    bounds = [(0, None) for _ in range(n_actions)]

    # Objective: minimize sum of costs to get smallest valid costs
    c_init = np.ones(n_actions)

    # Use scipy.optimize.minimize with constraints
    def obj(c):
        return np.sum(c)

    cons = [{'type': 'ineq', 'fun': lambda c, A=A_ub[i], b=b_ub[i]: b - np.dot(A, c)} for i in range(A_ub.shape[0])]

    res = minimize(obj, c_init, bounds=bounds, constraints=cons, method='SLSQP', options={'ftol':1e-9,'maxiter':1000})

    if not res.success:
        # fallback: pick max IR cost ignoring rejection constraints
        c_candidates = np.array([P[a] @ W_accept[assigns == a].min(axis=0) if np.any(assigns == a) else 0.0 for a in range(n_actions)])
        c_candidates = np.maximum(c_candidates, 0)
        c = c_candidates
    else:
        c = np.maximum(res.x, 0)

    # 3. Validate that for all rejected logs, no action yields nonnegative expected utility:
    if len(W_reject) > 0:
        for wj in W_reject:
            utils = P @ wj - c
            if np.any(utils >= -1e-8):
                # If violated, increase costs slightly for violating actions
                c += 1e-4

    # 4. Return agent setting: shape (n_actions, 6): [p_5cols, cost]
    agent_setting = np.hstack([P, c.reshape(-1, 1)])
    return agent_setting
```
