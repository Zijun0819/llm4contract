```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from scipy.optimize import linprog

def agent_solver(v: np.ndarray, content: list[dict]) -> np.ndarray:
    """
    Infer a valid agent setting matrix (actions x [probabilities over 5 outcomes + cost])
    that explains all historical interactions under IR and IC constraints.

    Args:
        v: np.ndarray of shape (5,), principal's reward vector for 5 outcomes.
        content: list of dicts, each with keys 'Contract' (list of 5 float payments),
                 'Principal Utility' (float), and 'Agent Action' (1 or -1).

    Returns:
        np.ndarray of shape (n_actions, 6), rows are [p1,...,p5,cost]
    """
    m = v.shape[0]  # number of outcomes (expected 5)
    logs_df = pd.DataFrame(content)

    # Separate accepted and rejected logs
    accept_df = logs_df[logs_df['Agent Action'] == 1]
    reject_df = logs_df[logs_df['Agent Action'] == -1]

    # If no accepted logs, return trivial uniform distribution with zero cost
    if len(accept_df) == 0:
        p_uniform = np.ones(m) / m
        return np.array([np.append(p_uniform, 0.0)])

    accept_contracts = np.vstack(accept_df['Contract'].to_numpy())
    accept_putils = accept_df['Principal Utility'].to_numpy()
    reject_contracts = np.vstack(reject_df['Contract'].to_numpy()) if len(reject_df) > 0 else np.empty((0, m))

    # Step 1: For each accepted contract, infer a plausible outcome distribution p (prob vector)
    # Solve LP for each accepted contract:
    #   sum p = 1
    #   p @ (v - w) = principal utility
    #   0 <= p <= 1
    # This ensures p explains the observed principal utility given contract w.
    # We add a small slack to allow numerical stability by relaxing the equality on principal utility slightly.

    def solve_p_for_accept(w: np.ndarray, pu: float) -> np.ndarray:
        A_eq = np.vstack([np.ones(m), v - w])
        b_eq = np.array([1.0, pu])
        bounds = [(0, 1) for _ in range(m)]
        # Relax equality on principal utility to inequality with small tolerance to improve feasibility
        # We solve two LPs and pick feasible solution:
        # LP1: p @ (v - w) >= pu - tol
        # LP2: p @ (v - w) <= pu + tol
        tol = 1e-8

        # First LP: minimize 0 subject to A_eq[0] = 1 and p@(v-w) >= pu - tol
        A_ub1 = np.array([-(v - w)])
        b_ub1 = np.array([-(pu - tol)])
        res1 = linprog(c=np.zeros(m), A_eq=[A_eq[0]], b_eq=[b_eq[0]], A_ub=A_ub1, b_ub=b_ub1, bounds=bounds, method='highs')

        # Second LP: minimize 0 subject to A_eq[0] = 1 and p@(v-w) <= pu + tol
        A_ub2 = np.array([v - w])
        b_ub2 = np.array([pu + tol])
        res2 = linprog(c=np.zeros(m), A_eq=[A_eq[0]], b_eq=[b_eq[0]], A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')

        # Choose feasible solution closest to pu in p@(v-w)
        candidates = []
        if res1.success:
            candidates.append(res1.x)
        if res2.success:
            candidates.append(res2.x)
        if candidates:
            # Pick candidate with p@(v-w) closest to pu
            best_p = min(candidates, key=lambda p: abs(p @ (v - w) - pu))
            p = best_p
        else:
            # fallback uniform distribution if infeasible
            p = np.ones(m) / m

        p = np.clip(p, 0, None)
        s = p.sum()
        if s > 0:
            p /= s
        else:
            p = np.ones(m) / m
        return p

    candidate_ps = np.array([solve_p_for_accept(w, pu) for w, pu in zip(accept_contracts, accept_putils)])

    # Step 2: Cluster candidate_ps to find representative actions (outcome distributions)
    # Use DBSCAN with cosine distance to adaptively find clusters with tight radius threshold.
    # Weight points by acceptance principal utility magnitude (importance).
    # If no clusters found, fallback to one cluster (all points).

    from sklearn.metrics.pairwise import cosine_distances

    # Weight vector for clustering: principal utility magnitude normalized
    weights = np.abs(accept_putils)
    if weights.sum() == 0:
        weights = np.ones_like(weights)
    weights /= weights.sum()

    # DBSCAN with cosine metric and small eps to find tight clusters
    # eps chosen adaptively: median of pairwise cosine distances * 0.3, min_samples=1 to allow small clusters
    if len(candidate_ps) == 1:
        labels = np.array([0])
    else:
        dist_mat = cosine_distances(candidate_ps)
        median_dist = np.median(dist_mat[np.triu_indices(len(candidate_ps), k=1)])
        eps = max(1e-3, median_dist * 0.3)
        dbscan = DBSCAN(eps=eps, min_samples=1, metric='cosine')
        labels = dbscan.fit_predict(candidate_ps)

    unique_labels = np.unique(labels)
    n_actions = len(unique_labels)

    # Compute weighted cluster centers
    p_centers = np.zeros((n_actions, m))
    for i, lab in enumerate(unique_labels):
        idxs = np.where(labels == lab)[0]
        if len(idxs) == 0:
            continue
        wts = weights[idxs]
        wts_sum = wts.sum()
        if wts_sum == 0:
            center = candidate_ps[idxs].mean(axis=0)
        else:
            center = np.average(candidate_ps[idxs], axis=0, weights=wts)
        center = np.clip(center, 0, None)
        s = center.sum()
        if s > 0:
            center /= s
        else:
            center = np.ones(m) / m
        p_centers[i] = center

    # Step 3: Initialize costs from IR constraints (agent rationality on accepted contracts)
    # For each action a:
    #   cost_a <= min_i (p_a @ w_i) for accepted contracts i assigned to cluster a
    #   cost_a >= 0
    costs = np.zeros(n_actions)
    for i, lab in enumerate(unique_labels):
        idxs = np.where(labels == lab)[0]
        if len(idxs) == 0:
            costs[i] = 0.0
            continue
        p_a = p_centers[i]
        payoffs = accept_contracts[idxs] @ p_a  # shape (#assigned,)
        min_payoff = np.min(payoffs)
        costs[i] = max(0.0, min_payoff)

    # Step 4: Enforce IC constraints from rejected contracts:
    # For each rejected contract w_r and action a:
    #   agent utility < 0 => p_a @ w_r - cost_a < 0 => cost_a > p_a @ w_r
    # So cost_a > max_r p_a @ w_r for all rejected contracts r
    # Add margin proportional to scale for strict inequality

    if reject_contracts.shape[0] > 0:
        rej_utils = reject_contracts @ p_centers.T  # shape (#rej, n_actions)
        max_rej_util = np.max(rej_utils, axis=0)  # max over rejected contracts per action
        margin = np.maximum(1e-7, 1e-4 * np.maximum(1.0, max_rej_util))
        costs = np.maximum(costs, max_rej_util + margin)

    # Step 5: Iterative global refinement of costs to satisfy IR and IC constraints with margin

    # Define margin for IR and IC
    margin_ir = 1e-7
    margin_ic = 1e-7

    # Prepare data for refinement
    # For each action a and accepted contract i assigned to a:
    #   p_a @ w_i - cost_a >= margin_ir
    # For each action a and rejected contract r:
    #   cost_a - p_a @ w_r >= margin_ic

    # Build constraints matrix for costs vector

    # Map accepted contracts to cluster indices
    accept_clusters = labels

    # Construct constraints:
    # For IR: cost_a <= p_a @ w_i - margin_ir  => cost_a - p_a @ w_i <= -margin_ir
    # For IC: cost_a >= p_a @ w_r + margin_ic  => -cost_a + p_a @ w_r <= -margin_ic

    # We solve a LP to find costs satisfying all inequalities

    # Number of constraints:
    n_ir = len(accept_contracts)
    n_ic = reject_contracts.shape[0] * n_actions if reject_contracts.shape[0] > 0 else 0
    n_constraints = n_ir + n_ic

    if n_constraints > 0:
        A_ub = np.zeros((n_constraints, n_actions))
        b_ub = np.zeros(n_constraints)

        # IR constraints
        for idx in range(n_ir):
            a = accept_clusters[idx]
            p_a = p_centers[a]
            w_i = accept_contracts[idx]
            val = p_a @ w_i
            # cost_a - val <= -margin_ir
            A_ub[idx, a] = 1.0
            b_ub[idx] = val - margin_ir

        # IC constraints
        if n_ic > 0:
            row = n_ir
            for r_idx in range(reject_contracts.shape[0]):
                w_r = reject_contracts[r_idx]
                for a in range(n_actions):
                    p_a = p_centers[a]
                    val = p_a @ w_r
                    # -cost_a + val <= -margin_ic  => cost_a >= val + margin_ic
                    A_ub[row, a] = -1.0
                    b_ub[row] = -val - margin_ic
                    row += 1

        # Bounds for costs: cost_a >= 0
        bounds = [(0, None) for _ in range(n_actions)]

        # Objective: minimize sum of costs (arbitrary, just to find feasible)
        c = np.ones(n_actions)

        res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

        if res.success:
            costs = res.x
        else:
            # If infeasible, relax margins and fallback to previous costs
            # Try relaxing margins to zero and re-solve
            margin_ir = 0.0
            margin_ic = 0.0
            for idx in range(n_ir):
                a = accept_clusters[idx]
                p_a = p_centers[a]
                w_i = accept_contracts[idx]
                val = p_a @ w_i
                A_ub[idx, a] = 1.0
                b_ub[idx] = val - margin_ir
            if n_ic > 0:
                row = n_ir
                for r_idx in range(reject_contracts.shape[0]):
                    w_r = reject_contracts[r_idx]
                    for a in range(n_actions):
                        p_a = p_centers[a]
                        val = p_a @ w_r
                        A_ub[row, a] = -1.0
                        b_ub[row] = -val - margin_ic
                        row += 1
            res2 = linprog(c=c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')
            if res2.success:
                costs = res2.x
            else:
                # fallback: keep previous costs clipped non-negative
                costs = np.maximum(costs, 0.0)

    # Numerical safety: ensure costs non-negative
    costs = np.maximum(costs, 0.0)

    # Step 6: Normalize p_centers rows to sum to 1 (numerical safety)
    p_centers = np.clip(p_centers, 0, None)
    row_sums = p_centers.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1.0
    p_centers /= row_sums

    # Step 7: Assemble agent setting matrix: [p1,...,p5, cost]
    agent_setting = np.hstack([p_centers, costs.reshape(-1, 1)])

    return agent_setting
```
