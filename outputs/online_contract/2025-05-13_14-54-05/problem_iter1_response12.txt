```python
import numpy as np
import pandas as pd
from scipy.optimize import linprog
from sklearn.cluster import AgglomerativeClustering


def agent_solver_v2(v: np.ndarray, content: pd.DataFrame) -> np.ndarray:
    """
    Infer a valid agent setting matrix from historical logs.
    Returns an n x 6 matrix where each row: [p_1,...,p_5,cost].
    """
    m = v.shape[0]
    L = len(content)

    # Extract contracts, utilities, actions arrays
    contracts = np.vstack(content['Contract'].values)  # L x m
    principal_utils = content['Principal Utility'].values  # L
    actions = content['Agent Action'].values  # L

    # Separate accepted and rejected logs
    accepted_idx = np.where(actions == 1)[0]
    rejected_idx = np.where(actions == -1)[0]

    accepted_wages = contracts[accepted_idx]  # A x m
    accepted_utils = principal_utils[accepted_idx]  # A
    rejected_wages = contracts[rejected_idx] if rejected_idx.size > 0 else np.zeros((0, m))

    # 1) Infer candidate outcome distributions p for accepted contracts by solving LP:
    # For each accepted contract i, solve for p_i:
    #   max sum_j p_j * w_j is not necessarily needed, but we want p_i s.t
    #   sum_j p_j = 1, p_j ≥ 0, and
    #   sum_j p_j * w_j = principal utility u_i + agent cost c_i (unknown).
    # Without direct agent cost, solve with utility constraints:
    # Agent utility = p_i · w_i - c_i ≥ 0 ⇒ c_i ≤ p_i · w_i
    # We approximate p_i by maximizing entropy under these constraints.

    def solve_p_given_w_u(w: np.ndarray, u: float) -> np.ndarray:
        # Find p with max entropy s.t sum p=1 and p·w = u (agent utility)
        # Since entropy maximization is hard here, approximate by min L2 norm from uniform
        # with linear constraints:
        # min ||p - uniform||^2 s.t p·w = u, sum p=1, p≥0
        from cvxopt import matrix, solvers
        solvers.options['show_progress'] = False
        m = len(w)
        P = 2 * matrix(np.eye(m))
        q = -2 * matrix(np.ones(m) / m)
        G = matrix(-np.eye(m))
        h = matrix(np.zeros(m))
        A = matrix(np.vstack([np.ones(m), w]))
        b = matrix(np.array([1.0, u]))
        sol = solvers.qp(P, q, G, h, A, b)
        if sol['status'] != 'optimal':
            return None
        p = np.array(sol['x']).flatten()
        return p

    # For each accepted contract, get agent utility lower bound
    # agent_utility_i = max_{p} p·w_i - cost_i >= 0
    # But cost unknown, so we get p subject to p·w_i >= 0 (agent utility ≥ 0)
    # Since principal utility = p·v - cost, and cost unknown, 
    # we cannot directly get p·w = agent utility.
    # Instead approximate p by maximizing p·w_i subject to p·v = principal utility + cost unknown
    # We relax assumption and find p maximizing p·w_i s.t sum p=1, p≥0.

    # Build candidate p's by solving max p·w_i s.t sum p=1,p≥0 → p is one-hot at max w_i[j]
    # To get richer p's, cluster accepted contracts by similarity of contract vectors

    # Cluster accepted contracts into k groups
    k = min(10, len(accepted_idx)) if len(accepted_idx) > 0 else 1
    if k == 0:
        # No accepted contracts, return trivial zero-cost random p
        p0 = np.eye(m)[0:1]
        c0 = np.zeros(1)
        return np.hstack([p0, c0[:, None]])

    clustering = AgglomerativeClustering(n_clusters=k).fit(accepted_wages)
    labels = clustering.labels_

    # For each cluster compute representative p as normalized average contract weights
    p_candidates = []
    for cluster_id in range(k):
        cluster_wages = accepted_wages[labels == cluster_id]
        if cluster_wages.shape[0] == 0:
            p_candidates.append(np.ones(m) / m)
            continue
        # Use average wage vector as proxy to p·w for agent utility max
        mean_w = cluster_wages.mean(axis=0)
        # To get p: solve LP max p·mean_w s.t sum p=1,p≥0
        # Solution: put all mass on argmax mean_w[j]
        jmax = np.argmax(mean_w)
        p = np.zeros(m)
        p[jmax] = 1.0
        p_candidates.append(p)
    p_candidates = np.array(p_candidates)  # k x m

    # Estimate costs from accepted contracts:
    # For each candidate action a, cost c_a = min_i (p_a · w_i - agent utility_i)
    # Agent utility_i unknown, but agent utility_i ≥ 0 if accepted, so cost ≤ p_a · w_i
    # Hence estimate c_a = min_i (p_a · w_i) over assigned accepted contracts i in cluster a

    costs = np.zeros(k)
    for a in range(k):
        cluster_idx = accepted_idx[labels == a]
        if cluster_idx.size == 0:
            costs[a] = 0.0
        else:
            p_a = p_candidates[a]
            wages_a = contracts[cluster_idx]  # N_a x m
            vals = wages_a @ p_a  # N_a
            costs[a] = np.clip(vals.min(), 0, None)  # cost ≥ 0

    # Enforce rejection constraints:
    # For each rejected contract j, agent utility = max_a (p_a · w_j - c_a) < 0
    # So for all rejected j: max_a (p_a · w_j - c_a) < 0
    # We check if this holds; if not, increase costs accordingly

    if rejected_idx.size > 0:
        rejected_w = contracts[rejected_idx]  # R x m
        utility_matrix = rejected_w @ p_candidates.T - costs  # R x k
        max_utilities = utility_matrix.max(axis=1)  # R
        violation = max_utilities >= 0
        if violation.any():
            # Increase costs to fix violations: for each violating contract j,
            # find action a maximizing p_a · w_j - c_a, increase c_a at least p_a · w_j
            for j in np.where(violation)[0]:
                w_j = rejected_w[j]
                vals = p_candidates @ w_j
                a_star = np.argmax(vals - costs)
                needed_cost = vals[a_star] + 1e-6  # small margin
                if costs[a_star] < needed_cost:
                    costs[a_star] = needed_cost

    # Normalize p_candidates to ensure sum to 1 (should already be one-hot)
    p_candidates = np.clip(p_candidates, 0, 1)
    p_candidates /= p_candidates.sum(axis=1, keepdims=True)

    # Ensure non-negative costs
    costs = np.clip(costs, 0, None)

    # Return agent setting as n x 6 matrix: [p_1,...,p_5,cost]
    agent_setting = np.hstack([p_candidates, costs[:, None]])
    return agent_setting
```
