import pickle
import time

import numpy as np
from scipy.optimize import linprog
import sys

from os import path
import logging
from utils.utils import extract_contract_elements
from utils.utils import get_func_name
# sys.path.insert(0, path.abspath(path.join(path.dirname(__file__), '../..', '..')))
from problems.online_contract_edgeAI import gpt

possible_func_names = ["agent_solver", "agent_solver_v1", "agent_solver_v2", "agent_solver_v3"]

func_name = get_func_name(gpt, possible_func_names)
llm_agent_solver = getattr(gpt, func_name)


# This eval is used for model training

def contract_oracle_solver(p, c, v):
    # --------------------------------------------------------------------
    # assume you already have:
    #   p         : np.ndarray, shape (n_actions, m_outcomes)
    #   costs     : np.ndarray, shape (n_actions,)     # your c vector
    #   v         : np.ndarray, shape (m_outcomes,)
    #   n_actions = p.shape[0]
    # --------------------------------------------------------------------
    n_actions = p.shape[0]
    epsilon = 1e-16  # small slack to enforce strict IC/IR
    tol = 1e-12  # tolerance for post‐solve check

    best_princ_util = -np.inf
    best_contract = None
    best_action = None
    best_agent_util = None

    for a_star in range(n_actions):
        # 1) objective: minimize p[a_star]·w
        c_lp = p[a_star].copy()

        # 2) build IC constraints in one go
        #    (p[a] - p[a_star])·w <= costs[a] - costs[a_star] - ε
        IC_A = p - p[a_star]  # shape (n_actions, m_outcomes)
        IC_b = c - c[a_star] - epsilon
        mask = np.arange(n_actions) != a_star
        A_ub = IC_A[mask]
        b_ub = IC_b[mask]

        # 3) IR constraint: -p[a_star]·w <= -costs[a_star] - ε
        A_ub = np.vstack([A_ub, -p[a_star]])
        b_ub = np.append(b_ub, -c[a_star] - epsilon)

        # 4) bounds: 0 ≤ w_i ≤ v_i
        bounds = [(0, v_i) for v_i in v]

        # 5) solve with Highs
        res = linprog(
            c=c_lp,
            A_ub=A_ub,
            b_ub=b_ub,
            bounds=bounds,
            method='highs',
        )
        if not res.success:
            continue

        w_opt = res.x

        # 6) verify IC with tolerance
        agent_utils = p.dot(w_opt) - c
        max_util = agent_utils.max()
        if agent_utils[a_star] + tol < max_util:
            # still violating IC
            print(
                f"IC violation for a*={a_star}: best at {np.argmax(agent_utils)} (Δ={max_util - agent_utils[a_star]:.2e})")
            continue

        # 7) compute principal’s utility
        princ_util = p[a_star].dot(v - w_opt)
        if princ_util > best_princ_util:
            best_princ_util = princ_util
            best_contract = w_opt.copy()
            best_action = a_star
            best_agent_util = max_util

    print("→ Best principal utility:", best_princ_util)
    print("→ The corresponding best action is:", best_action)
    print("→ The corresponding agent utility is:", best_agent_util)

    return best_princ_util, best_contract


def evaluate_llm(contract_elements: tuple):
    p, c, v, contract_logs = contract_elements

    oracle_best_prin_u, _ = contract_oracle_solver(p, c, v)

    # This contract is generated by the llm generated method or code
    content = contract_logs.to_dict(orient="records")
    # The inferred agent setting should be a ndarray in a shape of (inferred_n_actions, m_outcomes+1)
    time_start = time.time()
    inferred_agent_setting = llm_agent_solver(v, content)
    time_end = time.time()
    print(f"The running time is：{time_end - time_start:.4f} seconds")
    # print(f"The inferred contract is {inferred_agent_setting}")
    inferred_p = inferred_agent_setting[:, :-1]
    inferred_c = inferred_agent_setting[:, -1].reshape(-1)
    _, inferred_contract = contract_oracle_solver(inferred_p, inferred_c, v)

    agent_u = p.dot(inferred_contract) - c
    agent_inx = np.argmax(agent_u)
    real_agent_u = agent_u[agent_inx]
    real_prin_u = p[agent_inx].dot(v - inferred_contract)
    print("→ The oracle principal utility:   ", oracle_best_prin_u)
    print("→ Inferred principal utility:   ", real_prin_u)
    print("→ The corresponding best action is:", agent_inx)
    print("→ The corresponding agent utility is:", real_agent_u)

    _flag = 1 if real_agent_u < 0 else 0

    # Step 1: Check agent IR
    if real_agent_u < 0:
        real_prin_u = 0.0
    # oracle_best_prin_u is a constant since p, c, v are fixed, the only reason of it being here is to ensure the
    # obj is a positive value
    prin_gap = oracle_best_prin_u - real_prin_u
    print(f"→ The principal utility gap is: {oracle_best_prin_u}-{real_prin_u}={prin_gap}")
    ir_penalty = max(0, -real_agent_u)

    # Step 3: Combine
    _obj = prin_gap + ir_penalty
    print("→ Agent and principal score:   ", ir_penalty, prin_gap)

    return _obj, _flag


def extract_data(dataset_path):
    with open(dataset_path, "rb") as f:
        data = pickle.load(f)
        agent_setting = data["agent_setting"]
        contract_logs = data["contract_logs"]
        v = data["v"]

    print(f"[*] Dataset loaded: {dataset_path} with {n_instances} instances.")
    p, c = extract_contract_elements(agent_setting)

    return p, c, v, contract_logs


if __name__ == '__main__':
    print("[*] Running ...")

    # n_actions and m_outcomes are only used for validation, which are 100 and 5 by default during the training phase
    # _n_actions = int(sys.argv[1])
    # _m_outcomes = int(sys.argv[2])
    # mode = sys.argv[3] or "train"
    n_instances = 3

    # Identical with the path in the gen_instance.py
    dataset_path = path.join(path.dirname(__file__), "dataset")

    obj_list = []
    for i in range(n_instances):
        _dataset_path = path.join(dataset_path, f"train_0{i + 1}.pkl")
        # _dataset_path = path.join(dataset_path, f"edgeAI_7_10_0{i + 1}.pkl")
        _p, _c, _v, _contract_logs = extract_data(_dataset_path)
        obj, _ = evaluate_llm((_p, _c, _v, _contract_logs))
        print(f"[*] Instance {i}: {obj}")
        obj_list.append(obj)

    print("[*] Average:")
    print(np.mean(obj_list))
