import csv
import pickle
import time

import numpy as np
from scipy.optimize import linprog
import sys

from os import path
import logging
from utils.utils import extract_contract_elements, get_func_name, save_results
sys.path.insert(0, path.abspath(path.join(path.dirname(__file__), '../../..', '..')))
from problems.online_contract_edgeAI import gpt

possible_func_names = ["agent_solver", "agent_solver_v1", "agent_solver_v2", "agent_solver_v3"]

func_name = get_func_name(gpt, possible_func_names)
llm_agent_solver = getattr(gpt, func_name)


# This eval is used for evaluate the training results
def cal_agent_principal(p, v, c, contract):
    agent_u = p.dot(contract) - c
    agent_inx = np.argmax(agent_u)
    print(np.max(agent_u))
    real_agent_u = agent_u[agent_inx]
    real_prin_u = p[agent_inx].dot(v - contract)

    return real_agent_u, real_prin_u


def contract_oracle_solver(p, c, v):
    # --------------------------------------------------------------------
    # assume you already have:
    #   p         : np.ndarray, shape (n_actions, m_outcomes)
    #   costs     : np.ndarray, shape (n_actions,)     # your c vector
    #   v         : np.ndarray, shape (m_outcomes,)
    #   n_actions = p.shape[0]
    # --------------------------------------------------------------------
    n_actions = p.shape[0]
    epsilon = 1e-16  # small slack to enforce strict IC/IR
    tol = 1e-12  # tolerance for post‐solve check

    best_princ_util = -np.inf
    best_contract = None
    best_action = None
    best_agent_util = None

    for a_star in range(n_actions):
        # 1) objective: minimize p[a_star]·w
        c_lp = p[a_star].copy()

        # 2) build IC constraints in one go
        #    (p[a] - p[a_star])·w <= costs[a] - costs[a_star] - ε
        IC_A = p - p[a_star]  # shape (n_actions, m_outcomes)
        IC_b = c - c[a_star] - epsilon
        mask = np.arange(n_actions) != a_star
        A_ub = IC_A[mask]
        b_ub = IC_b[mask]

        # 3) IR constraint: -p[a_star]·w <= -costs[a_star] - ε
        A_ub = np.vstack([A_ub, -p[a_star]])
        b_ub = np.append(b_ub, -c[a_star] - epsilon)

        # 4) bounds: 0 ≤ w_i ≤ v_i
        bounds = [(0, v_i) for v_i in v]

        # 5) solve with Highs
        res = linprog(
            c=c_lp,
            A_ub=A_ub,
            b_ub=b_ub,
            bounds=bounds,
            method='highs',
        )
        if not res.success:
            continue

        w_opt = res.x

        # 6) verify IC with tolerance
        agent_utils = p.dot(w_opt) - c
        max_util = agent_utils.max()
        if agent_utils[a_star] + tol < max_util:
            # still violating IC
            print(
                f"IC violation for a*={a_star}: best at {np.argmax(agent_utils)} (Δ={max_util - agent_utils[a_star]:.2e})")
            continue

        # 7) compute principal’s utility
        princ_util = p[a_star].dot(v - w_opt)
        if princ_util > best_princ_util:
            best_princ_util = princ_util
            best_contract = w_opt.copy()
            best_action = a_star
            best_agent_util = max_util

    print("→ Best principal utility:", best_princ_util)
    print("→ The corresponding best action is:", best_action)
    print("→ The corresponding agent utility is:", best_agent_util)

    return best_princ_util, best_contract


def evaluate_llm(contract_elements: tuple):
    p, c, v, contract_logs = contract_elements

    oracle_best_prin_u, _ = contract_oracle_solver(p, c, v)
    print("→ Oracle principal utility:", oracle_best_prin_u)

    # This contract is generated by the llm generated method or code
    content = contract_logs.to_dict(orient="records")
    # The inferred agent setting should be a ndarray in a shape of (inferred_n_actions, m_outcomes+1)
    time_start = time.time()
    inferred_agent_setting = llm_agent_solver(v, content)
    time_end = time.time()
    infer_time = time_end - time_start
    print(f"The running time is：{time_end - time_start:.4f} seconds")
    inferred_p = inferred_agent_setting[:, :-1]
    inferred_c = inferred_agent_setting[:, -1].reshape(-1)
    _, inferred_contract = contract_oracle_solver(inferred_p, inferred_c, v)
    # print("->Inferred contract for edge AI bonus incentive is:", inferred_contract)
    wo_bonus_contract = np.zeros_like(inferred_contract)

    real_agent_u, real_prin_u = cal_agent_principal(p, v, c, inferred_contract)
    print("→ Inferred real agent&principal utility:   ", real_agent_u + 4e-5, real_prin_u)
    wobc_agent_u, wobc_prin_u = cal_agent_principal(p, v, c, wo_bonus_contract)
    print("→ Without bonus agent&principal utility:   ", wobc_agent_u + 4e-5, wobc_prin_u)

    _flag = 1 if real_agent_u < 0 else 0

    # Step 1: Check agent IR
    if real_agent_u < 0:
        real_prin_u = 0.0

    # Step 2: Compute components, oracle_best_prin_u is a constant, the only reason it being here is to ensure obj > 0.
    # It is redundant and can be dropped without affect the algorithm.
    prin_gap = oracle_best_prin_u - real_prin_u
    ir_penalty = max(0, -real_agent_u)

    # Step 3: Combine
    _obj = prin_gap + ir_penalty
    # Metrics calculation
    inferred_contract_to_the_optimal = round(real_prin_u / oracle_best_prin_u, 4)
    bonus_improve_prin_u = round((real_prin_u-wobc_prin_u)/wobc_prin_u, 4)
    bonus_improve_agent_u = round(real_agent_u/4e-5, 4)
    print("→ Agent and principal score:   ", ir_penalty, prin_gap)
    print(f"-> The inferred contract to the optimal is: {inferred_contract_to_the_optimal}")
    print(f"-> The bonus mechanism improve the principal utility: {bonus_improve_prin_u}")
    if real_agent_u > 0:
        print(f"-> The inferred contract improve the agent utility: {bonus_improve_agent_u}")

    evaluation_metrics = [inferred_contract_to_the_optimal, bonus_improve_prin_u, bonus_improve_agent_u]

    return _obj, _flag, evaluation_metrics, infer_time


def extract_data(dataset_path):
    with open(dataset_path, "rb") as f:
        data = pickle.load(f)
        agent_setting = data["agent_setting"]
        contract_logs = data["contract_logs"]
        v = data["v"]

    print(f"[*] Dataset loaded: {dataset_path} with instances.")
    p, c = extract_contract_elements(agent_setting)

    return p, c, v, contract_logs


def run_llm(cfg):
    print("[*] Running ...")

    # n_actions and m_outcomes are only used for validation, which are 100 and 5 by default during the training phase
    n_actions_l = cfg.problem.n_actions_l
    m_outcomes_l = cfg.problem.m_outcomes_l
    n_instances = 3

    # Identical with the path in the gen_instance.py
    dataset_path = path.join(path.dirname(__file__), "../dataset")
    obj_list = []
    flag_list = []
    contract_to_optimal = []
    bonus_prin_u = []
    bonus_agent_u = []
    infer_time_l = []

    for m in m_outcomes_l:
        row_cto = []
        row_bpu = []
        row_bau = []
        row_itl = []
        for n in n_actions_l:
            avg_res = []
            itl = []
            for instance in range(n_instances):
                _dataset_path = path.join(dataset_path, f"edgeAI_{n}_{m}_0{instance+1}.pkl")
                _p, _c, _v, _contract_logs = extract_data(_dataset_path)
                logging.info(f"[*] Evaluating {_dataset_path}")
                obj, flag, eval_metrics, infer_time = evaluate_llm((_p, _c, _v, _contract_logs))
                avg_res.append(eval_metrics)
                flag_list.append(flag)
                obj_list.append(obj)
                itl.append(infer_time)
            metrics = np.mean(avg_res, axis=0)
            row_cto.append(round((metrics[0]), 4))
            row_bpu.append(round((metrics[1]), 4))
            row_bau.append(round((metrics[2]), 4))
            row_itl.append(round(np.mean(itl), 4))
        contract_to_optimal.append(row_cto)
        bonus_prin_u.append(row_bpu)
        bonus_agent_u.append(row_bau)
        infer_time_l.append(row_itl)
    print(f"[*] Average IR violation rate is {np.mean(flag_list):.4f}")
    print(f"[*] Average for the problem setting of edge AI: {np.mean(obj_list)}")

    save_results("edgeAI_to_optimal.csv", contract_to_optimal)
    save_results("edgeAI_bonus_prin.csv", bonus_prin_u)
    save_results("edgeAI_bonus_agent.csv", bonus_agent_u)
    save_results("edgeAI_inference_time.csv", infer_time_l)
